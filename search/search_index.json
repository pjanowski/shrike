{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Shrike: Compliant Azure ML Utilities Compliant Machine Learning is the practice of training, validating and deploying machine learning models withou seeing the private data. It is needed in many enterprises to satsify the strict compliance and privacy guarantees that they provide to their customers. The library shrike is a set of Python utilities for compliant machine learning, with a special emphasis on running pipeline in the platform of Azure Machine Learning . This library mainly contains three components, that are shrike.compliant_logging : utlities for compliant logging and exception handling; shrike.pipeline : helper code for manging, validating and submitting Azure Machine Learning pipelines based on azure-ml-component ; shrike.build : helper code for packaging, building, validating, signing and registering Azure Machine Learning components. Documentation For the full documentation of shrike with detailed examples and API reference, please see the docs page . Installation The library shrike is publicly available in PyPi. There are three optional extra dependenciies - pipeline , build and dev , among which pipeline is for submitting Azure Machine Learning pipelines, build is for signing and registering components, and dev is for the development environment of shrike . If only the compliant-logging feature would be used, please pip install without any extras: pip install shrike If it will be used for signing and registering components, please type with [build] : pip install shrike[build] If it will be used for submitting Azure Machine Learning pipelines, please type with [pipeline] : pip install shrike[pipeline] If you would like to contribute to the source code, please install with all the dependencies: pip install shrike[pipeline,build,dev] Migration from aml-build-tooling , aml-ds-pipeline-contrib , and confidential-ml-utils If you have been using \"aml-build-tooling\", \"aml-ds-pipeline-contrib\", and confidential-ml-utils libraries, please use the migration script ( migration.py ) to convert your repo or file and adopt the shrike package with one simple command: python migraton.py --input_path PATH/TO/YOUR/REPO/OR/FILE :warning: This command will update files in-place . Please make a copy of your repo/file if you do not want to do so. Need Support? When you have any feature requests or technical questions or find any bugs, please don't hesitate to file issues. If you are Microsoft employees, please refer to the support page for details; If you are outside Microsoft, feel free to send an email to aml-ds@microsoft.com . Contributing This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Trademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines . Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.","title":"Home"},{"location":"#shrike-compliant-azure-ml-utilities","text":"Compliant Machine Learning is the practice of training, validating and deploying machine learning models withou seeing the private data. It is needed in many enterprises to satsify the strict compliance and privacy guarantees that they provide to their customers. The library shrike is a set of Python utilities for compliant machine learning, with a special emphasis on running pipeline in the platform of Azure Machine Learning . This library mainly contains three components, that are shrike.compliant_logging : utlities for compliant logging and exception handling; shrike.pipeline : helper code for manging, validating and submitting Azure Machine Learning pipelines based on azure-ml-component ; shrike.build : helper code for packaging, building, validating, signing and registering Azure Machine Learning components.","title":"Shrike: Compliant Azure ML Utilities"},{"location":"#documentation","text":"For the full documentation of shrike with detailed examples and API reference, please see the docs page .","title":"Documentation"},{"location":"#installation","text":"The library shrike is publicly available in PyPi. There are three optional extra dependenciies - pipeline , build and dev , among which pipeline is for submitting Azure Machine Learning pipelines, build is for signing and registering components, and dev is for the development environment of shrike . If only the compliant-logging feature would be used, please pip install without any extras: pip install shrike If it will be used for signing and registering components, please type with [build] : pip install shrike[build] If it will be used for submitting Azure Machine Learning pipelines, please type with [pipeline] : pip install shrike[pipeline] If you would like to contribute to the source code, please install with all the dependencies: pip install shrike[pipeline,build,dev]","title":"Installation"},{"location":"#migration-from-aml-build-tooling-aml-ds-pipeline-contrib-and-confidential-ml-utils","text":"If you have been using \"aml-build-tooling\", \"aml-ds-pipeline-contrib\", and confidential-ml-utils libraries, please use the migration script ( migration.py ) to convert your repo or file and adopt the shrike package with one simple command: python migraton.py --input_path PATH/TO/YOUR/REPO/OR/FILE :warning: This command will update files in-place . Please make a copy of your repo/file if you do not want to do so.","title":"Migration from aml-build-tooling, aml-ds-pipeline-contrib, and confidential-ml-utils"},{"location":"#need-support","text":"When you have any feature requests or technical questions or find any bugs, please don't hesitate to file issues. If you are Microsoft employees, please refer to the support page for details; If you are outside Microsoft, feel free to send an email to aml-ds@microsoft.com .","title":"Need Support?"},{"location":"#contributing","text":"This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"Contributing"},{"location":"#trademarks","text":"This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines . Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.","title":"Trademarks"},{"location":"build/build-for-sign-and-register/","text":"Creating an Azure DevOps Build for Signing and Registering By reading through this doc, you will be able to have a high-level understanding of how to use shrike.build , and create a single-YAML pipeline build in Azure DevOps for validating, signing and registering AML components. Requirements To enjoy this tutorial, you need to have at least one AML component YAML specification file in your team's repository, have an AML service connection set up in your Azure DevOps for your Azure subscription, have an ESRP service connection set up in your Azure DevOps, and have a basic knowledge of Azure DevOps pipeline YAML schema . Configuration Command line arguments and configuration YAML file are both supported by shrike.build . The order of precedence from least to greatest (the last listed variables override all other variables) is: default values, configuration file, command line arguments. An example of configuration YAML file: # Choose from two signing mode: aml, or aether signing_mode : aml # Two methods are provided to find \"active\" components: all, or smart # For \"all\" option, all the components will be validated/signed/registered # For \"smart\" option, only those changed components will be processed. activation_method : all # Regular expression that a branch must satisfy in order for this code to # sign or register components. compliant_branch : ^refs/heads/main$ # Determine the working directory. Default: '.' working_directory : '.' # Glob path of all component specification files. component_specification_glob : 'steps/**/module_spec.yaml' log_format : '[%(name)s][%(levelname)s] - %(message)s' # List of workspace ARM IDs workspaces : - /subscriptions/48bbc269-ce89-4f6f-9a12-c6f91fcb772d/resourcegroups/aml1p-rg/providers/Microsoft.MachineLearningServices/workspaces/aml1p-ml-wus2 - /subscriptions/2dea9532-70d2-472d-8ebd-6f53149ad551/resourcegroups/MOP.HERON.PROD.aa8dad83-8884-48de-8b0e-3f3880e7386a/providers/Microsoft.MachineLearningServices/workspaces/amlworkspace4frmqmkryoyzc # Boolean argument: What to do when the same version of a component has already been registered. # Default: False fail_if_version_exists : False # Boolean argument: Will the build number be used or not use_build_number : True To consume this configuration file, we should pass its path to the command line, that is python -m shrike.build.commands.prepare --configuration-file PATH/TO/MY_CONFIGURATION_FILE If we want to override the values of work_directory and fail_if_version_exists at runtime, we should append them to the command line: python -m shrike.build.commands.prepare --configuration-file PATH/TO/MY_CONFIGURATION_FILE --working-directory MY_ANOTHER_WORKDIRECTORY --fail-if-version-exists \"Smart\" mode The shrike package supports a \"smart\" activation_method . Using this \"smart\" mode will only register the components that were modified. The logic used to identify which components are modified is as follows. If a file located in the component folder is changed, then the component is considered to be modified. If a file listed in the additional_includes file (file directly listed, or its parent folder listed) is changed, then the component is considered to be modified. Note: A corollary of point 2 above is that if you modify a function in a helper file listed in the additional_includes , your component will be considered as modified even if it does not use that function at all. That is why we use quotes around \"smart\": the logic is not smart enough to detect only the components truly affected by a change (implementing that logic would be a much more complicated task). Note: Another corollary of point 2 is that if you want to use the \"smart\" mode, you need to be as accurate as possible with the files listed in the additional_includes , otherwise components might be registered even though the changes didn't really affect them. Imagine the extreme case where you have a huge utils directory listed in additional_includes instead of the specific list of utils files: every change to that directory, even if not relevant to your component of interest, will trigger the registration. This would defeat the purpose of having a smart mode in the first place. It is worth reiterating that for the tool to work properly, the name of the compliant branch in your config file should be of the form \" ^refs/heads/<YourCompliantBranchName>$ \" . (Notice how it starts with \" ^refs/heads/ \" and ends with \" $ \".) To identify the latest merge into the compliant branch, the tool relies on the Azure DevOps convention that the commit message starts with \"Merged PR\". If you customize the commit message, please make sure it still starts with \"Merged PR\", otherwise the \"smart\" logic will not work properly. Preparation step In this section, we briefly describe the workflow of the prepare command in the library shrike , that is Search all AML components in the working directory by matching the glob path of component specification files, Validate all \"active\" components, Build all \"active\" components, and Create files catlog.json and catalog.json.sig for each \"active\" component. Note: While building \"active\" components, all additional dependency files specified in .additional_includes will be copied into the component build folder by the prepare command. However, for those dependecy files that are not checked into the repository, such as Odinmal Jar (from NuGet packages) and .zip files, we need to write extra \"tasks\" to copy them into the component build folder. A sample YAML script of preparation step - task : AzureCLI@2 displayName : Preparation inputs : azureSubscription : $(MY_AML_WORKSPACE_SERVICE_CONNECTION) scriptLocation : inlineScript scriptType : pscore inlineScript : | python -m shrike.build.commands.prepare --configuration-file PATH/TO/MY_CONFIGURATION_FILE workingDirectory : $(MY_WORK_DIRECTORY) ESRP CodeSign After creating catlog.json and catalog.json.sig files for each built component in the preparation step, we leverage the ESRP, that is Engineer Sercurity and Release Platform , to sign the contents of components. In the sample yaml script below, we need to customize ConnectedServiceName and FolderPath . In TEEGit repo, the name of ESRP service connection for Torus tenant (\u200bcdc5aeea-15c5-4db6-b079-fcadd2505dc2\u200b) is Substrate AI ESRP . For other repos, if the service connection for ESRP has not been set up yet, please refer to the ESRP CodeSign task Wiki for detailed instructions. - task : EsrpCodeSigning@1 displayName : ESRP CodeSigning inputs : ConnectedServiceName : $(MY_ESRP_SERVICE_CONNECTION) FolderPath : $(MY_WORK_DIRECTORY) Pattern : '*.sig' signConfigType : inlineSignParams inlineOperation : | [ { \"KeyCode\": \"CP-460703-Pgp\", \"OperationCode\": \"LinuxSign\", \"parameters\": {}, \"toolName\": \"sign\", \"toolVersion\": \"1.0\" } ] SessionTimeout : 20 VerboseLogin : true Note: This step requires one-time authorization from the administrator of your ESRP service connection. Please contact your manager or tech lead for authorization questions. Component registration The last step is to register all signed components in our AML workspaces. The register class in the shrike library implements the registration procedure by executing Azure CLI command az ml component --create --file {component} . The Python call is python - m shrike . build . commands . register -- configuration - file path / to / config In this step, the register class can detect signed and built components There are five configuration parameters related with registration step: --compliant-branch , --source-branch , --fail-if-version-exists , --use-build-number , and --all-component-version . We should customize them in the configure-file according to use cases. The register class checks whether the value of source_branch matches that of compliant_branch before starting registration. If their pattern doesn't match, an error message will be logged and the registraion step will be terminated. If fail_if_version_exists is True, an error is raised and the registration step is terminated when the version number of some signed component already exists in the workspace; Otherwise, only a warning is raised and the registration step continues. If all_component_version is not None , the value of all_component_version is used as the version number for all signed components. If use_build_number is True, the build number is used as the version number for all signed components (Overriding the value of all_component_version if all_component_version is not None ). A sample YAML task for registration is - task : AzureCLI@2 displayName : AML Component Registration inputs : azureSubscription : $(MY_AML_WORKSPACE_SERVICE_CONNECTION) scriptLocation : inlineScript scriptType : pscore inlineScript : | python -m shrike.build.commands.register --configuration-file PATH/TO/MY_CONFIGURATION_FILE workingDirectory : $(MY_WORK_DIRECTORY) Note: The shrike library is version-aware. For a component of product-ready version number (e.g., a.b.c), it is set as the default version in the registration step; Otherwise, for a component of non-product-ready version number (e.g., a.b.c-alpha), it will not be labelled as default. Handling components which use binaries For some components (e.g., Linux/Windows components running .NET Core DLLs or Windows Exes, or HDI components leveraging the ODIN-ML JAR or Spark .NET), the signed snapshot needs to contain some binaries. As long as those binaries are compiled from human-reviewed source code or come from internal (authenticated) feeds , this is fine. Teams may inject essentially arbitrary logic into their Azure DevOps pipeline, either for compiling C# code, or downloading \\& extracting NuGets from the Polymer NuGet feed. \u00c6ther-style code signing This tool also assists with \u00c6ther-style code signing. Just write a configuration file like: component_specification_glob : '**/ModuleAutoApprovalManifest.json' signing_mode : aether and then run a code signing step like this just after the \"prepare\" command. Note: your ESRP service connection will need to have access to the CP-230012 key, otherwise you'll encounter the error described in: Got unauthorized to access CP-230012 when calling Aether-style signing service - task : EsrpCodeSigning@1 displayName : sign modules inputs : ConnectedServiceName : $(MY_ESRP_SERVICE_CONNECTION) FolderPath : $(MY_WORK_DIRECTORY) Pattern : '*.cat' signConfigType : inlineSignParams inlineOperation : | [ { \"keyCode\": \"CP-230012\", \"operationSetCode\": \"SigntoolSign\", \"parameters\": [ { \"parameterName\": \"OpusName\", \"parameterValue\": \"Microsoft\" }, { \"parameterName\": \"OpusInfo\", \"parameterValue\": \"http://www.microsoft.com\" }, { \"parameterName\": \"PageHash\", \"parameterValue\": \"/NPH\" }, { \"parameterName\": \"FileDigest\", \"parameterValue\": \"/fd sha256\" }, { \"parameterName\": \"TimeStamp\", \"parameterValue\": \"/tr \\\"http://rfc3161.gtm.corp.microsoft.com/TSS/HttpTspServer\\\" /td sha256\" } ], \"toolName\": \"signtool.exe\", \"toolVersion\": \"6.2.9304.0\" } ] SessionTimeout : 20 VerboseLogin : true \u00c6ther does not support \"true\" CI/CD, but you will be able to use your build drops to register compliant \u00c6ther modules following Signed Builds . For reference, you may imitate this build used by the AML Data Science team . Note: there is no need to run the AML-style and \u00c6ther-style code signing in separate jobs. So long as they both run in a Windows VM, it may be the same job.","title":"Sign & register components"},{"location":"build/build-for-sign-and-register/#creating-an-azure-devops-build-for-signing-and-registering","text":"By reading through this doc, you will be able to have a high-level understanding of how to use shrike.build , and create a single-YAML pipeline build in Azure DevOps for validating, signing and registering AML components.","title":"Creating an Azure DevOps Build for Signing and Registering"},{"location":"build/build-for-sign-and-register/#requirements","text":"To enjoy this tutorial, you need to have at least one AML component YAML specification file in your team's repository, have an AML service connection set up in your Azure DevOps for your Azure subscription, have an ESRP service connection set up in your Azure DevOps, and have a basic knowledge of Azure DevOps pipeline YAML schema .","title":"Requirements"},{"location":"build/build-for-sign-and-register/#configuration","text":"Command line arguments and configuration YAML file are both supported by shrike.build . The order of precedence from least to greatest (the last listed variables override all other variables) is: default values, configuration file, command line arguments. An example of configuration YAML file: # Choose from two signing mode: aml, or aether signing_mode : aml # Two methods are provided to find \"active\" components: all, or smart # For \"all\" option, all the components will be validated/signed/registered # For \"smart\" option, only those changed components will be processed. activation_method : all # Regular expression that a branch must satisfy in order for this code to # sign or register components. compliant_branch : ^refs/heads/main$ # Determine the working directory. Default: '.' working_directory : '.' # Glob path of all component specification files. component_specification_glob : 'steps/**/module_spec.yaml' log_format : '[%(name)s][%(levelname)s] - %(message)s' # List of workspace ARM IDs workspaces : - /subscriptions/48bbc269-ce89-4f6f-9a12-c6f91fcb772d/resourcegroups/aml1p-rg/providers/Microsoft.MachineLearningServices/workspaces/aml1p-ml-wus2 - /subscriptions/2dea9532-70d2-472d-8ebd-6f53149ad551/resourcegroups/MOP.HERON.PROD.aa8dad83-8884-48de-8b0e-3f3880e7386a/providers/Microsoft.MachineLearningServices/workspaces/amlworkspace4frmqmkryoyzc # Boolean argument: What to do when the same version of a component has already been registered. # Default: False fail_if_version_exists : False # Boolean argument: Will the build number be used or not use_build_number : True To consume this configuration file, we should pass its path to the command line, that is python -m shrike.build.commands.prepare --configuration-file PATH/TO/MY_CONFIGURATION_FILE If we want to override the values of work_directory and fail_if_version_exists at runtime, we should append them to the command line: python -m shrike.build.commands.prepare --configuration-file PATH/TO/MY_CONFIGURATION_FILE --working-directory MY_ANOTHER_WORKDIRECTORY --fail-if-version-exists","title":"Configuration"},{"location":"build/build-for-sign-and-register/#smart-mode","text":"The shrike package supports a \"smart\" activation_method . Using this \"smart\" mode will only register the components that were modified. The logic used to identify which components are modified is as follows. If a file located in the component folder is changed, then the component is considered to be modified. If a file listed in the additional_includes file (file directly listed, or its parent folder listed) is changed, then the component is considered to be modified. Note: A corollary of point 2 above is that if you modify a function in a helper file listed in the additional_includes , your component will be considered as modified even if it does not use that function at all. That is why we use quotes around \"smart\": the logic is not smart enough to detect only the components truly affected by a change (implementing that logic would be a much more complicated task). Note: Another corollary of point 2 is that if you want to use the \"smart\" mode, you need to be as accurate as possible with the files listed in the additional_includes , otherwise components might be registered even though the changes didn't really affect them. Imagine the extreme case where you have a huge utils directory listed in additional_includes instead of the specific list of utils files: every change to that directory, even if not relevant to your component of interest, will trigger the registration. This would defeat the purpose of having a smart mode in the first place. It is worth reiterating that for the tool to work properly, the name of the compliant branch in your config file should be of the form \" ^refs/heads/<YourCompliantBranchName>$ \" . (Notice how it starts with \" ^refs/heads/ \" and ends with \" $ \".) To identify the latest merge into the compliant branch, the tool relies on the Azure DevOps convention that the commit message starts with \"Merged PR\". If you customize the commit message, please make sure it still starts with \"Merged PR\", otherwise the \"smart\" logic will not work properly.","title":"\"Smart\" mode"},{"location":"build/build-for-sign-and-register/#preparation-step","text":"In this section, we briefly describe the workflow of the prepare command in the library shrike , that is Search all AML components in the working directory by matching the glob path of component specification files, Validate all \"active\" components, Build all \"active\" components, and Create files catlog.json and catalog.json.sig for each \"active\" component. Note: While building \"active\" components, all additional dependency files specified in .additional_includes will be copied into the component build folder by the prepare command. However, for those dependecy files that are not checked into the repository, such as Odinmal Jar (from NuGet packages) and .zip files, we need to write extra \"tasks\" to copy them into the component build folder. A sample YAML script of preparation step - task : AzureCLI@2 displayName : Preparation inputs : azureSubscription : $(MY_AML_WORKSPACE_SERVICE_CONNECTION) scriptLocation : inlineScript scriptType : pscore inlineScript : | python -m shrike.build.commands.prepare --configuration-file PATH/TO/MY_CONFIGURATION_FILE workingDirectory : $(MY_WORK_DIRECTORY)","title":"Preparation step"},{"location":"build/build-for-sign-and-register/#esrp-codesign","text":"After creating catlog.json and catalog.json.sig files for each built component in the preparation step, we leverage the ESRP, that is Engineer Sercurity and Release Platform , to sign the contents of components. In the sample yaml script below, we need to customize ConnectedServiceName and FolderPath . In TEEGit repo, the name of ESRP service connection for Torus tenant (\u200bcdc5aeea-15c5-4db6-b079-fcadd2505dc2\u200b) is Substrate AI ESRP . For other repos, if the service connection for ESRP has not been set up yet, please refer to the ESRP CodeSign task Wiki for detailed instructions. - task : EsrpCodeSigning@1 displayName : ESRP CodeSigning inputs : ConnectedServiceName : $(MY_ESRP_SERVICE_CONNECTION) FolderPath : $(MY_WORK_DIRECTORY) Pattern : '*.sig' signConfigType : inlineSignParams inlineOperation : | [ { \"KeyCode\": \"CP-460703-Pgp\", \"OperationCode\": \"LinuxSign\", \"parameters\": {}, \"toolName\": \"sign\", \"toolVersion\": \"1.0\" } ] SessionTimeout : 20 VerboseLogin : true Note: This step requires one-time authorization from the administrator of your ESRP service connection. Please contact your manager or tech lead for authorization questions.","title":"ESRP CodeSign"},{"location":"build/build-for-sign-and-register/#component-registration","text":"The last step is to register all signed components in our AML workspaces. The register class in the shrike library implements the registration procedure by executing Azure CLI command az ml component --create --file {component} . The Python call is python - m shrike . build . commands . register -- configuration - file path / to / config In this step, the register class can detect signed and built components There are five configuration parameters related with registration step: --compliant-branch , --source-branch , --fail-if-version-exists , --use-build-number , and --all-component-version . We should customize them in the configure-file according to use cases. The register class checks whether the value of source_branch matches that of compliant_branch before starting registration. If their pattern doesn't match, an error message will be logged and the registraion step will be terminated. If fail_if_version_exists is True, an error is raised and the registration step is terminated when the version number of some signed component already exists in the workspace; Otherwise, only a warning is raised and the registration step continues. If all_component_version is not None , the value of all_component_version is used as the version number for all signed components. If use_build_number is True, the build number is used as the version number for all signed components (Overriding the value of all_component_version if all_component_version is not None ). A sample YAML task for registration is - task : AzureCLI@2 displayName : AML Component Registration inputs : azureSubscription : $(MY_AML_WORKSPACE_SERVICE_CONNECTION) scriptLocation : inlineScript scriptType : pscore inlineScript : | python -m shrike.build.commands.register --configuration-file PATH/TO/MY_CONFIGURATION_FILE workingDirectory : $(MY_WORK_DIRECTORY) Note: The shrike library is version-aware. For a component of product-ready version number (e.g., a.b.c), it is set as the default version in the registration step; Otherwise, for a component of non-product-ready version number (e.g., a.b.c-alpha), it will not be labelled as default.","title":"Component registration"},{"location":"build/build-for-sign-and-register/#handling-components-which-use-binaries","text":"For some components (e.g., Linux/Windows components running .NET Core DLLs or Windows Exes, or HDI components leveraging the ODIN-ML JAR or Spark .NET), the signed snapshot needs to contain some binaries. As long as those binaries are compiled from human-reviewed source code or come from internal (authenticated) feeds , this is fine. Teams may inject essentially arbitrary logic into their Azure DevOps pipeline, either for compiling C# code, or downloading \\& extracting NuGets from the Polymer NuGet feed.","title":"Handling components which use binaries"},{"location":"build/build-for-sign-and-register/#ther-style-code-signing","text":"This tool also assists with \u00c6ther-style code signing. Just write a configuration file like: component_specification_glob : '**/ModuleAutoApprovalManifest.json' signing_mode : aether and then run a code signing step like this just after the \"prepare\" command. Note: your ESRP service connection will need to have access to the CP-230012 key, otherwise you'll encounter the error described in: Got unauthorized to access CP-230012 when calling Aether-style signing service - task : EsrpCodeSigning@1 displayName : sign modules inputs : ConnectedServiceName : $(MY_ESRP_SERVICE_CONNECTION) FolderPath : $(MY_WORK_DIRECTORY) Pattern : '*.cat' signConfigType : inlineSignParams inlineOperation : | [ { \"keyCode\": \"CP-230012\", \"operationSetCode\": \"SigntoolSign\", \"parameters\": [ { \"parameterName\": \"OpusName\", \"parameterValue\": \"Microsoft\" }, { \"parameterName\": \"OpusInfo\", \"parameterValue\": \"http://www.microsoft.com\" }, { \"parameterName\": \"PageHash\", \"parameterValue\": \"/NPH\" }, { \"parameterName\": \"FileDigest\", \"parameterValue\": \"/fd sha256\" }, { \"parameterName\": \"TimeStamp\", \"parameterValue\": \"/tr \\\"http://rfc3161.gtm.corp.microsoft.com/TSS/HttpTspServer\\\" /td sha256\" } ], \"toolName\": \"signtool.exe\", \"toolVersion\": \"6.2.9304.0\" } ] SessionTimeout : 20 VerboseLogin : true \u00c6ther does not support \"true\" CI/CD, but you will be able to use your build drops to register compliant \u00c6ther modules following Signed Builds . For reference, you may imitate this build used by the AML Data Science team . Note: there is no need to run the AML-style and \u00c6ther-style code signing in separate jobs. So long as they both run in a Windows VM, it may be the same job.","title":"&AElig;ther-style code signing"},{"location":"build/prepare/","text":"Prepare Prepare all_files_in_snapshot ( self , manifest ) Return a list of all normalized files in the snapshot. The input ( manifest ) is assumed to be some file, whether AML-style component spec or Aether-style auto-approval manifest, in the \"root\" of the snapshot. Source code in shrike/build/commands/prepare.py def all_files_in_snapshot ( self , manifest : str ) -> List [ str ]: \"\"\" Return a list of all normalized files in the snapshot. The input (`manifest`) is assumed to be some file, whether AML-style component spec or Aether-style auto-approval manifest, in the \"root\" of the snapshot. \"\"\" folder_path = self . folder_path ( manifest ) log . info ( \"Absolute path for current component is: \" + folder_path ) # Generate a list of all files in this components folder (including subdirectories) rv = [] # Make sure we pick up Linux-style \"hidden\" files like .amlignore and # hidden \"directories\", as well as hidden files in hidden directories. # https://stackoverflow.com/a/65205404 # https://stackoverflow.com/a/41447012 for root , _ , file_paths in os . walk ( folder_path ): for file in file_paths : file_path = os . path . join ( root , file ) normalized_path = self . normalize_path ( file_path ) rv . append ( normalized_path ) return rv build_all_components ( self , files ) For each component specification file, run az ml component build , and register the status (+ register error if build failed). Returns the list of \"built\" component files. Source code in shrike/build/commands/prepare.py def build_all_components ( self , files : List [ str ]) -> List [ str ]: \"\"\" For each component specification file, run `az ml component build`, and register the status (+ register error if build failed). Returns the list of \"built\" component files. \"\"\" rv = [] for component in files : path = Path ( component ) rv . append ( str ( path . parent / \".build\" / path . name )) build_component_success = self . execute_azure_cli_command ( f \"ml component build --file { component } \" ) if build_component_success : log . info ( f \"Component { component } is built.\" ) else : self . register_error ( f \"Error when building component { component } .\" ) return rv component_is_active ( self , component , modified_files ) This function returns True if any of the 'modified_files' potentially affects the 'component' (i.e. if it is directly in one of the 'component' subfolders, or if it is covered by the additional_includes files). If the component has been deleted, returns False. Source code in shrike/build/commands/prepare.py def component_is_active ( self , component , modified_files ) -> bool : \"\"\" This function returns True if any of the 'modified_files' potentially affects the 'component' (i.e. if it is directly in one of the 'component' subfolders, or if it is covered by the additional_includes files). If the component has been deleted, returns False. \"\"\" log . info ( \"Assessing whether component '\" + component + \"' is active...\" ) # Let's first take care of the case where the component has been deleted if not ( Path ( component ) . exists ()): return False # Let's grab the contents of the additional_includes file if it exists. # First, we figure out the name of the additional_includes file, based on the component name component_name_without_extension = Path ( component ) . name . split ( \".yaml\" )[ 0 ] # Then, we construct the path of the additional_includes file component_additional_includes_path = os . path . join ( Path ( component ) . parent , component_name_without_extension + \".additional_includes\" , ) # And we finally load it if Path ( component_additional_includes_path ) . exists (): with open ( component_additional_includes_path , \"r\" ) as component_additional_includes : component_additional_includes_contents = ( component_additional_includes . readlines () ) else : component_additional_includes_contents = None # make the paths in the additional_includes file absolute if not ( component_additional_includes_contents is None ): for line_number in range ( 0 , len ( component_additional_includes_contents )): component_additional_includes_contents [ line_number ] = str ( Path ( os . path . join ( Path ( component ) . parent , component_additional_includes_contents [ line_number ] . rstrip ( \" \\n \" ), ) ) . resolve () ) # loop over all modified files; if current file is in subfolder of component or covered by additional includes, return True for modified_file in modified_files : if self . is_in_subfolder ( modified_file , component ) or self . is_in_additional_includes ( modified_file , component_additional_includes_contents ): return True return False create_catalog_files ( self , files ) Create the appropriate kind of catalog file(s), using the configured method (\"aml\" or \"aether\"). Source code in shrike/build/commands/prepare.py def create_catalog_files ( self , files : List [ str ]): \"\"\" Create the appropriate kind of catalog file(s), using the configured method (\"aml\" or \"aether\"). \"\"\" signing_mode = self . config . signing_mode if signing_mode == \"aml\" : self . create_catalog_files_for_aml ( files ) elif signing_mode == \"aether\" : self . create_catalog_files_for_aether ( files ) else : raise ValueError ( f \"Invalid signing_mode provided: ' { signing_mode } '\" ) create_catalog_files_for_aether ( self , files ) Create Aether-friendly .cat files, by first creating a CDF file, then finding and running makecat.exe to create the catalog file. Source code in shrike/build/commands/prepare.py def create_catalog_files_for_aether ( self , files : List [ str ]) -> None : \"\"\" Create Aether-friendly .cat files, by first creating a CDF file, then finding and running `makecat.exe` to create the catalog file. \"\"\" makecat_default = self . config . makecat_default makecat_directory = self . config . makecat_directory makecat = os . path . join ( makecat_directory , makecat_default ) if not os . path . exists ( makecat ): log . info ( f \"Default makecat location { makecat } does not exist\" ) for path in Path ( makecat_directory ) . rglob ( \"makecat.exe\" ): if \"x64\" in str ( path ) . lower (): makecat = path break log . info ( f \"Makecat location: { makecat } \" ) for file in files : directory = os . path . dirname ( file ) name = os . path . split ( directory )[ - 1 ] cat_name = f \" { name } .cat\" cdf_name = f \" { name } .cdf\" path_to_cdf = os . path . join ( directory , cdf_name ) cdf_contents = f \"\"\"[CatalogHeader] Name= { cat_name } PublicVersion=0x0000001 EncodingType=0x00010001 PageHashes=true CATATTR1=0x00010001:OSAttr:2:6.2 [CatalogFiles] \"\"\" files_in_module = self . all_files_in_snapshot ( file ) hash_lines = map ( lambda p : f \"<HASH> { p } = { p } \" , files_in_module ) all_hashes = \" \\n \" . join ( hash_lines ) cdf_contents += all_hashes log . info ( f \"CDF file contents: \\n { cdf_contents } \" ) with open ( path_to_cdf , \"w\" , encoding = \"ascii\" ) as output : output . write ( cdf_contents ) success = self . execute_command ([ str ( makecat ), path_to_cdf , \"-v\" ]) if success : log . info ( f \"Creating Aether catalog files for { name } is successful.\" ) shutil . move ( cat_name , directory ) else : self . register_error ( f \"Error when creating Aether catalog files for { name } .\" ) log . info ( f \"Removing { cdf_name } \" ) os . remove ( path_to_cdf ) log . info ( f \"Finish creating aether catalog files for { name } .\" ) create_catalog_files_for_aml ( self , files ) Create AML-friendly catalog.json and catalog.json.sig files, using SHA-256 hash. Source code in shrike/build/commands/prepare.py def create_catalog_files_for_aml ( self , files : List [ str ]) -> None : \"\"\" Create AML-friendly catalog.json and catalog.json.sig files, using SHA-256 hash. \"\"\" # For each component spec file in the input list, we'll do the following... for f in files : log . info ( f \"Processing file { f } \" ) component_folder_path = self . folder_path ( f ) # remove catalog files if already present log . info ( \"Deleting old catalog files if present\" ) delete_two_catalog_files ( component_folder_path ) files_for_catalog = self . all_files_in_snapshot ( f ) log . info ( \"The following list of files will be added to the catalog.\" ) log . info ( files_for_catalog ) # Prepare the catlog stub: {'HashAlgorithm': 'SHA256', 'CatalogItems': {}} catalog = create_catalog_stub () # Add an entry to the catalog for each file for file_for_catalog in files_for_catalog : catalog = add_file_to_catalog ( file_for_catalog , catalog , component_folder_path ) # order the CatalogItems dictionary catalog [ \"CatalogItems\" ] = collections . OrderedDict ( sorted ( catalog [ \"CatalogItems\" ] . items ()) ) # Write the 2 catalog files log . info ( catalog ) write_two_catalog_files ( catalog , component_folder_path ) log . info ( \"Finished creating catalog files.\" ) find_component_specification_files ( self ) Find the list of \"active\" component specification files using the configured method (\"all\" or \"smart\"). Source code in shrike/build/commands/prepare.py def find_component_specification_files ( self ) -> List [ str ]: \"\"\" Find the list of \"active\" component specification files using the configured method (\"all\" or \"smart\"). \"\"\" activation_method = self . config . activation_method if activation_method == \"all\" : rv = self . find_component_specification_files_using_all () elif activation_method == \"smart\" : rv = self . find_component_specification_files_using_smart () else : raise ValueError ( f \"Invalid activation_method provided: ' { activation_method } '\" ) return rv find_component_specification_files_using_all ( self , dir = None ) Find all component specification files in the configured working directory matching the configured glob. Return the absolute paths of these files in the format of a list of string. Source code in shrike/build/commands/prepare.py def find_component_specification_files_using_all ( self , dir = None ) -> List [ str ]: \"\"\" Find all component specification files in the configured working directory matching the configured glob. Return the absolute paths of these files in the format of a list of string. \"\"\" if dir is None : dir = self . config . working_directory all_spec_yaml_files_absolute_paths = [ str ( p . absolute ()) for p in Path ( dir ) . glob ( self . config . component_specification_glob ) ] return all_spec_yaml_files_absolute_paths find_component_specification_files_using_smart ( self ) This function returns the list of components (as a list of absolute paths) potentially affected by the latest commit. Source code in shrike/build/commands/prepare.py def find_component_specification_files_using_smart ( self ) -> List [ str ]: \"\"\" This function returns the list of components (as a list of absolute paths) potentially affected by the latest commit. \"\"\" log . info ( \"Determining which components are potentially affected by the current change.\" ) [ repo , current_branch , compliant_branch ] = self . identify_repo_and_branches () modified_files = self . get_modified_files ( repo , current_branch , compliant_branch ) active_components = self . infer_active_components_from_modified_files ( modified_files ) return active_components folder_path ( self , file ) Return the normalized path of the directory containing a file. Source code in shrike/build/commands/prepare.py def folder_path ( self , file : str ) -> str : \"\"\" Return the normalized path of the directory containing a file. \"\"\" return self . normalize_path ( Path ( file ) . parent , directory = True ) get_modified_files ( self , repo , current_branch , compliant_branch ) This function returns the paths of files that have been modified. 3 scenarios are supported. 1/ 'Build - before Merge'; when the 'prepare' command is run as part of a build, but before the actual merge (in this case, the name of the current branch starts with 'refs/pull/' - this is the default Azure DevOps behavior). 2/ 'Build - after Merge'; when the 'prepare' command is run as part of a build, after the actual merge (in this case, the name of the current branch is the same as the name of the compliant branch). 3/ 'Manual'; when the prepare command is run manually (typically before publishing the PR). Source code in shrike/build/commands/prepare.py def get_modified_files ( self , repo , current_branch , compliant_branch ) -> Set [ str ]: \"\"\" This function returns the paths of files that have been modified. 3 scenarios are supported.\\n 1/ 'Build - before Merge'; when the 'prepare' command is run as part of a build, but before the actual merge (in this case, the name of the current branch starts with 'refs/pull/' - this is the default Azure DevOps behavior).\\n 2/ 'Build - after Merge'; when the 'prepare' command is run as part of a build, after the actual merge (in this case, the name of the current branch is the same as the name of the compliant branch).\\n 3/ 'Manual'; when the prepare command is run manually (typically before publishing the PR). \"\"\" res = set () # Grab the diff differently depending on the scenario if current_branch . replace ( \"refs/heads/\" , \"\" ) == compliant_branch : # 'Build - after Merge' case: we will take the diff between the # tree of the latest commit to the compliant branch, and the tree # of the previous commit to the compliant branch corresponding to a # PR (we assume the commit summary starts with 'Merged PR') log . info ( \"We are in the 'Build - after Merge' case (the current branch is the compliant branch).\" ) current_commit = repo . remotes . origin . refs [ compliant_branch ] . commit self . log_commit_info ( current_commit , \"Current commit to compliant branch\" ) previous_commit = ( self . get_previous_compliant_commit_corresponding_to_pull_request ( current_commit , consider_current_commit = False , ) ) self . log_commit_info ( previous_commit , \"Previous PR commit to compliant branch\" ) elif current_branch . startswith ( \"refs/pull/\" ): # 'Build - before Merge': we will take the diff between the tree of # the current commit, and the tree of the previous commit to the # compliant branch corresponding to a PR (we assume the commit # summary starts with 'Merged PR') log . info ( \"We are in the 'Build - before Merge' case (the current branch is not the compliant branch and its name starts with 'refs/pull/').\" ) current_commit = repo . commit () self . log_commit_info ( current_commit , \"Current commit to current branch\" ) latest_commit_to_compliant_branch = repo . remotes . origin . refs [ compliant_branch ] . commit previous_commit = ( self . get_previous_compliant_commit_corresponding_to_pull_request ( latest_commit_to_compliant_branch , consider_current_commit = True , ) ) self . log_commit_info ( previous_commit , \"Previous PR commit to compliant branch\" ) else : # 'Manual' Case: we will take the diff between the current branch # and the compliant branch (we're assuming the compliant branch is # locally up to date here) log . info ( \"We are in the 'Manual' case (the current branch is NOT the compliant branch and its name does not start with 'refs/pull/').\" ) try : current_commit = repo . heads [ current_branch ] . commit # this won't work when running the Manual case from the DevOps portal, but the below will except ( IndexError , AttributeError ): current_commit = repo . commit () self . log_commit_info ( current_commit , \"Current commit to current branch\" ) try : previous_commit = repo . heads [ compliant_branch ] . commit # this won't work when running the Manual case from the DevOps portal, but the below will except ( IndexError , AttributeError ): latest_commit_to_compliant_branch = repo . remotes . origin . refs [ compliant_branch ] . commit previous_commit = ( self . get_previous_compliant_commit_corresponding_to_pull_request ( latest_commit_to_compliant_branch , consider_current_commit = True , ) ) self . log_commit_info ( previous_commit , \"Previous commit to compliant branch\" ) # take the actual diff diff = current_commit . tree . diff ( previous_commit . tree ) # let's build a set with the paths of modified files found in the diff object log . debug ( \"Working directory: \" + self . config . working_directory ) log . debug ( \"repo.working_dir: \" + repo . working_dir ) log . debug ( \"repo.working_tree_dir: \" + repo . working_tree_dir ) log . debug ( \"repo.git_dir: \" + repo . git_dir ) for d in diff : log . debug ( \"d.a_path: \" + d . a_path ) log . debug ( \"Path(d.a_path).absolute(): \" + str ( Path ( d . a_path ) . absolute ())) log . debug ( \"Path(d.a_path).resolve(): \" + str ( Path ( d . a_path ) . resolve ())) r_a = str ( Path ( repo . git_dir ) . parent / Path ( d . a_path )) res . add ( r_a ) r_b = str ( Path ( repo . git_dir ) . parent / Path ( d . b_path )) res . add ( r_b ) log . info ( \"The list of modified files is:\" ) log . info ( res ) return res get_previous_compliant_commit_corresponding_to_pull_request ( self , latest_commit , consider_current_commit ) This function will return the previous commit in the repo 's compliant_branch_name corresponding to a PR (i.e. that starts with \"Merged PR\"). If consider_current_commit is set to True, the latest_commit will be considered. If set to false, only previous commits will be considered. Source code in shrike/build/commands/prepare.py def get_previous_compliant_commit_corresponding_to_pull_request ( self , latest_commit , consider_current_commit ): \"\"\" This function will return the previous commit in the `repo`'s `compliant_branch_name` corresponding to a PR (i.e. that starts with \"Merged PR\"). If `consider_current_commit` is set to True, the `latest_commit` will be considered. If set to false, only previous commits will be considered. \"\"\" target_string = \"Merged PR\" if consider_current_commit and latest_commit . summary . startswith ( target_string ): return latest_commit previous_commit = latest_commit for c in previous_commit . iter_parents (): if c . summary . startswith ( target_string ): previous_commit = c break return previous_commit identify_repo_and_branches ( self ) This function returns the current repository, along with the name of the current and compliant branches [repo, current_branch, compliant_branch]. Throws if no repo can be found. Source code in shrike/build/commands/prepare.py def identify_repo_and_branches ( self ): \"\"\" This function returns the current repository, along with the name of the current and compliant branches [repo, current_branch, compliant_branch]. Throws if no repo can be found. \"\"\" # identify the repository curr_path = Path ( self . config . working_directory ) . resolve () try : repo = Repo ( curr_path , search_parent_directories = True ) log . info ( \"Found a valid repository in \" + repo . git_dir ) except ( InvalidGitRepositoryError , NoSuchPathError ): message = ( str ( curr_path ) + \" or its parents do not contain a valid repo path or cannot be accessed.\" ) raise Exception ( message ) try : current_branch = str ( repo . head . ref ) # when running from our build the repo head is detached so this will throw an exception except TypeError : current_branch = os . environ . get ( \"BUILD_SOURCEBRANCH\" ) log . info ( \"The current branch is: '\" + str ( current_branch ) + \"'.\" ) # Identify the compliant branch if not ( self . config . compliant_branch . startswith ( \"^refs/heads/\" )) or not ( self . config . compliant_branch . endswith ( \"$\" ) ): raise Exception ( \"The name of the compliant branch found in the config file should start with '^refs/heads/' and end with '$'. Currently it is: '\" + self . config . compliant_branch + \"'.\" ) else : compliant_branch = self . config . compliant_branch . replace ( \"^refs/heads/\" , \"\" )[ 0 : - 1 ] log . info ( \"The compliant branch is: '\" + compliant_branch + \"'.\" ) return [ repo , current_branch , compliant_branch ] infer_active_components_from_modified_files ( self , modified_files ) This function returns the list of components (as a list of directories paths) potentially affected by changes in the modified_files . Source code in shrike/build/commands/prepare.py def infer_active_components_from_modified_files ( self , modified_files ) -> List [ str ]: \"\"\" This function returns the list of components (as a list of directories paths) potentially affected by changes in the `modified_files`. \"\"\" rv = [] # We will go over components one by one all_components_in_repo = self . find_component_specification_files_using_all () log . info ( \"List of all components in repo:\" ) log . info ( all_components_in_repo ) for component in all_components_in_repo : if self . component_is_active ( component , modified_files ): rv . append ( component ) # No need to dedup rv since we are only considering components once log . info ( \"The active components are:\" ) log . info ( rv ) return rv is_in_additional_includes ( self , modified_file , component_additional_includes_contents ) This function returns True if 'modified_file' is covered by the additional_includes file 'component_additional_includes_contents'. Source code in shrike/build/commands/prepare.py def is_in_additional_includes ( self , modified_file , component_additional_includes_contents ) -> bool : \"\"\" This function returns True if 'modified_file' is covered by the additional_includes file 'component_additional_includes_contents'. \"\"\" # first tackle the trivial case of no additional_includes file if component_additional_includes_contents is None : log . debug ( \"The component's additional_includes file is empty, returning False.\" ) return False # now the regular scenario for line in component_additional_includes_contents : # when the line from additional_includes is a file, we directly chech its path against that of modified_file if Path ( line ) . is_file (): if str ( Path ( modified_file ) . resolve ()) == str ( Path ( line ) . resolve () ): # can't use 'samefile' here because modified_file is not guaranteed to exist, we resolve the path and do basic == test log . info ( \"'\" + modified_file + \" is directly listed in the additional_includes file.\" ) return True # slightly more complicated case: when the line in additional_includes is a directory, we can just call the is_in_subfolder function if Path ( line ) . is_dir (): if self . is_in_subfolder ( modified_file , line ): log . info ( \"'\" + modified_file + \" is in one of the directories listed in the additional_includes file.\" ) return True log . debug ( \"'\" + modified_file + \" is NOT referenced by the additional_includes file (neither directly nor indirectly).\" ) return False is_in_subfolder ( self , modified_file , component ) This function returns True if 'modified_file' is in a subfolder of 'component' ('component' can be either the path to a file, or a directory). If the component has been deleted, returns False. Source code in shrike/build/commands/prepare.py def is_in_subfolder ( self , modified_file , component ) -> bool : \"\"\" This function returns True if 'modified_file' is in a subfolder of 'component' ('component' can be either the path to a file, or a directory). If the component has been deleted, returns False. \"\"\" # Let's first take care of the case where the component has been deleted if not ( Path ( component ) . exists ()): log . debug ( \"'\" + component + \"' does not exist, returning False.\" ) return False # Case where the component has not been deleted for parent in Path ( modified_file ) . parents : if parent . exists (): if Path ( component ) . is_dir (): if parent . samefile ( Path ( component )): log . info ( \"'\" + modified_file + \" is in a subfolder of '\" + component + \"'.\" ) return True else : if parent . samefile ( Path ( component ) . parent ): log . info ( \"'\" + modified_file + \" is in a subfolder of '\" + component + \"'.\" ) return True log . debug ( \"'\" + modified_file + \" is NOT in a subfolder of '\" + component + \"'.\" ) return False run_with_config ( self ) Run the subclasses command with the specified configuration object. Before this method is invoked, there is no guarantee that self.config will be populated; after it is invoked, that is guaranteed. Implementations of this method should NOT mutate the logging tree in any way. They should also NOT raise any exceptions; rather they should call the register_error method, which will ensure non-zero exit code. Implementations can raise specific \"status information\" (e.g., a component is not \"active\") by calling register_component_status . Source code in shrike/build/commands/prepare.py def run_with_config ( self ): log . info ( \"Running component preparation logic.\" ) self . telemetry_logging ( command = \"prepare\" ) component_files = self . find_component_specification_files () if not self . config . suppress_adding_repo_pr_tags : component_files = self . add_repo_and_last_pr_to_tags ( component_files ) if self . config . signing_mode == \"aml\" : self . ensure_component_cli_installed () self . attach_workspace () self . validate_all_components ( component_files ) built_component_files = self . build_all_components ( component_files ) else : built_component_files = component_files self . create_catalog_files ( built_component_files ) validate_all_components ( self , files ) For each component specification file, run az ml component validate , and register the status (+ register error if validation failed). Source code in shrike/build/commands/prepare.py def validate_all_components ( self , files : List [ str ]) -> None : \"\"\" For each component specification file, run `az ml component validate`, and register the status (+ register error if validation failed). \"\"\" for component in files : validate_component_success = self . execute_azure_cli_command ( f \"ml component validate --file { component } \" ) if validate_component_success : # If the az ml validation succeeds, we continue to check whether # the \"code\" snapshot parameter is specified in the spec file # https://componentsdk.z22.web.core.windows.net/components/component-spec-topics/code-snapshot.html with open ( component , \"r\" ) as spec_file : spec = YAML ( typ = \"safe\" ) . load ( spec_file ) spec_code = spec . get ( \"code\" ) if spec_code and spec_code not in [ \".\" , \"./\" ]: self . register_component_status ( component , \"validate\" , \"failed\" ) self . register_error ( \"Code snapshot parameter is not supported. Please use .additional_includes for your component.\" ) else : log . info ( f \"Component { component } is valid.\" ) self . register_component_status ( component , \"validate\" , \"succeeded\" ) else : self . register_component_status ( component , \"validate\" , \"failed\" ) self . register_error ( f \"Error when validating component { component } .\" )","title":"prepare"},{"location":"build/prepare/#prepare","text":"","title":"Prepare"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare","text":"","title":"Prepare"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.all_files_in_snapshot","text":"Return a list of all normalized files in the snapshot. The input ( manifest ) is assumed to be some file, whether AML-style component spec or Aether-style auto-approval manifest, in the \"root\" of the snapshot. Source code in shrike/build/commands/prepare.py def all_files_in_snapshot ( self , manifest : str ) -> List [ str ]: \"\"\" Return a list of all normalized files in the snapshot. The input (`manifest`) is assumed to be some file, whether AML-style component spec or Aether-style auto-approval manifest, in the \"root\" of the snapshot. \"\"\" folder_path = self . folder_path ( manifest ) log . info ( \"Absolute path for current component is: \" + folder_path ) # Generate a list of all files in this components folder (including subdirectories) rv = [] # Make sure we pick up Linux-style \"hidden\" files like .amlignore and # hidden \"directories\", as well as hidden files in hidden directories. # https://stackoverflow.com/a/65205404 # https://stackoverflow.com/a/41447012 for root , _ , file_paths in os . walk ( folder_path ): for file in file_paths : file_path = os . path . join ( root , file ) normalized_path = self . normalize_path ( file_path ) rv . append ( normalized_path ) return rv","title":"all_files_in_snapshot()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.build_all_components","text":"For each component specification file, run az ml component build , and register the status (+ register error if build failed). Returns the list of \"built\" component files. Source code in shrike/build/commands/prepare.py def build_all_components ( self , files : List [ str ]) -> List [ str ]: \"\"\" For each component specification file, run `az ml component build`, and register the status (+ register error if build failed). Returns the list of \"built\" component files. \"\"\" rv = [] for component in files : path = Path ( component ) rv . append ( str ( path . parent / \".build\" / path . name )) build_component_success = self . execute_azure_cli_command ( f \"ml component build --file { component } \" ) if build_component_success : log . info ( f \"Component { component } is built.\" ) else : self . register_error ( f \"Error when building component { component } .\" ) return rv","title":"build_all_components()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.component_is_active","text":"This function returns True if any of the 'modified_files' potentially affects the 'component' (i.e. if it is directly in one of the 'component' subfolders, or if it is covered by the additional_includes files). If the component has been deleted, returns False. Source code in shrike/build/commands/prepare.py def component_is_active ( self , component , modified_files ) -> bool : \"\"\" This function returns True if any of the 'modified_files' potentially affects the 'component' (i.e. if it is directly in one of the 'component' subfolders, or if it is covered by the additional_includes files). If the component has been deleted, returns False. \"\"\" log . info ( \"Assessing whether component '\" + component + \"' is active...\" ) # Let's first take care of the case where the component has been deleted if not ( Path ( component ) . exists ()): return False # Let's grab the contents of the additional_includes file if it exists. # First, we figure out the name of the additional_includes file, based on the component name component_name_without_extension = Path ( component ) . name . split ( \".yaml\" )[ 0 ] # Then, we construct the path of the additional_includes file component_additional_includes_path = os . path . join ( Path ( component ) . parent , component_name_without_extension + \".additional_includes\" , ) # And we finally load it if Path ( component_additional_includes_path ) . exists (): with open ( component_additional_includes_path , \"r\" ) as component_additional_includes : component_additional_includes_contents = ( component_additional_includes . readlines () ) else : component_additional_includes_contents = None # make the paths in the additional_includes file absolute if not ( component_additional_includes_contents is None ): for line_number in range ( 0 , len ( component_additional_includes_contents )): component_additional_includes_contents [ line_number ] = str ( Path ( os . path . join ( Path ( component ) . parent , component_additional_includes_contents [ line_number ] . rstrip ( \" \\n \" ), ) ) . resolve () ) # loop over all modified files; if current file is in subfolder of component or covered by additional includes, return True for modified_file in modified_files : if self . is_in_subfolder ( modified_file , component ) or self . is_in_additional_includes ( modified_file , component_additional_includes_contents ): return True return False","title":"component_is_active()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.create_catalog_files","text":"Create the appropriate kind of catalog file(s), using the configured method (\"aml\" or \"aether\"). Source code in shrike/build/commands/prepare.py def create_catalog_files ( self , files : List [ str ]): \"\"\" Create the appropriate kind of catalog file(s), using the configured method (\"aml\" or \"aether\"). \"\"\" signing_mode = self . config . signing_mode if signing_mode == \"aml\" : self . create_catalog_files_for_aml ( files ) elif signing_mode == \"aether\" : self . create_catalog_files_for_aether ( files ) else : raise ValueError ( f \"Invalid signing_mode provided: ' { signing_mode } '\" )","title":"create_catalog_files()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.create_catalog_files_for_aether","text":"Create Aether-friendly .cat files, by first creating a CDF file, then finding and running makecat.exe to create the catalog file. Source code in shrike/build/commands/prepare.py def create_catalog_files_for_aether ( self , files : List [ str ]) -> None : \"\"\" Create Aether-friendly .cat files, by first creating a CDF file, then finding and running `makecat.exe` to create the catalog file. \"\"\" makecat_default = self . config . makecat_default makecat_directory = self . config . makecat_directory makecat = os . path . join ( makecat_directory , makecat_default ) if not os . path . exists ( makecat ): log . info ( f \"Default makecat location { makecat } does not exist\" ) for path in Path ( makecat_directory ) . rglob ( \"makecat.exe\" ): if \"x64\" in str ( path ) . lower (): makecat = path break log . info ( f \"Makecat location: { makecat } \" ) for file in files : directory = os . path . dirname ( file ) name = os . path . split ( directory )[ - 1 ] cat_name = f \" { name } .cat\" cdf_name = f \" { name } .cdf\" path_to_cdf = os . path . join ( directory , cdf_name ) cdf_contents = f \"\"\"[CatalogHeader] Name= { cat_name } PublicVersion=0x0000001 EncodingType=0x00010001 PageHashes=true CATATTR1=0x00010001:OSAttr:2:6.2 [CatalogFiles] \"\"\" files_in_module = self . all_files_in_snapshot ( file ) hash_lines = map ( lambda p : f \"<HASH> { p } = { p } \" , files_in_module ) all_hashes = \" \\n \" . join ( hash_lines ) cdf_contents += all_hashes log . info ( f \"CDF file contents: \\n { cdf_contents } \" ) with open ( path_to_cdf , \"w\" , encoding = \"ascii\" ) as output : output . write ( cdf_contents ) success = self . execute_command ([ str ( makecat ), path_to_cdf , \"-v\" ]) if success : log . info ( f \"Creating Aether catalog files for { name } is successful.\" ) shutil . move ( cat_name , directory ) else : self . register_error ( f \"Error when creating Aether catalog files for { name } .\" ) log . info ( f \"Removing { cdf_name } \" ) os . remove ( path_to_cdf ) log . info ( f \"Finish creating aether catalog files for { name } .\" )","title":"create_catalog_files_for_aether()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.create_catalog_files_for_aml","text":"Create AML-friendly catalog.json and catalog.json.sig files, using SHA-256 hash. Source code in shrike/build/commands/prepare.py def create_catalog_files_for_aml ( self , files : List [ str ]) -> None : \"\"\" Create AML-friendly catalog.json and catalog.json.sig files, using SHA-256 hash. \"\"\" # For each component spec file in the input list, we'll do the following... for f in files : log . info ( f \"Processing file { f } \" ) component_folder_path = self . folder_path ( f ) # remove catalog files if already present log . info ( \"Deleting old catalog files if present\" ) delete_two_catalog_files ( component_folder_path ) files_for_catalog = self . all_files_in_snapshot ( f ) log . info ( \"The following list of files will be added to the catalog.\" ) log . info ( files_for_catalog ) # Prepare the catlog stub: {'HashAlgorithm': 'SHA256', 'CatalogItems': {}} catalog = create_catalog_stub () # Add an entry to the catalog for each file for file_for_catalog in files_for_catalog : catalog = add_file_to_catalog ( file_for_catalog , catalog , component_folder_path ) # order the CatalogItems dictionary catalog [ \"CatalogItems\" ] = collections . OrderedDict ( sorted ( catalog [ \"CatalogItems\" ] . items ()) ) # Write the 2 catalog files log . info ( catalog ) write_two_catalog_files ( catalog , component_folder_path ) log . info ( \"Finished creating catalog files.\" )","title":"create_catalog_files_for_aml()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.find_component_specification_files","text":"Find the list of \"active\" component specification files using the configured method (\"all\" or \"smart\"). Source code in shrike/build/commands/prepare.py def find_component_specification_files ( self ) -> List [ str ]: \"\"\" Find the list of \"active\" component specification files using the configured method (\"all\" or \"smart\"). \"\"\" activation_method = self . config . activation_method if activation_method == \"all\" : rv = self . find_component_specification_files_using_all () elif activation_method == \"smart\" : rv = self . find_component_specification_files_using_smart () else : raise ValueError ( f \"Invalid activation_method provided: ' { activation_method } '\" ) return rv","title":"find_component_specification_files()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.find_component_specification_files_using_all","text":"Find all component specification files in the configured working directory matching the configured glob. Return the absolute paths of these files in the format of a list of string. Source code in shrike/build/commands/prepare.py def find_component_specification_files_using_all ( self , dir = None ) -> List [ str ]: \"\"\" Find all component specification files in the configured working directory matching the configured glob. Return the absolute paths of these files in the format of a list of string. \"\"\" if dir is None : dir = self . config . working_directory all_spec_yaml_files_absolute_paths = [ str ( p . absolute ()) for p in Path ( dir ) . glob ( self . config . component_specification_glob ) ] return all_spec_yaml_files_absolute_paths","title":"find_component_specification_files_using_all()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.find_component_specification_files_using_smart","text":"This function returns the list of components (as a list of absolute paths) potentially affected by the latest commit. Source code in shrike/build/commands/prepare.py def find_component_specification_files_using_smart ( self ) -> List [ str ]: \"\"\" This function returns the list of components (as a list of absolute paths) potentially affected by the latest commit. \"\"\" log . info ( \"Determining which components are potentially affected by the current change.\" ) [ repo , current_branch , compliant_branch ] = self . identify_repo_and_branches () modified_files = self . get_modified_files ( repo , current_branch , compliant_branch ) active_components = self . infer_active_components_from_modified_files ( modified_files ) return active_components","title":"find_component_specification_files_using_smart()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.folder_path","text":"Return the normalized path of the directory containing a file. Source code in shrike/build/commands/prepare.py def folder_path ( self , file : str ) -> str : \"\"\" Return the normalized path of the directory containing a file. \"\"\" return self . normalize_path ( Path ( file ) . parent , directory = True )","title":"folder_path()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.get_modified_files","text":"This function returns the paths of files that have been modified. 3 scenarios are supported. 1/ 'Build - before Merge'; when the 'prepare' command is run as part of a build, but before the actual merge (in this case, the name of the current branch starts with 'refs/pull/' - this is the default Azure DevOps behavior). 2/ 'Build - after Merge'; when the 'prepare' command is run as part of a build, after the actual merge (in this case, the name of the current branch is the same as the name of the compliant branch). 3/ 'Manual'; when the prepare command is run manually (typically before publishing the PR). Source code in shrike/build/commands/prepare.py def get_modified_files ( self , repo , current_branch , compliant_branch ) -> Set [ str ]: \"\"\" This function returns the paths of files that have been modified. 3 scenarios are supported.\\n 1/ 'Build - before Merge'; when the 'prepare' command is run as part of a build, but before the actual merge (in this case, the name of the current branch starts with 'refs/pull/' - this is the default Azure DevOps behavior).\\n 2/ 'Build - after Merge'; when the 'prepare' command is run as part of a build, after the actual merge (in this case, the name of the current branch is the same as the name of the compliant branch).\\n 3/ 'Manual'; when the prepare command is run manually (typically before publishing the PR). \"\"\" res = set () # Grab the diff differently depending on the scenario if current_branch . replace ( \"refs/heads/\" , \"\" ) == compliant_branch : # 'Build - after Merge' case: we will take the diff between the # tree of the latest commit to the compliant branch, and the tree # of the previous commit to the compliant branch corresponding to a # PR (we assume the commit summary starts with 'Merged PR') log . info ( \"We are in the 'Build - after Merge' case (the current branch is the compliant branch).\" ) current_commit = repo . remotes . origin . refs [ compliant_branch ] . commit self . log_commit_info ( current_commit , \"Current commit to compliant branch\" ) previous_commit = ( self . get_previous_compliant_commit_corresponding_to_pull_request ( current_commit , consider_current_commit = False , ) ) self . log_commit_info ( previous_commit , \"Previous PR commit to compliant branch\" ) elif current_branch . startswith ( \"refs/pull/\" ): # 'Build - before Merge': we will take the diff between the tree of # the current commit, and the tree of the previous commit to the # compliant branch corresponding to a PR (we assume the commit # summary starts with 'Merged PR') log . info ( \"We are in the 'Build - before Merge' case (the current branch is not the compliant branch and its name starts with 'refs/pull/').\" ) current_commit = repo . commit () self . log_commit_info ( current_commit , \"Current commit to current branch\" ) latest_commit_to_compliant_branch = repo . remotes . origin . refs [ compliant_branch ] . commit previous_commit = ( self . get_previous_compliant_commit_corresponding_to_pull_request ( latest_commit_to_compliant_branch , consider_current_commit = True , ) ) self . log_commit_info ( previous_commit , \"Previous PR commit to compliant branch\" ) else : # 'Manual' Case: we will take the diff between the current branch # and the compliant branch (we're assuming the compliant branch is # locally up to date here) log . info ( \"We are in the 'Manual' case (the current branch is NOT the compliant branch and its name does not start with 'refs/pull/').\" ) try : current_commit = repo . heads [ current_branch ] . commit # this won't work when running the Manual case from the DevOps portal, but the below will except ( IndexError , AttributeError ): current_commit = repo . commit () self . log_commit_info ( current_commit , \"Current commit to current branch\" ) try : previous_commit = repo . heads [ compliant_branch ] . commit # this won't work when running the Manual case from the DevOps portal, but the below will except ( IndexError , AttributeError ): latest_commit_to_compliant_branch = repo . remotes . origin . refs [ compliant_branch ] . commit previous_commit = ( self . get_previous_compliant_commit_corresponding_to_pull_request ( latest_commit_to_compliant_branch , consider_current_commit = True , ) ) self . log_commit_info ( previous_commit , \"Previous commit to compliant branch\" ) # take the actual diff diff = current_commit . tree . diff ( previous_commit . tree ) # let's build a set with the paths of modified files found in the diff object log . debug ( \"Working directory: \" + self . config . working_directory ) log . debug ( \"repo.working_dir: \" + repo . working_dir ) log . debug ( \"repo.working_tree_dir: \" + repo . working_tree_dir ) log . debug ( \"repo.git_dir: \" + repo . git_dir ) for d in diff : log . debug ( \"d.a_path: \" + d . a_path ) log . debug ( \"Path(d.a_path).absolute(): \" + str ( Path ( d . a_path ) . absolute ())) log . debug ( \"Path(d.a_path).resolve(): \" + str ( Path ( d . a_path ) . resolve ())) r_a = str ( Path ( repo . git_dir ) . parent / Path ( d . a_path )) res . add ( r_a ) r_b = str ( Path ( repo . git_dir ) . parent / Path ( d . b_path )) res . add ( r_b ) log . info ( \"The list of modified files is:\" ) log . info ( res ) return res","title":"get_modified_files()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.get_previous_compliant_commit_corresponding_to_pull_request","text":"This function will return the previous commit in the repo 's compliant_branch_name corresponding to a PR (i.e. that starts with \"Merged PR\"). If consider_current_commit is set to True, the latest_commit will be considered. If set to false, only previous commits will be considered. Source code in shrike/build/commands/prepare.py def get_previous_compliant_commit_corresponding_to_pull_request ( self , latest_commit , consider_current_commit ): \"\"\" This function will return the previous commit in the `repo`'s `compliant_branch_name` corresponding to a PR (i.e. that starts with \"Merged PR\"). If `consider_current_commit` is set to True, the `latest_commit` will be considered. If set to false, only previous commits will be considered. \"\"\" target_string = \"Merged PR\" if consider_current_commit and latest_commit . summary . startswith ( target_string ): return latest_commit previous_commit = latest_commit for c in previous_commit . iter_parents (): if c . summary . startswith ( target_string ): previous_commit = c break return previous_commit","title":"get_previous_compliant_commit_corresponding_to_pull_request()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.identify_repo_and_branches","text":"This function returns the current repository, along with the name of the current and compliant branches [repo, current_branch, compliant_branch]. Throws if no repo can be found. Source code in shrike/build/commands/prepare.py def identify_repo_and_branches ( self ): \"\"\" This function returns the current repository, along with the name of the current and compliant branches [repo, current_branch, compliant_branch]. Throws if no repo can be found. \"\"\" # identify the repository curr_path = Path ( self . config . working_directory ) . resolve () try : repo = Repo ( curr_path , search_parent_directories = True ) log . info ( \"Found a valid repository in \" + repo . git_dir ) except ( InvalidGitRepositoryError , NoSuchPathError ): message = ( str ( curr_path ) + \" or its parents do not contain a valid repo path or cannot be accessed.\" ) raise Exception ( message ) try : current_branch = str ( repo . head . ref ) # when running from our build the repo head is detached so this will throw an exception except TypeError : current_branch = os . environ . get ( \"BUILD_SOURCEBRANCH\" ) log . info ( \"The current branch is: '\" + str ( current_branch ) + \"'.\" ) # Identify the compliant branch if not ( self . config . compliant_branch . startswith ( \"^refs/heads/\" )) or not ( self . config . compliant_branch . endswith ( \"$\" ) ): raise Exception ( \"The name of the compliant branch found in the config file should start with '^refs/heads/' and end with '$'. Currently it is: '\" + self . config . compliant_branch + \"'.\" ) else : compliant_branch = self . config . compliant_branch . replace ( \"^refs/heads/\" , \"\" )[ 0 : - 1 ] log . info ( \"The compliant branch is: '\" + compliant_branch + \"'.\" ) return [ repo , current_branch , compliant_branch ]","title":"identify_repo_and_branches()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.infer_active_components_from_modified_files","text":"This function returns the list of components (as a list of directories paths) potentially affected by changes in the modified_files . Source code in shrike/build/commands/prepare.py def infer_active_components_from_modified_files ( self , modified_files ) -> List [ str ]: \"\"\" This function returns the list of components (as a list of directories paths) potentially affected by changes in the `modified_files`. \"\"\" rv = [] # We will go over components one by one all_components_in_repo = self . find_component_specification_files_using_all () log . info ( \"List of all components in repo:\" ) log . info ( all_components_in_repo ) for component in all_components_in_repo : if self . component_is_active ( component , modified_files ): rv . append ( component ) # No need to dedup rv since we are only considering components once log . info ( \"The active components are:\" ) log . info ( rv ) return rv","title":"infer_active_components_from_modified_files()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.is_in_additional_includes","text":"This function returns True if 'modified_file' is covered by the additional_includes file 'component_additional_includes_contents'. Source code in shrike/build/commands/prepare.py def is_in_additional_includes ( self , modified_file , component_additional_includes_contents ) -> bool : \"\"\" This function returns True if 'modified_file' is covered by the additional_includes file 'component_additional_includes_contents'. \"\"\" # first tackle the trivial case of no additional_includes file if component_additional_includes_contents is None : log . debug ( \"The component's additional_includes file is empty, returning False.\" ) return False # now the regular scenario for line in component_additional_includes_contents : # when the line from additional_includes is a file, we directly chech its path against that of modified_file if Path ( line ) . is_file (): if str ( Path ( modified_file ) . resolve ()) == str ( Path ( line ) . resolve () ): # can't use 'samefile' here because modified_file is not guaranteed to exist, we resolve the path and do basic == test log . info ( \"'\" + modified_file + \" is directly listed in the additional_includes file.\" ) return True # slightly more complicated case: when the line in additional_includes is a directory, we can just call the is_in_subfolder function if Path ( line ) . is_dir (): if self . is_in_subfolder ( modified_file , line ): log . info ( \"'\" + modified_file + \" is in one of the directories listed in the additional_includes file.\" ) return True log . debug ( \"'\" + modified_file + \" is NOT referenced by the additional_includes file (neither directly nor indirectly).\" ) return False","title":"is_in_additional_includes()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.is_in_subfolder","text":"This function returns True if 'modified_file' is in a subfolder of 'component' ('component' can be either the path to a file, or a directory). If the component has been deleted, returns False. Source code in shrike/build/commands/prepare.py def is_in_subfolder ( self , modified_file , component ) -> bool : \"\"\" This function returns True if 'modified_file' is in a subfolder of 'component' ('component' can be either the path to a file, or a directory). If the component has been deleted, returns False. \"\"\" # Let's first take care of the case where the component has been deleted if not ( Path ( component ) . exists ()): log . debug ( \"'\" + component + \"' does not exist, returning False.\" ) return False # Case where the component has not been deleted for parent in Path ( modified_file ) . parents : if parent . exists (): if Path ( component ) . is_dir (): if parent . samefile ( Path ( component )): log . info ( \"'\" + modified_file + \" is in a subfolder of '\" + component + \"'.\" ) return True else : if parent . samefile ( Path ( component ) . parent ): log . info ( \"'\" + modified_file + \" is in a subfolder of '\" + component + \"'.\" ) return True log . debug ( \"'\" + modified_file + \" is NOT in a subfolder of '\" + component + \"'.\" ) return False","title":"is_in_subfolder()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.run_with_config","text":"Run the subclasses command with the specified configuration object. Before this method is invoked, there is no guarantee that self.config will be populated; after it is invoked, that is guaranteed. Implementations of this method should NOT mutate the logging tree in any way. They should also NOT raise any exceptions; rather they should call the register_error method, which will ensure non-zero exit code. Implementations can raise specific \"status information\" (e.g., a component is not \"active\") by calling register_component_status . Source code in shrike/build/commands/prepare.py def run_with_config ( self ): log . info ( \"Running component preparation logic.\" ) self . telemetry_logging ( command = \"prepare\" ) component_files = self . find_component_specification_files () if not self . config . suppress_adding_repo_pr_tags : component_files = self . add_repo_and_last_pr_to_tags ( component_files ) if self . config . signing_mode == \"aml\" : self . ensure_component_cli_installed () self . attach_workspace () self . validate_all_components ( component_files ) built_component_files = self . build_all_components ( component_files ) else : built_component_files = component_files self . create_catalog_files ( built_component_files )","title":"run_with_config()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.validate_all_components","text":"For each component specification file, run az ml component validate , and register the status (+ register error if validation failed). Source code in shrike/build/commands/prepare.py def validate_all_components ( self , files : List [ str ]) -> None : \"\"\" For each component specification file, run `az ml component validate`, and register the status (+ register error if validation failed). \"\"\" for component in files : validate_component_success = self . execute_azure_cli_command ( f \"ml component validate --file { component } \" ) if validate_component_success : # If the az ml validation succeeds, we continue to check whether # the \"code\" snapshot parameter is specified in the spec file # https://componentsdk.z22.web.core.windows.net/components/component-spec-topics/code-snapshot.html with open ( component , \"r\" ) as spec_file : spec = YAML ( typ = \"safe\" ) . load ( spec_file ) spec_code = spec . get ( \"code\" ) if spec_code and spec_code not in [ \".\" , \"./\" ]: self . register_component_status ( component , \"validate\" , \"failed\" ) self . register_error ( \"Code snapshot parameter is not supported. Please use .additional_includes for your component.\" ) else : log . info ( f \"Component { component } is valid.\" ) self . register_component_status ( component , \"validate\" , \"succeeded\" ) else : self . register_component_status ( component , \"validate\" , \"failed\" ) self . register_error ( f \"Error when validating component { component } .\" )","title":"validate_all_components()"},{"location":"build/register/","text":"Register Register find_signed_component_specification_files ( self , dir = None ) Find all signed component (AML format) generated in the prepare step. Return the absolute paths of these components in the format of a list of string. Source code in shrike/build/commands/register.py def find_signed_component_specification_files ( self , dir = None ) -> List [ str ]: \"\"\" Find all signed component (AML format) generated in the `prepare` step. Return the absolute paths of these components in the format of a list of string. \"\"\" if dir is None : dir = self . config . working_directory signed_component_spec_files = [] # Find the path of spec files in the '.build' folders for spec_path in Path ( dir ) . glob ( \"**/.build/\" + Path ( self . config . component_specification_glob ) . name ): # Check whether the component is signed by examining catalog files if ( spec_path . parent . joinpath ( \"catalog.json\" ) . exists () and spec_path . parent . joinpath ( \"catalog.json.sig\" ) . exists () ): signed_component_spec_files . append ( os . path . abspath ( spec_path )) log . info ( f \"Find a signed component for AML: { spec_path } \" ) elif spec_path . parent . joinpath ( \".build.cat\" ) . exists (): log . info ( f \"Find a signed component for Aether: { spec_path } \" ) else : log . warning ( f \"Find an unsigned component: { spec_path } \" ) log . info ( str ( spec_path . parent . joinpath ( \"catalog.json\" ))) if len ( signed_component_spec_files ) == 0 : log . info ( \"Cannot find any signed components for AML.\" ) else : log . info ( f \"Find { len ( signed_component_spec_files ) } signed components for AML.\" ) return signed_component_spec_files list_registered_component ( self ) Log all registered component in the attached workspace by using az ml command. Source code in shrike/build/commands/register.py def list_registered_component ( self ) -> None : \"\"\" Log all registered component in the attached workspace by using az ml command. \"\"\" list_registered_component_success = self . execute_azure_cli_command ( f \"ml component list -o table\" ) if not list_registered_component_success : self . register_error ( f \"Error when listing registered components.\" ) register_all_signed_components ( self , files ) For each signed component specification file, run az ml component create , and register the status (+ register error if registration failed). Source code in shrike/build/commands/register.py def register_all_signed_components ( self , files : List [ str ]) -> None : \"\"\" For each signed component specification file, run `az ml component create`, and register the status (+ register error if registration failed). \"\"\" for component in files : register_command , stderr_is_failure = self . register_component_command ( component ) register_component_success = self . execute_azure_cli_command ( command = register_command , stderr_is_failure = stderr_is_failure , ) if register_component_success : log . info ( f \"Component { component } is registered.\" ) self . register_component_status ( component , \"register\" , \"succeeded\" ) else : self . register_component_status ( component , \"register\" , \"failed\" ) self . register_error ( f \"Error when registering component { component } .\" ) run_with_config ( self ) Running component registration logic. Source code in shrike/build/commands/register.py def run_with_config ( self ): \"\"\" Running component registration logic. \"\"\" self . telemetry_logging ( command = \"register\" ) self . validate_branch () component_path = self . find_signed_component_specification_files () for workspace_id in self . config . workspaces : log . info ( f \"Start registering signed components in { workspace_id } \" ) self . attach_workspace ( workspace_id ) log . info ( \"List of components in workspace before current registration.\" ) self . list_registered_component () self . register_all_signed_components ( files = component_path ) log . info ( \"List of components in workspace after current registration.\" ) self . list_registered_component () validate_branch ( self ) Check whether the current source branch name matches the configured regular expression of branch name. Fail if it doesn't match. Source code in shrike/build/commands/register.py def validate_branch ( self ) -> None : \"\"\" Check whether the current source branch name matches the configured regular expression of branch name. Fail if it doesn't match. \"\"\" log . info ( f \"Expected branch: { self . config . compliant_branch } \" ) log . info ( f \"Current branch: { self . config . source_branch } \" ) if re . match ( self . config . compliant_branch , self . config . source_branch ): log . info ( f \"Current branch matches configured regular expression.\" ) else : raise ValueError ( f \"Current branch name doesn't match configured name pattern.\" )","title":"register"},{"location":"build/register/#register","text":"","title":"Register"},{"location":"build/register/#shrike.build.commands.register.Register","text":"","title":"Register"},{"location":"build/register/#shrike.build.commands.register.Register.find_signed_component_specification_files","text":"Find all signed component (AML format) generated in the prepare step. Return the absolute paths of these components in the format of a list of string. Source code in shrike/build/commands/register.py def find_signed_component_specification_files ( self , dir = None ) -> List [ str ]: \"\"\" Find all signed component (AML format) generated in the `prepare` step. Return the absolute paths of these components in the format of a list of string. \"\"\" if dir is None : dir = self . config . working_directory signed_component_spec_files = [] # Find the path of spec files in the '.build' folders for spec_path in Path ( dir ) . glob ( \"**/.build/\" + Path ( self . config . component_specification_glob ) . name ): # Check whether the component is signed by examining catalog files if ( spec_path . parent . joinpath ( \"catalog.json\" ) . exists () and spec_path . parent . joinpath ( \"catalog.json.sig\" ) . exists () ): signed_component_spec_files . append ( os . path . abspath ( spec_path )) log . info ( f \"Find a signed component for AML: { spec_path } \" ) elif spec_path . parent . joinpath ( \".build.cat\" ) . exists (): log . info ( f \"Find a signed component for Aether: { spec_path } \" ) else : log . warning ( f \"Find an unsigned component: { spec_path } \" ) log . info ( str ( spec_path . parent . joinpath ( \"catalog.json\" ))) if len ( signed_component_spec_files ) == 0 : log . info ( \"Cannot find any signed components for AML.\" ) else : log . info ( f \"Find { len ( signed_component_spec_files ) } signed components for AML.\" ) return signed_component_spec_files","title":"find_signed_component_specification_files()"},{"location":"build/register/#shrike.build.commands.register.Register.list_registered_component","text":"Log all registered component in the attached workspace by using az ml command. Source code in shrike/build/commands/register.py def list_registered_component ( self ) -> None : \"\"\" Log all registered component in the attached workspace by using az ml command. \"\"\" list_registered_component_success = self . execute_azure_cli_command ( f \"ml component list -o table\" ) if not list_registered_component_success : self . register_error ( f \"Error when listing registered components.\" )","title":"list_registered_component()"},{"location":"build/register/#shrike.build.commands.register.Register.register_all_signed_components","text":"For each signed component specification file, run az ml component create , and register the status (+ register error if registration failed). Source code in shrike/build/commands/register.py def register_all_signed_components ( self , files : List [ str ]) -> None : \"\"\" For each signed component specification file, run `az ml component create`, and register the status (+ register error if registration failed). \"\"\" for component in files : register_command , stderr_is_failure = self . register_component_command ( component ) register_component_success = self . execute_azure_cli_command ( command = register_command , stderr_is_failure = stderr_is_failure , ) if register_component_success : log . info ( f \"Component { component } is registered.\" ) self . register_component_status ( component , \"register\" , \"succeeded\" ) else : self . register_component_status ( component , \"register\" , \"failed\" ) self . register_error ( f \"Error when registering component { component } .\" )","title":"register_all_signed_components()"},{"location":"build/register/#shrike.build.commands.register.Register.run_with_config","text":"Running component registration logic. Source code in shrike/build/commands/register.py def run_with_config ( self ): \"\"\" Running component registration logic. \"\"\" self . telemetry_logging ( command = \"register\" ) self . validate_branch () component_path = self . find_signed_component_specification_files () for workspace_id in self . config . workspaces : log . info ( f \"Start registering signed components in { workspace_id } \" ) self . attach_workspace ( workspace_id ) log . info ( \"List of components in workspace before current registration.\" ) self . list_registered_component () self . register_all_signed_components ( files = component_path ) log . info ( \"List of components in workspace after current registration.\" ) self . list_registered_component ()","title":"run_with_config()"},{"location":"build/register/#shrike.build.commands.register.Register.validate_branch","text":"Check whether the current source branch name matches the configured regular expression of branch name. Fail if it doesn't match. Source code in shrike/build/commands/register.py def validate_branch ( self ) -> None : \"\"\" Check whether the current source branch name matches the configured regular expression of branch name. Fail if it doesn't match. \"\"\" log . info ( f \"Expected branch: { self . config . compliant_branch } \" ) log . info ( f \"Current branch: { self . config . source_branch } \" ) if re . match ( self . config . compliant_branch , self . config . source_branch ): log . info ( f \"Current branch matches configured regular expression.\" ) else : raise ValueError ( f \"Current branch name doesn't match configured name pattern.\" )","title":"validate_branch()"},{"location":"compliant_logging/","text":"Exception Handling First execute pip install shrike to install this library. Then wrap any methods which may throw an exception with the decorator prefix_stack_trace . Here's a simple example. Your code may explicitly raise the Public* exceptions ( PublicValueError , PublicRuntimeError , PublicArgumentError , PublicKeyError , PublicTypeError ) when you know that the content of the exception does not contain any private content. The messages in these exceptions will be preserved, even if keep_message is set to False . from shrike.compliant_logging.exceptions import prefix_stack_trace @prefix_stack_trace () def main (): print ( \"Hello, world!\" ) if __name__ == \"__main__\" : main () Logging Call shrike.compliant_logging.enable_compliant_logging to set up data category-aware logging. Then continue to use standard Python logging functionality as before! Add a category=DataCategory.PUBLIC argument to have your log lines prefixed with SystemLog: . Here is a full-fledged example: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. import argparse import shrike from shrike.compliant_logging.constants import DataCategory import logging if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--prefix\" , default = \"SystemLog:\" ) parser . add_argument ( \"--log_level\" , default = \"INFO\" ) args = parser . parse_args () shrike . compliant_logging . enable_compliant_logging ( args . prefix , level = args . log_level , format = \" %(prefix)s%(levelname)s : %(name)s : %(message)s \" , ) # Output will be: # WARNING:root:private info # SystemLog:WARNING:root:public info logging . warning ( \"private info\" ) logging . warning ( \"public info\" , category = DataCategory . PUBLIC ) logger = logging . getLogger ( __name__ ) # Output will be: # SystemLog:INFO:__main__:public info # SystemLog:WARNING:__main__:public info # WARNING:__main__:private info logger . info ( \"public info\" , category = DataCategory . PUBLIC ) logger . warning ( \"public info\" , category = DataCategory . PUBLIC ) logger . warning ( \"private info\" ) Examples The simplest use case (wrap your main method in a decorator) is: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Simplest example of how to use the prefix_stack_trace decorator. \"\"\" from shrike.compliant_logging.exceptions import prefix_stack_trace # Output will be: # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\hello-world.py\", line 11, in main # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: **Exception message scrubbed** @prefix_stack_trace () def main (): print ( \"Hello, world!\" ) print ( 1 / 0 ) if __name__ == \"__main__\" : main () Prefixing stack trace Some configuration options around prefixing the stack trace. You can: - customize the prefix and the exception message - keep the original exception message (don't scrub) - pass an allow_list of strings. Exception messages will be scrubbed unless the message or the exception type regex match one of the allow_list strings. # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Demonstrate how scrubbing options. \"\"\" from shrike.compliant_logging.exceptions import prefix_stack_trace # Output will be: # # MyCustomPrefix Traceback (most recent call last): # MyCustomPrefix File \".\\prefix-stack-trace.py\", line 11, in main # MyCustomPrefix print(1 / 0) # MyCustomPrefix ZeroDivisionError: **Exception message scrubbed** @prefix_stack_trace ( prefix = \"MyCustomPrefix\" ) def custom_prefix (): print ( 1 / 0 ) # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \"/mnt/c/code/shrike/docs/samples/prefix-stack-trace.py\", line 20, in scrub2 # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: Private data was divided by zero @prefix_stack_trace ( scrub_message = \"Private data was divided by zero\" ) def custom_message (): print ( 1 / 0 ) # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \"/mnt/c/code/shrike/docs/samples/prefix-stack-trace.py\", line 24, in scrub3 # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: division by zero @prefix_stack_trace ( keep_message = True ) def keep_exception_message (): print ( 1 / 0 ) # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \"/mnt/c/code/shrike/docs/samples/prefix-stack-trace.py\", line 28, in scrub4 # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: division by zero @prefix_stack_trace ( keep_message = False , allow_list = [ \"ZeroDivision\" ]) def keep_allowed_exceptions (): print ( 1 / 0 ) # Output will be: # # SystemLog: 2020-11-12 16:56:59 Traceback (most recent call last): # SystemLog: 2020-11-12 16:56:59 File \"prefix-stack-trace.py\", line 56, in keep_allowed_exceptions # SystemLog: 2020-11-12 16:56:59 print(1 / 0) # SystemLog: 2020-11-12 16:56:59 ZeroDivisionError: **Exception message scrubbed** @prefix_stack_trace ( add_timestamp = True ) def add_timestamp (): print ( 1 / 0 ) if __name__ == \"__main__\" : try : custom_prefix () except : pass try : custom_message () except : pass try : keep_exception_message () except : pass try : keep_allowed_exceptions () except : pass try : add_timestamp () except : pass With statements Use this library with with statements: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Simple script with examples of how to use \"with statements\" to capture information about failed module imports. \"\"\" from shrike.compliant_logging.exceptions import PrefixStackTrace # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\docs\\logging\\with-statement.py\", line 11, in <module> # SystemLog: import my_custom_library # noqa: F401 # SystemLog: ModuleNotFoundError: **Exception message scrubbed** with PrefixStackTrace (): # Import statement which could raise an exception containing sensitive # data. import my_custom_library # noqa: F401 # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\docs\\logging\\with-statement.py\", line 22, in <module> # SystemLog: import another_custom_library # noqa: F401 # SystemLog: ModuleNotFoundError: No module named 'another_custom_library' with PrefixStackTrace ( keep_message = True ): # Import statement which will never raise an exception containing sensitive # data. import another_custom_library # noqa: F401 Directly with try / except statements Using this library directly inside try / except statements: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Simple script with examples of how to directly use the function print_prefixed_stack_trace to capture information about failed module imports. \"\"\" from shrike.compliant_logging.exceptions import print_prefixed_stack_trace_and_raise try : # Import statement which could raise an exception containing sensitive # data. import my_custom_library # noqa: F401 except BaseException as e : # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\try-except.py\", line 10, in <module> # SystemLog: import my_custom_library # SystemLog: ModuleNotFoundError: **Exception message scrubbed** print_prefixed_stack_trace_and_raise ( err = e ) try : # Import statement which will never raise an exception containing sensitive # data. import another_custom_library # noqa: F401 except BaseException as e : # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\try-except.py\", line 17, in <module> # SystemLog: import another_custom_library # SystemLog: ModuleNotFoundError: No module named 'another_custom_library' print_prefixed_stack_trace_and_raise ( err = e , keep_message = True ) Public exception types Using the Public* exception types: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Sample use of Public* exception types. \"\"\" from shrike.compliant_logging.exceptions import prefix_stack_trace , PublicValueError def divide ( a , b ): if not b : raise PublicValueError ( \"Second argument cannot be null or zero.\" ) return a / b # Output will be: # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\docs\\logging\\public-exceptions.py\", line 24, in main # SystemLog: divide(1, 0) # SystemLog: File \".\\docs\\logging\\public-exceptions.py\", line 13, in divide # SystemLog: raise PublicValueError(\"Second argument cannot be null or zero.\") # SystemLog: compliant_logging.exceptions.PublicValueError: SystemLog:Second argument cannot be null or zero. @prefix_stack_trace () def main (): divide ( 1 , 0 ) if __name__ == \"__main__\" : main () Exception or Stack trace parsing The stack_trace_extractor namespace contains simple tools to grab Python or C# stack traces and exceptions from log files. Sometimes the file that has the stack trace you need may also contain sensitive data. Use this tool to parse and print the stack trace, exception type and optionally exception message (careful as exception messages may also potentially hold private data.) from compliant_ml_utils.stack_trace_extractor import StacktraceExtractor extractor = StacktraceExtractor () extractor . extract ( \"log_file\" )","title":"Logging examples"},{"location":"compliant_logging/#exception-handling","text":"First execute pip install shrike to install this library. Then wrap any methods which may throw an exception with the decorator prefix_stack_trace . Here's a simple example. Your code may explicitly raise the Public* exceptions ( PublicValueError , PublicRuntimeError , PublicArgumentError , PublicKeyError , PublicTypeError ) when you know that the content of the exception does not contain any private content. The messages in these exceptions will be preserved, even if keep_message is set to False . from shrike.compliant_logging.exceptions import prefix_stack_trace @prefix_stack_trace () def main (): print ( \"Hello, world!\" ) if __name__ == \"__main__\" : main ()","title":"Exception Handling"},{"location":"compliant_logging/#logging","text":"Call shrike.compliant_logging.enable_compliant_logging to set up data category-aware logging. Then continue to use standard Python logging functionality as before! Add a category=DataCategory.PUBLIC argument to have your log lines prefixed with SystemLog: . Here is a full-fledged example: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. import argparse import shrike from shrike.compliant_logging.constants import DataCategory import logging if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--prefix\" , default = \"SystemLog:\" ) parser . add_argument ( \"--log_level\" , default = \"INFO\" ) args = parser . parse_args () shrike . compliant_logging . enable_compliant_logging ( args . prefix , level = args . log_level , format = \" %(prefix)s%(levelname)s : %(name)s : %(message)s \" , ) # Output will be: # WARNING:root:private info # SystemLog:WARNING:root:public info logging . warning ( \"private info\" ) logging . warning ( \"public info\" , category = DataCategory . PUBLIC ) logger = logging . getLogger ( __name__ ) # Output will be: # SystemLog:INFO:__main__:public info # SystemLog:WARNING:__main__:public info # WARNING:__main__:private info logger . info ( \"public info\" , category = DataCategory . PUBLIC ) logger . warning ( \"public info\" , category = DataCategory . PUBLIC ) logger . warning ( \"private info\" )","title":"Logging"},{"location":"compliant_logging/#examples","text":"The simplest use case (wrap your main method in a decorator) is: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Simplest example of how to use the prefix_stack_trace decorator. \"\"\" from shrike.compliant_logging.exceptions import prefix_stack_trace # Output will be: # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\hello-world.py\", line 11, in main # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: **Exception message scrubbed** @prefix_stack_trace () def main (): print ( \"Hello, world!\" ) print ( 1 / 0 ) if __name__ == \"__main__\" : main ()","title":"Examples"},{"location":"compliant_logging/#prefixing-stack-trace","text":"Some configuration options around prefixing the stack trace. You can: - customize the prefix and the exception message - keep the original exception message (don't scrub) - pass an allow_list of strings. Exception messages will be scrubbed unless the message or the exception type regex match one of the allow_list strings. # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Demonstrate how scrubbing options. \"\"\" from shrike.compliant_logging.exceptions import prefix_stack_trace # Output will be: # # MyCustomPrefix Traceback (most recent call last): # MyCustomPrefix File \".\\prefix-stack-trace.py\", line 11, in main # MyCustomPrefix print(1 / 0) # MyCustomPrefix ZeroDivisionError: **Exception message scrubbed** @prefix_stack_trace ( prefix = \"MyCustomPrefix\" ) def custom_prefix (): print ( 1 / 0 ) # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \"/mnt/c/code/shrike/docs/samples/prefix-stack-trace.py\", line 20, in scrub2 # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: Private data was divided by zero @prefix_stack_trace ( scrub_message = \"Private data was divided by zero\" ) def custom_message (): print ( 1 / 0 ) # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \"/mnt/c/code/shrike/docs/samples/prefix-stack-trace.py\", line 24, in scrub3 # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: division by zero @prefix_stack_trace ( keep_message = True ) def keep_exception_message (): print ( 1 / 0 ) # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \"/mnt/c/code/shrike/docs/samples/prefix-stack-trace.py\", line 28, in scrub4 # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: division by zero @prefix_stack_trace ( keep_message = False , allow_list = [ \"ZeroDivision\" ]) def keep_allowed_exceptions (): print ( 1 / 0 ) # Output will be: # # SystemLog: 2020-11-12 16:56:59 Traceback (most recent call last): # SystemLog: 2020-11-12 16:56:59 File \"prefix-stack-trace.py\", line 56, in keep_allowed_exceptions # SystemLog: 2020-11-12 16:56:59 print(1 / 0) # SystemLog: 2020-11-12 16:56:59 ZeroDivisionError: **Exception message scrubbed** @prefix_stack_trace ( add_timestamp = True ) def add_timestamp (): print ( 1 / 0 ) if __name__ == \"__main__\" : try : custom_prefix () except : pass try : custom_message () except : pass try : keep_exception_message () except : pass try : keep_allowed_exceptions () except : pass try : add_timestamp () except : pass","title":"Prefixing stack trace"},{"location":"compliant_logging/#with-statements","text":"Use this library with with statements: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Simple script with examples of how to use \"with statements\" to capture information about failed module imports. \"\"\" from shrike.compliant_logging.exceptions import PrefixStackTrace # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\docs\\logging\\with-statement.py\", line 11, in <module> # SystemLog: import my_custom_library # noqa: F401 # SystemLog: ModuleNotFoundError: **Exception message scrubbed** with PrefixStackTrace (): # Import statement which could raise an exception containing sensitive # data. import my_custom_library # noqa: F401 # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\docs\\logging\\with-statement.py\", line 22, in <module> # SystemLog: import another_custom_library # noqa: F401 # SystemLog: ModuleNotFoundError: No module named 'another_custom_library' with PrefixStackTrace ( keep_message = True ): # Import statement which will never raise an exception containing sensitive # data. import another_custom_library # noqa: F401","title":"With statements"},{"location":"compliant_logging/#directly-with-try-except-statements","text":"Using this library directly inside try / except statements: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Simple script with examples of how to directly use the function print_prefixed_stack_trace to capture information about failed module imports. \"\"\" from shrike.compliant_logging.exceptions import print_prefixed_stack_trace_and_raise try : # Import statement which could raise an exception containing sensitive # data. import my_custom_library # noqa: F401 except BaseException as e : # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\try-except.py\", line 10, in <module> # SystemLog: import my_custom_library # SystemLog: ModuleNotFoundError: **Exception message scrubbed** print_prefixed_stack_trace_and_raise ( err = e ) try : # Import statement which will never raise an exception containing sensitive # data. import another_custom_library # noqa: F401 except BaseException as e : # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\try-except.py\", line 17, in <module> # SystemLog: import another_custom_library # SystemLog: ModuleNotFoundError: No module named 'another_custom_library' print_prefixed_stack_trace_and_raise ( err = e , keep_message = True )","title":"Directly with try / except statements"},{"location":"compliant_logging/#public-exception-types","text":"Using the Public* exception types: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Sample use of Public* exception types. \"\"\" from shrike.compliant_logging.exceptions import prefix_stack_trace , PublicValueError def divide ( a , b ): if not b : raise PublicValueError ( \"Second argument cannot be null or zero.\" ) return a / b # Output will be: # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\docs\\logging\\public-exceptions.py\", line 24, in main # SystemLog: divide(1, 0) # SystemLog: File \".\\docs\\logging\\public-exceptions.py\", line 13, in divide # SystemLog: raise PublicValueError(\"Second argument cannot be null or zero.\") # SystemLog: compliant_logging.exceptions.PublicValueError: SystemLog:Second argument cannot be null or zero. @prefix_stack_trace () def main (): divide ( 1 , 0 ) if __name__ == \"__main__\" : main ()","title":"Public exception types"},{"location":"compliant_logging/#exception-or-stack-trace-parsing","text":"The stack_trace_extractor namespace contains simple tools to grab Python or C# stack traces and exceptions from log files. Sometimes the file that has the stack trace you need may also contain sensitive data. Use this tool to parse and print the stack trace, exception type and optionally exception message (careful as exception messages may also potentially hold private data.) from compliant_ml_utils.stack_trace_extractor import StacktraceExtractor extractor = StacktraceExtractor () extractor . extract ( \"log_file\" )","title":"Exception or Stack trace parsing"},{"location":"compliant_logging/constants/","text":"Constants Constant values used by this library. DataCategory Enumeration of data categories in compliant machine learning. Values: - PRIVATE: data which is private. Researchers may not view this. - PUBLIC: data which may safely be viewed by researchers.","title":"constants"},{"location":"compliant_logging/constants/#constants","text":"Constant values used by this library.","title":"Constants"},{"location":"compliant_logging/constants/#shrike.compliant_logging.constants.DataCategory","text":"Enumeration of data categories in compliant machine learning. Values: - PRIVATE: data which is private. Researchers may not view this. - PUBLIC: data which may safely be viewed by researchers.","title":"DataCategory"},{"location":"compliant_logging/exceptions/","text":"Exceptions Decorators and utilities for prefixing exception stack traces while obscuring the exception message itself. PublicArgumentError Argument error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicFileNotFoundError File not found error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicIndexError Index error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicIOError I/O error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicKeyError Key error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicNotImplementedError Not implemented error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicRuntimeError Runtime error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicTypeError Type error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicValueError Value error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. is_exception_allowed ( exception , allow_list ) Check if message is allowed, either by allow_list , or default_allow_list . Parameters: Name Type Description Default exception Union[BaseException, traceback.TracebackException] the exception to test required allow_list list list of regex expressions. If any expression matches the exception name or message, it will be considered allowed. required Returns: Type Description bool bool: True if message is allowed, False otherwise. Source code in shrike/compliant_logging/exceptions.py def is_exception_allowed ( exception : Union [ BaseException , TracebackException ], allow_list : list ) -> bool : \"\"\" Check if message is allowed, either by `allow_list`, or `default_allow_list`. Args: exception (TracebackException): the exception to test allow_list (list): list of regex expressions. If any expression matches the exception name or message, it will be considered allowed. Returns: bool: True if message is allowed, False otherwise. \"\"\" if not isinstance ( exception , TracebackException ): exception = TracebackException . from_exception ( exception ) # empty list means all messages are allowed for expr in allow_list + default_allow_list : if re . search ( expr , getattr ( exception , \"_str\" , \"\" ), re . IGNORECASE ): return True if re . search ( expr , getattr ( exception . exc_type , \"__name__\" , \"\" ), re . IGNORECASE ): return True return False prefix_stack_trace ( file =< _io . TextIOWrapper name = '<stderr>' mode = 'w' encoding = 'UTF-8' > , disable = False , prefix = 'SystemLog:' , scrub_message = '**Exception message scrubbed**' , keep_message = False , allow_list = [], add_timestamp = False ) Decorator which wraps the decorated function and prints the stack trace of exceptions which occur, prefixed with prefix and with exception messages scrubbed (replaced with scrub_message ). To use this, just add @prefix_stack_trace() above your function definition, e.g. @prefix_stack_trace() def foo(x): pass Source code in shrike/compliant_logging/exceptions.py def prefix_stack_trace ( file : TextIO = sys . stderr , disable : bool = bool ( sys . flags . debug ), prefix : str = PREFIX , scrub_message : str = SCRUB_MESSAGE , keep_message : bool = False , allow_list : list = [], add_timestamp : bool = False , ) -> Callable : \"\"\" Decorator which wraps the decorated function and prints the stack trace of exceptions which occur, prefixed with `prefix` and with exception messages scrubbed (replaced with `scrub_message`). To use this, just add `@prefix_stack_trace()` above your function definition, e.g. @prefix_stack_trace() def foo(x): pass \"\"\" return _PrefixStackTraceWrapper ( file , disable , prefix , scrub_message , keep_message , allow_list , add_timestamp ) print_prefixed_stack_trace_and_raise ( file =< _io . TextIOWrapper name = '<stderr>' mode = 'w' encoding = 'UTF-8' > , prefix = 'SystemLog:' , scrub_message = '**Exception message scrubbed**' , keep_message = False , allow_list = [], add_timestamp = False , err = None ) Print the current exception and stack trace to file (usually client standard error), prefixing the stack trace with prefix . Parameters: Name Type Description Default keep_message bool if True, don't scrub message. If false, scrub (unless allowed). False allow_list list exception allow_list. Ignored if keep_message is True. If empty all messages will be srubbed. [] err Optional[BaseException] the error that was thrown. None accepted for backwards compatibility. None Source code in shrike/compliant_logging/exceptions.py def print_prefixed_stack_trace_and_raise ( file : TextIO = sys . stderr , prefix : str = PREFIX , scrub_message : str = SCRUB_MESSAGE , keep_message : bool = False , allow_list : list = [], add_timestamp : bool = False , err : Optional [ BaseException ] = None , ) -> None : \"\"\" Print the current exception and stack trace to `file` (usually client standard error), prefixing the stack trace with `prefix`. Args: keep_message (bool): if True, don't scrub message. If false, scrub (unless allowed). allow_list (list): exception allow_list. Ignored if keep_message is True. If empty all messages will be srubbed. err: the error that was thrown. None accepted for backwards compatibility. \"\"\" if err is None : err = sys . exc_info ()[ 1 ] scrubbed_err = scrub_exception ( err , scrub_message , prefix , keep_message , allow_list ) tb_exception = TracebackException . from_exception ( scrubbed_err ) # type: ignore for execution in tb_exception . format (): if \"return function(*func_args, **func_kwargs)\" in execution : # Do not show the stack trace for our decorator. continue for line in execution . splitlines (): if add_timestamp : current_time = time . strftime ( \"%Y-%m- %d %H:%M:%S\" , time . localtime ()) print ( f \" { prefix } { current_time } { line } \" , file = file ) else : print ( f \" { prefix } { line } \" , file = file ) raise scrubbed_err # type: ignore scrub_exception ( exception , scrub_message , prefix , keep_message , allow_list , _seen = None ) Recursively scrub all potentially private data from an exception, using the logic in _attribute_transformer . Inspired by Dan Schwartz's closed-source implementation: https://dev.azure.com/eemo/TEE/_git/TEEGit?path=%2FOffline%2FFocusedInbox%2FComTriage%2Fcomtriage%2Futils%2Fscrubber.py&version=GBcompliant%2FComTriage&_a=content which is closely based on the CPython implementation of the TracebackException class: https://github.com/python/cpython/blob/master/Lib/traceback.py#L478 Source code in shrike/compliant_logging/exceptions.py def scrub_exception ( exception : Optional [ BaseException ], scrub_message : str , prefix : str , keep_message : bool , allow_list : list , _seen : Optional [ Set [ int ]] = None , ) -> Optional [ BaseException ]: \"\"\" Recursively scrub all potentially private data from an exception, using the logic in `_attribute_transformer`. Inspired by Dan Schwartz's closed-source implementation: https://dev.azure.com/eemo/TEE/_git/TEEGit?path=%2FOffline%2FFocusedInbox%2FComTriage%2Fcomtriage%2Futils%2Fscrubber.py&version=GBcompliant%2FComTriage&_a=content which is closely based on the CPython implementation of the TracebackException class: https://github.com/python/cpython/blob/master/Lib/traceback.py#L478 \"\"\" if not exception : return None # Handle loops in __cause__ or __context__ . if _seen is None : _seen = set () _seen . add ( id ( exception )) # Gracefully handle being called with no type or value. if exception . __cause__ is not None and id ( exception . __cause__ ) not in _seen : exception . __cause__ = scrub_exception ( exception . __cause__ , scrub_message , prefix , keep_message , allow_list , _seen , ) if exception . __context__ is not None and id ( exception . __context__ ) not in _seen : exception . __context__ = scrub_exception ( exception . __context__ , scrub_message , prefix , keep_message , allow_list , _seen , ) keep = keep_message or is_exception_allowed ( exception , allow_list ) transformer = _attribute_transformer ( prefix , scrub_message , keep ) for attr in dir ( exception ): if attr and not attr . startswith ( \"__\" ): try : value = getattr ( exception , attr ) except AttributeError : # In some cases, e.g. FileNotFoundError, there are attributes # which show up in dir(e), but for which an AttributeError is # thrown when attempting to access the value. See, e.g.: # https://stackoverflow.com/q/47775772 . continue try : # If unable to transform or set the attribute, replace the # entire exception since the attribute value is readable, but # we are unable to scrub it. new_value = transformer ( value ) setattr ( exception , attr , new_value ) except BaseException as e : new_exception = PublicRuntimeError ( f \" { prefix } Obtained { type ( e ) . __name__ } when trying to scrub { attr } from { type ( exception ) . __name__ } \" # noqa: E501 ) new_exception . __cause__ = exception . __cause__ new_exception . __context__ = exception . __context__ exception = new_exception break return exception","title":"exceptions"},{"location":"compliant_logging/exceptions/#exceptions","text":"Decorators and utilities for prefixing exception stack traces while obscuring the exception message itself.","title":"Exceptions"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicArgumentError","text":"Argument error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicArgumentError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicFileNotFoundError","text":"File not found error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicFileNotFoundError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicIndexError","text":"Index error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicIndexError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicIOError","text":"I/O error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicIOError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicKeyError","text":"Key error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicKeyError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicNotImplementedError","text":"Not implemented error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicNotImplementedError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicRuntimeError","text":"Runtime error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicRuntimeError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicTypeError","text":"Type error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicTypeError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicValueError","text":"Value error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicValueError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.is_exception_allowed","text":"Check if message is allowed, either by allow_list , or default_allow_list . Parameters: Name Type Description Default exception Union[BaseException, traceback.TracebackException] the exception to test required allow_list list list of regex expressions. If any expression matches the exception name or message, it will be considered allowed. required Returns: Type Description bool bool: True if message is allowed, False otherwise. Source code in shrike/compliant_logging/exceptions.py def is_exception_allowed ( exception : Union [ BaseException , TracebackException ], allow_list : list ) -> bool : \"\"\" Check if message is allowed, either by `allow_list`, or `default_allow_list`. Args: exception (TracebackException): the exception to test allow_list (list): list of regex expressions. If any expression matches the exception name or message, it will be considered allowed. Returns: bool: True if message is allowed, False otherwise. \"\"\" if not isinstance ( exception , TracebackException ): exception = TracebackException . from_exception ( exception ) # empty list means all messages are allowed for expr in allow_list + default_allow_list : if re . search ( expr , getattr ( exception , \"_str\" , \"\" ), re . IGNORECASE ): return True if re . search ( expr , getattr ( exception . exc_type , \"__name__\" , \"\" ), re . IGNORECASE ): return True return False","title":"is_exception_allowed()"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.prefix_stack_trace","text":"Decorator which wraps the decorated function and prints the stack trace of exceptions which occur, prefixed with prefix and with exception messages scrubbed (replaced with scrub_message ). To use this, just add @prefix_stack_trace() above your function definition, e.g. @prefix_stack_trace() def foo(x): pass Source code in shrike/compliant_logging/exceptions.py def prefix_stack_trace ( file : TextIO = sys . stderr , disable : bool = bool ( sys . flags . debug ), prefix : str = PREFIX , scrub_message : str = SCRUB_MESSAGE , keep_message : bool = False , allow_list : list = [], add_timestamp : bool = False , ) -> Callable : \"\"\" Decorator which wraps the decorated function and prints the stack trace of exceptions which occur, prefixed with `prefix` and with exception messages scrubbed (replaced with `scrub_message`). To use this, just add `@prefix_stack_trace()` above your function definition, e.g. @prefix_stack_trace() def foo(x): pass \"\"\" return _PrefixStackTraceWrapper ( file , disable , prefix , scrub_message , keep_message , allow_list , add_timestamp )","title":"prefix_stack_trace()"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.print_prefixed_stack_trace_and_raise","text":"Print the current exception and stack trace to file (usually client standard error), prefixing the stack trace with prefix . Parameters: Name Type Description Default keep_message bool if True, don't scrub message. If false, scrub (unless allowed). False allow_list list exception allow_list. Ignored if keep_message is True. If empty all messages will be srubbed. [] err Optional[BaseException] the error that was thrown. None accepted for backwards compatibility. None Source code in shrike/compliant_logging/exceptions.py def print_prefixed_stack_trace_and_raise ( file : TextIO = sys . stderr , prefix : str = PREFIX , scrub_message : str = SCRUB_MESSAGE , keep_message : bool = False , allow_list : list = [], add_timestamp : bool = False , err : Optional [ BaseException ] = None , ) -> None : \"\"\" Print the current exception and stack trace to `file` (usually client standard error), prefixing the stack trace with `prefix`. Args: keep_message (bool): if True, don't scrub message. If false, scrub (unless allowed). allow_list (list): exception allow_list. Ignored if keep_message is True. If empty all messages will be srubbed. err: the error that was thrown. None accepted for backwards compatibility. \"\"\" if err is None : err = sys . exc_info ()[ 1 ] scrubbed_err = scrub_exception ( err , scrub_message , prefix , keep_message , allow_list ) tb_exception = TracebackException . from_exception ( scrubbed_err ) # type: ignore for execution in tb_exception . format (): if \"return function(*func_args, **func_kwargs)\" in execution : # Do not show the stack trace for our decorator. continue for line in execution . splitlines (): if add_timestamp : current_time = time . strftime ( \"%Y-%m- %d %H:%M:%S\" , time . localtime ()) print ( f \" { prefix } { current_time } { line } \" , file = file ) else : print ( f \" { prefix } { line } \" , file = file ) raise scrubbed_err # type: ignore","title":"print_prefixed_stack_trace_and_raise()"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.scrub_exception","text":"Recursively scrub all potentially private data from an exception, using the logic in _attribute_transformer . Inspired by Dan Schwartz's closed-source implementation: https://dev.azure.com/eemo/TEE/_git/TEEGit?path=%2FOffline%2FFocusedInbox%2FComTriage%2Fcomtriage%2Futils%2Fscrubber.py&version=GBcompliant%2FComTriage&_a=content which is closely based on the CPython implementation of the TracebackException class: https://github.com/python/cpython/blob/master/Lib/traceback.py#L478 Source code in shrike/compliant_logging/exceptions.py def scrub_exception ( exception : Optional [ BaseException ], scrub_message : str , prefix : str , keep_message : bool , allow_list : list , _seen : Optional [ Set [ int ]] = None , ) -> Optional [ BaseException ]: \"\"\" Recursively scrub all potentially private data from an exception, using the logic in `_attribute_transformer`. Inspired by Dan Schwartz's closed-source implementation: https://dev.azure.com/eemo/TEE/_git/TEEGit?path=%2FOffline%2FFocusedInbox%2FComTriage%2Fcomtriage%2Futils%2Fscrubber.py&version=GBcompliant%2FComTriage&_a=content which is closely based on the CPython implementation of the TracebackException class: https://github.com/python/cpython/blob/master/Lib/traceback.py#L478 \"\"\" if not exception : return None # Handle loops in __cause__ or __context__ . if _seen is None : _seen = set () _seen . add ( id ( exception )) # Gracefully handle being called with no type or value. if exception . __cause__ is not None and id ( exception . __cause__ ) not in _seen : exception . __cause__ = scrub_exception ( exception . __cause__ , scrub_message , prefix , keep_message , allow_list , _seen , ) if exception . __context__ is not None and id ( exception . __context__ ) not in _seen : exception . __context__ = scrub_exception ( exception . __context__ , scrub_message , prefix , keep_message , allow_list , _seen , ) keep = keep_message or is_exception_allowed ( exception , allow_list ) transformer = _attribute_transformer ( prefix , scrub_message , keep ) for attr in dir ( exception ): if attr and not attr . startswith ( \"__\" ): try : value = getattr ( exception , attr ) except AttributeError : # In some cases, e.g. FileNotFoundError, there are attributes # which show up in dir(e), but for which an AttributeError is # thrown when attempting to access the value. See, e.g.: # https://stackoverflow.com/q/47775772 . continue try : # If unable to transform or set the attribute, replace the # entire exception since the attribute value is readable, but # we are unable to scrub it. new_value = transformer ( value ) setattr ( exception , attr , new_value ) except BaseException as e : new_exception = PublicRuntimeError ( f \" { prefix } Obtained { type ( e ) . __name__ } when trying to scrub { attr } from { type ( exception ) . __name__ } \" # noqa: E501 ) new_exception . __cause__ = exception . __cause__ new_exception . __context__ = exception . __context__ exception = new_exception break return exception","title":"scrub_exception()"},{"location":"compliant_logging/logging/","text":"Logging Utilities around logging data which may or may not contain private content. CompliantLogger Subclass of the default logging class with an explicit category parameter on all logging methods. It will pass an extra param with prefix key (value depending on whether category is public or private) to the handlers. The default value for data category is PRIVATE for all methods. Implementation is inspired by: https://github.com/python/cpython/blob/3.8/Lib/logging/ init .py enable_compliant_logging ( prefix = 'SystemLog:' , ** kwargs ) The default format is logging.BASIC_FORMAT ( %(levelname)s:%(name)s:%(message)s ). All other kwargs are passed to logging.basicConfig . Sets the default logger class and root logger to be compliant. This means the format string %(prefix) will work. Set the format using the format kwarg. If running in Python >= 3.8, will attempt to add force=True to the kwargs for logging.basicConfig. After calling this method, use the kwarg category to pass in a value of DataCategory to denote data category. The default is PRIVATE . That is, if no changes are made to an existing set of log statements, the log output should be the same. The standard implementation of the logging API is a good reference: https://github.com/python/cpython/blob/3.9/Lib/logging/ init .py Source code in shrike/compliant_logging/logging.py def enable_compliant_logging ( prefix : str = \"SystemLog:\" , ** kwargs ) -> None : \"\"\" The default format is `logging.BASIC_FORMAT` (`%(levelname)s:%(name)s:%(message)s`). All other kwargs are passed to `logging.basicConfig`. Sets the default logger class and root logger to be compliant. This means the format string `%(prefix)` will work. Set the format using the `format` kwarg. If running in Python >= 3.8, will attempt to add `force=True` to the kwargs for logging.basicConfig. After calling this method, use the kwarg `category` to pass in a value of `DataCategory` to denote data category. The default is `PRIVATE`. That is, if no changes are made to an existing set of log statements, the log output should be the same. The standard implementation of the logging API is a good reference: https://github.com/python/cpython/blob/3.9/Lib/logging/__init__.py \"\"\" set_prefix ( prefix ) if \"format\" not in kwargs : kwargs [ \"format\" ] = f \"%(prefix)s { logging . BASIC_FORMAT } \" # Ensure that all loggers created via `logging.getLogger` are instances of # the `CompliantLogger` class. logging . setLoggerClass ( CompliantLogger ) if len ( logging . root . handlers ) > 0 : p = get_prefix () for line in _logging_basic_config_set_warning . splitlines (): print ( f \" { p }{ line } \" , file = sys . stderr ) if \"force\" not in kwargs and sys . version_info >= ( 3 , 8 ): kwargs [ \"force\" ] = True old_root = logging . root root = CompliantLogger ( logging . root . name ) root . handlers = old_root . handlers logging . root = root logging . Logger . root = root # type: ignore logging . Logger . manager = logging . Manager ( root ) # type: ignore # https://github.com/kivy/kivy/issues/6733 logging . basicConfig ( ** kwargs ) enable_confidential_logging ( prefix = 'SystemLog:' , ** kwargs ) This function is a duplicate of the function enable_compliant_logging . We encourage users to use enable_compliant_logging . Source code in shrike/compliant_logging/logging.py def enable_confidential_logging ( prefix : str = \"SystemLog:\" , ** kwargs ) -> None : \"\"\" This function is a duplicate of the function `enable_compliant_logging`. We encourage users to use `enable_compliant_logging`. \"\"\" print ( f \" { prefix } The function enable_confidential_logging() is on the way\" \" to deprecation. Please use enable_compliant_logging() instead.\" , file = sys . stderr , ) enable_compliant_logging ( prefix , ** kwargs ) get_prefix () Obtain the current global prefix to use when logging public (non-private) data. Source code in shrike/compliant_logging/logging.py def get_prefix () -> Optional [ str ]: \"\"\" Obtain the current global prefix to use when logging public (non-private) data. \"\"\" return _PREFIX set_prefix ( prefix ) Set the global prefix to use when logging public (non-private) data. This method is thread-safe. Source code in shrike/compliant_logging/logging.py def set_prefix ( prefix : str ) -> None : \"\"\" Set the global prefix to use when logging public (non-private) data. This method is thread-safe. \"\"\" with _LOCK : global _PREFIX _PREFIX = prefix","title":"logging"},{"location":"compliant_logging/logging/#logging","text":"Utilities around logging data which may or may not contain private content.","title":"Logging"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.CompliantLogger","text":"Subclass of the default logging class with an explicit category parameter on all logging methods. It will pass an extra param with prefix key (value depending on whether category is public or private) to the handlers. The default value for data category is PRIVATE for all methods. Implementation is inspired by: https://github.com/python/cpython/blob/3.8/Lib/logging/ init .py","title":"CompliantLogger"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.enable_compliant_logging","text":"The default format is logging.BASIC_FORMAT ( %(levelname)s:%(name)s:%(message)s ). All other kwargs are passed to logging.basicConfig . Sets the default logger class and root logger to be compliant. This means the format string %(prefix) will work. Set the format using the format kwarg. If running in Python >= 3.8, will attempt to add force=True to the kwargs for logging.basicConfig. After calling this method, use the kwarg category to pass in a value of DataCategory to denote data category. The default is PRIVATE . That is, if no changes are made to an existing set of log statements, the log output should be the same. The standard implementation of the logging API is a good reference: https://github.com/python/cpython/blob/3.9/Lib/logging/ init .py Source code in shrike/compliant_logging/logging.py def enable_compliant_logging ( prefix : str = \"SystemLog:\" , ** kwargs ) -> None : \"\"\" The default format is `logging.BASIC_FORMAT` (`%(levelname)s:%(name)s:%(message)s`). All other kwargs are passed to `logging.basicConfig`. Sets the default logger class and root logger to be compliant. This means the format string `%(prefix)` will work. Set the format using the `format` kwarg. If running in Python >= 3.8, will attempt to add `force=True` to the kwargs for logging.basicConfig. After calling this method, use the kwarg `category` to pass in a value of `DataCategory` to denote data category. The default is `PRIVATE`. That is, if no changes are made to an existing set of log statements, the log output should be the same. The standard implementation of the logging API is a good reference: https://github.com/python/cpython/blob/3.9/Lib/logging/__init__.py \"\"\" set_prefix ( prefix ) if \"format\" not in kwargs : kwargs [ \"format\" ] = f \"%(prefix)s { logging . BASIC_FORMAT } \" # Ensure that all loggers created via `logging.getLogger` are instances of # the `CompliantLogger` class. logging . setLoggerClass ( CompliantLogger ) if len ( logging . root . handlers ) > 0 : p = get_prefix () for line in _logging_basic_config_set_warning . splitlines (): print ( f \" { p }{ line } \" , file = sys . stderr ) if \"force\" not in kwargs and sys . version_info >= ( 3 , 8 ): kwargs [ \"force\" ] = True old_root = logging . root root = CompliantLogger ( logging . root . name ) root . handlers = old_root . handlers logging . root = root logging . Logger . root = root # type: ignore logging . Logger . manager = logging . Manager ( root ) # type: ignore # https://github.com/kivy/kivy/issues/6733 logging . basicConfig ( ** kwargs )","title":"enable_compliant_logging()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.enable_confidential_logging","text":"This function is a duplicate of the function enable_compliant_logging . We encourage users to use enable_compliant_logging . Source code in shrike/compliant_logging/logging.py def enable_confidential_logging ( prefix : str = \"SystemLog:\" , ** kwargs ) -> None : \"\"\" This function is a duplicate of the function `enable_compliant_logging`. We encourage users to use `enable_compliant_logging`. \"\"\" print ( f \" { prefix } The function enable_confidential_logging() is on the way\" \" to deprecation. Please use enable_compliant_logging() instead.\" , file = sys . stderr , ) enable_compliant_logging ( prefix , ** kwargs )","title":"enable_confidential_logging()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.get_prefix","text":"Obtain the current global prefix to use when logging public (non-private) data. Source code in shrike/compliant_logging/logging.py def get_prefix () -> Optional [ str ]: \"\"\" Obtain the current global prefix to use when logging public (non-private) data. \"\"\" return _PREFIX","title":"get_prefix()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.set_prefix","text":"Set the global prefix to use when logging public (non-private) data. This method is thread-safe. Source code in shrike/compliant_logging/logging.py def set_prefix ( prefix : str ) -> None : \"\"\" Set the global prefix to use when logging public (non-private) data. This method is thread-safe. \"\"\" with _LOCK : global _PREFIX _PREFIX = prefix","title":"set_prefix()"},{"location":"compliant_logging/rfc-logging/","text":"Logging in Compliant ML Owner Approvers Participants Daniel Miller Jeff Omhover AML DS Team In many corporations, data scientists are working to build and train machine learning models under extremely strict compliance and privacy requirements. These can include: Being unable to directly access or view the customer data used to train a model. Being unable to run unsigned code against customer data, except possibly in specialized compute clusters with no network access and other hardening in place. All training logs removed, except possibly those starting with some fixed prefix (e.g., SystemLog: ). Machine learning is already a hard problem \u2014 building and training models under these constraints is even more difficult. This RFC ( R equest F or C omment) proposes the code patterns and expected behavior of a to-be-written logging utility for compliant machine learning. It begins by outlining the requirements this utility would need to satisfy, then gives a concrete proposal with sample code. It proceeds to outline some alternatives, along with known risks of the proposal. The following topics are out of scope for this RFC: How to implement the proposed behavior. It considers only usage patterns and the intended behavior of the logging utility library. Compliant argument parsing. Compliant exception handling (how to keep and prefix the stack trace and exception type, while scrubbing exception messages). Languages besides Python. Requirements Every call to log a message should be forced to include information on whether the message is \"safe\" (does not contain any customer or sensitive data). If the message is safe, the log line should be mutated to contain a configurable prefix (e.g., SystemLog: ). The default behavior of this library should be to not add the \"don't scrub\" prefix. It should only add that prefix if a conscious decision has been made, possibly by choosing a different method name or parameter value. This library should not rely on users naming variables \"well\". That is, if a user accidentally names a logger safeLogger , there should be no data leak if that logger was not pre-configured on some special way. Code which consumes this library should have the same \"look and feel\" as code consuming the Python standard library's logging module. :warning: This entire document relies on the assumption that the filtering mechanism looks for a specific prefix in log lines. Proposal I propose that the existing logging paradigm in Python be mostly unchanged, with the following small differences: Calls to getLogger must include a safe_prefix parameter. Open question: should this be more flexible, e.g. a callable or dictionary mapping data categories to prefixes or format strings? Calls to logger.* have a new optional parameter taking enum values. The default value is CONTAINS_PRIVATE_DATA , i.e. if this parameter is not explicitly provided the \"safe\" prefix will not be added. Here is some sample code following this proposal. import argparse from shrike.compliant_logging import logging from shrike.compliant_logging.constants import DataCategory def main ( logger ): # HAS a prefix. logger . warning ( 'safe message' , category = DataCategory . ONLY_PUBLIC_DATA ) # DOES NOT have a prefix. logger . warning ( 'unsafe message' ) if __name__ == '__main__' : parser = argparse . ArgumentParser () parser . add_argument ( '--prefix' , default = 'SystemLog:' ) args = parser . parse_args () # logger will be an instance of a subclass of the logging.Logger class. logger = logging . getLogger ( __name__ , args . prefix ) main ( logger ) Open question: should the compliant logging.getLogger method throw an exception if the non-compliant logging namespace is in scope? Alternatives Global log function Global log function. Not just prefix, more complicated \"mutation\" logic. Hash unsafe log lines Provide option to hash unsafe log lines. Not compliant. Other option, record their length Unsafe log message of length 23 Write public logs to pre-specified location Instead of adding a predetermined prefix to \"public\" logs, depending on the log filtering / scrubbing mechanisms, another alternative would be to write public logs to a specific file. Risks Abuse This logging utility will not prevent malicious abuse. That is, there is no way from the code to stop someone from writing a line like this. logger . info ( 'private data' , category = DataCategory . ONLY_PUBLIC_DATA ) However, this risk is not new, nor does it arise uniquely because of the proposed library. There is already nothing to prevent a malicious actor from writing this line print ( 'SystemLog: private data' ) Compliant machine learning in this context involves an element of trust, i.e. it is not designed or intended to stop malicious actors. We do not attempt to mitigate this risk by filtering out specific types of objects from \"public\" logs. Standard Python types like str and pandas.DataFrame can easily contain sensitive customer data. If we exclude those from logging, we will be excluding nearly all helpful logs.","title":"Logging design"},{"location":"compliant_logging/rfc-logging/#logging-in-compliant-ml","text":"Owner Approvers Participants Daniel Miller Jeff Omhover AML DS Team In many corporations, data scientists are working to build and train machine learning models under extremely strict compliance and privacy requirements. These can include: Being unable to directly access or view the customer data used to train a model. Being unable to run unsigned code against customer data, except possibly in specialized compute clusters with no network access and other hardening in place. All training logs removed, except possibly those starting with some fixed prefix (e.g., SystemLog: ). Machine learning is already a hard problem \u2014 building and training models under these constraints is even more difficult. This RFC ( R equest F or C omment) proposes the code patterns and expected behavior of a to-be-written logging utility for compliant machine learning. It begins by outlining the requirements this utility would need to satisfy, then gives a concrete proposal with sample code. It proceeds to outline some alternatives, along with known risks of the proposal. The following topics are out of scope for this RFC: How to implement the proposed behavior. It considers only usage patterns and the intended behavior of the logging utility library. Compliant argument parsing. Compliant exception handling (how to keep and prefix the stack trace and exception type, while scrubbing exception messages). Languages besides Python.","title":"Logging in Compliant ML"},{"location":"compliant_logging/rfc-logging/#requirements","text":"Every call to log a message should be forced to include information on whether the message is \"safe\" (does not contain any customer or sensitive data). If the message is safe, the log line should be mutated to contain a configurable prefix (e.g., SystemLog: ). The default behavior of this library should be to not add the \"don't scrub\" prefix. It should only add that prefix if a conscious decision has been made, possibly by choosing a different method name or parameter value. This library should not rely on users naming variables \"well\". That is, if a user accidentally names a logger safeLogger , there should be no data leak if that logger was not pre-configured on some special way. Code which consumes this library should have the same \"look and feel\" as code consuming the Python standard library's logging module. :warning: This entire document relies on the assumption that the filtering mechanism looks for a specific prefix in log lines.","title":"Requirements"},{"location":"compliant_logging/rfc-logging/#proposal","text":"I propose that the existing logging paradigm in Python be mostly unchanged, with the following small differences: Calls to getLogger must include a safe_prefix parameter. Open question: should this be more flexible, e.g. a callable or dictionary mapping data categories to prefixes or format strings? Calls to logger.* have a new optional parameter taking enum values. The default value is CONTAINS_PRIVATE_DATA , i.e. if this parameter is not explicitly provided the \"safe\" prefix will not be added. Here is some sample code following this proposal. import argparse from shrike.compliant_logging import logging from shrike.compliant_logging.constants import DataCategory def main ( logger ): # HAS a prefix. logger . warning ( 'safe message' , category = DataCategory . ONLY_PUBLIC_DATA ) # DOES NOT have a prefix. logger . warning ( 'unsafe message' ) if __name__ == '__main__' : parser = argparse . ArgumentParser () parser . add_argument ( '--prefix' , default = 'SystemLog:' ) args = parser . parse_args () # logger will be an instance of a subclass of the logging.Logger class. logger = logging . getLogger ( __name__ , args . prefix ) main ( logger ) Open question: should the compliant logging.getLogger method throw an exception if the non-compliant logging namespace is in scope?","title":"Proposal"},{"location":"compliant_logging/rfc-logging/#alternatives","text":"","title":"Alternatives"},{"location":"compliant_logging/rfc-logging/#global-log-function","text":"Global log function. Not just prefix, more complicated \"mutation\" logic.","title":"Global log function"},{"location":"compliant_logging/rfc-logging/#hash-unsafe-log-lines","text":"Provide option to hash unsafe log lines. Not compliant. Other option, record their length Unsafe log message of length 23","title":"Hash unsafe log lines"},{"location":"compliant_logging/rfc-logging/#write-public-logs-to-pre-specified-location","text":"Instead of adding a predetermined prefix to \"public\" logs, depending on the log filtering / scrubbing mechanisms, another alternative would be to write public logs to a specific file.","title":"Write public logs to pre-specified location"},{"location":"compliant_logging/rfc-logging/#risks","text":"","title":"Risks"},{"location":"compliant_logging/rfc-logging/#abuse","text":"This logging utility will not prevent malicious abuse. That is, there is no way from the code to stop someone from writing a line like this. logger . info ( 'private data' , category = DataCategory . ONLY_PUBLIC_DATA ) However, this risk is not new, nor does it arise uniquely because of the proposed library. There is already nothing to prevent a malicious actor from writing this line print ( 'SystemLog: private data' ) Compliant machine learning in this context involves an element of trust, i.e. it is not designed or intended to stop malicious actors. We do not attempt to mitigate this risk by filtering out specific types of objects from \"public\" logs. Standard Python types like str and pandas.DataFrame can easily contain sensitive customer data. If we exclude those from logging, we will be excluding nearly all helpful logs.","title":"Abuse"},{"location":"pipeline/aml-connect/","text":"AML Connect Helper code for connecting to AzureML and sharing one workspace accross code. add_cli_args ( parser ) Adds parser arguments for connecting to AzureML Parameters: Name Type Description Default parser argparse.ArgumentParser parser to add AzureML arguments to required Returns: Type Description argparse.ArgumentParser that same parser Source code in shrike/pipeline/aml_connect.py def add_cli_args ( parser ): \"\"\"Adds parser arguments for connecting to AzureML Args: parser (argparse.ArgumentParser): parser to add AzureML arguments to Returns: argparse.ArgumentParser: that same parser \"\"\" parser . add_argument ( \"--aml-subscription-id\" , dest = \"aml_subscription_id\" , type = str , required = False , help = \"\" , ) parser . add_argument ( \"--aml-resource-group\" , dest = \"aml_resource_group\" , type = str , required = False , help = \"\" , ) parser . add_argument ( \"--aml-workspace\" , dest = \"aml_workspace_name\" , type = str , required = False , help = \"\" ) parser . add_argument ( \"--aml-config\" , dest = \"aml_config\" , type = str , required = False , help = \"path to aml config.json file\" , ) parser . add_argument ( \"--aml-auth\" , dest = \"aml_auth\" , type = str , choices = [ \"azurecli\" , \"msi\" , \"interactive\" ], default = \"interactive\" , ) parser . add_argument ( \"--aml-tenant\" , dest = \"aml_tenant\" , type = str , default = None , help = \"tenant to use for auth (default: auto)\" , ) parser . add_argument ( \"--aml-force\" , dest = \"aml_force\" , type = lambda x : ( str ( x ) . lower () in [ \"true\" , \"1\" , \"yes\" ] ), # we want to use --aml-force True default = False , help = \"force tenant auth (default: False)\" , ) return parser azureml_connect ( ** kwargs ) Calls azureml_connect_cli with an argparse-like structure based on keyword arguments Source code in shrike/pipeline/aml_connect.py def azureml_connect ( ** kwargs ): \"\"\"Calls azureml_connect_cli with an argparse-like structure based on keyword arguments\"\"\" keys = [ \"aml_subscription_id\" , \"aml_resource_group\" , \"aml_workspace_name\" , \"aml_config\" , \"aml_auth\" , \"aml_tenant\" , \"aml_force\" , ] aml_args = dict ([( k , kwargs . get ( k )) for k in keys ]) azureml_argparse_tuple = namedtuple ( \"AzureMLArguments\" , aml_args ) aml_argparse = azureml_argparse_tuple ( ** aml_args ) return azureml_connect_cli ( aml_argparse ) azureml_connect_cli ( args ) Connects to an AzureML workspace. Parameters: Name Type Description Default args argparse.Namespace arguments to connect to AzureML required Returns: Type Description azureml.core.Workspace AzureML workspace Source code in shrike/pipeline/aml_connect.py def azureml_connect_cli ( args ): \"\"\"Connects to an AzureML workspace. Args: args (argparse.Namespace): arguments to connect to AzureML Returns: azureml.core.Workspace: AzureML workspace \"\"\" if args . aml_auth == \"msi\" : from azureml.core.authentication import MsiAuthentication auth = MsiAuthentication () elif args . aml_auth == \"azurecli\" : from azureml.core.authentication import AzureCliAuthentication auth = AzureCliAuthentication () elif args . aml_auth == \"interactive\" : from azureml.core.authentication import InteractiveLoginAuthentication auth = InteractiveLoginAuthentication ( tenant_id = args . aml_tenant , force = args . aml_force ) else : auth = None if args . aml_config : config_dir = os . path . dirname ( args . aml_config ) config_file_name = os . path . basename ( args . aml_config ) aml_ws = Workspace . from_config ( path = config_dir , _file_name = config_file_name , auth = auth ) else : aml_ws = Workspace . get ( subscription_id = args . aml_subscription_id , name = args . aml_workspace_name , resource_group = args . aml_resource_group , auth = auth , ) print ( \"Connected to Workspace\" , \"-- subscription:\" + aml_ws . subscription_id , \"-- name: \" + aml_ws . name , \"-- Azure region: \" + aml_ws . location , \"-- Resource group: \" + aml_ws . resource_group , sep = \" \\n \" , ) return current_workspace ( aml_ws ) current_workspace ( workspace = None ) Sets/Gets the current AML workspace used all accross code. Parameters: Name Type Description Default workspace azureml.core.Workspace any given workspace None Returns: Type Description azureml.core.Workspace current (last) workspace given to current_workspace() Source code in shrike/pipeline/aml_connect.py def current_workspace ( workspace = None ): \"\"\"Sets/Gets the current AML workspace used all accross code. Args: workspace (azureml.core.Workspace): any given workspace Returns: azureml.core.Workspace: current (last) workspace given to current_workspace() \"\"\" global CURRENT_AML_WORKSPACE if workspace : CURRENT_AML_WORKSPACE = workspace if not CURRENT_AML_WORKSPACE : raise Exception ( \"You need to initialize current_workspace() with an AML workspace\" ) return CURRENT_AML_WORKSPACE main () Main function (for testing) Source code in shrike/pipeline/aml_connect.py def main (): \"\"\"Main function (for testing)\"\"\" parser = argparse . ArgumentParser ( description = __doc__ ) group = parser . add_argument_group ( \"AzureML connect arguments\" ) add_cli_args ( group ) args , unknown_args = parser . parse_known_args () if unknown_args : print ( \"WARNING: you have provided unknown arguments {} \" . format ( unknown_args )) return azureml_connect_cli ( args )","title":"aml_connect"},{"location":"pipeline/aml-connect/#aml-connect","text":"Helper code for connecting to AzureML and sharing one workspace accross code.","title":"AML Connect"},{"location":"pipeline/aml-connect/#shrike.pipeline.aml_connect.add_cli_args","text":"Adds parser arguments for connecting to AzureML Parameters: Name Type Description Default parser argparse.ArgumentParser parser to add AzureML arguments to required Returns: Type Description argparse.ArgumentParser that same parser Source code in shrike/pipeline/aml_connect.py def add_cli_args ( parser ): \"\"\"Adds parser arguments for connecting to AzureML Args: parser (argparse.ArgumentParser): parser to add AzureML arguments to Returns: argparse.ArgumentParser: that same parser \"\"\" parser . add_argument ( \"--aml-subscription-id\" , dest = \"aml_subscription_id\" , type = str , required = False , help = \"\" , ) parser . add_argument ( \"--aml-resource-group\" , dest = \"aml_resource_group\" , type = str , required = False , help = \"\" , ) parser . add_argument ( \"--aml-workspace\" , dest = \"aml_workspace_name\" , type = str , required = False , help = \"\" ) parser . add_argument ( \"--aml-config\" , dest = \"aml_config\" , type = str , required = False , help = \"path to aml config.json file\" , ) parser . add_argument ( \"--aml-auth\" , dest = \"aml_auth\" , type = str , choices = [ \"azurecli\" , \"msi\" , \"interactive\" ], default = \"interactive\" , ) parser . add_argument ( \"--aml-tenant\" , dest = \"aml_tenant\" , type = str , default = None , help = \"tenant to use for auth (default: auto)\" , ) parser . add_argument ( \"--aml-force\" , dest = \"aml_force\" , type = lambda x : ( str ( x ) . lower () in [ \"true\" , \"1\" , \"yes\" ] ), # we want to use --aml-force True default = False , help = \"force tenant auth (default: False)\" , ) return parser","title":"add_cli_args()"},{"location":"pipeline/aml-connect/#shrike.pipeline.aml_connect.azureml_connect","text":"Calls azureml_connect_cli with an argparse-like structure based on keyword arguments Source code in shrike/pipeline/aml_connect.py def azureml_connect ( ** kwargs ): \"\"\"Calls azureml_connect_cli with an argparse-like structure based on keyword arguments\"\"\" keys = [ \"aml_subscription_id\" , \"aml_resource_group\" , \"aml_workspace_name\" , \"aml_config\" , \"aml_auth\" , \"aml_tenant\" , \"aml_force\" , ] aml_args = dict ([( k , kwargs . get ( k )) for k in keys ]) azureml_argparse_tuple = namedtuple ( \"AzureMLArguments\" , aml_args ) aml_argparse = azureml_argparse_tuple ( ** aml_args ) return azureml_connect_cli ( aml_argparse )","title":"azureml_connect()"},{"location":"pipeline/aml-connect/#shrike.pipeline.aml_connect.azureml_connect_cli","text":"Connects to an AzureML workspace. Parameters: Name Type Description Default args argparse.Namespace arguments to connect to AzureML required Returns: Type Description azureml.core.Workspace AzureML workspace Source code in shrike/pipeline/aml_connect.py def azureml_connect_cli ( args ): \"\"\"Connects to an AzureML workspace. Args: args (argparse.Namespace): arguments to connect to AzureML Returns: azureml.core.Workspace: AzureML workspace \"\"\" if args . aml_auth == \"msi\" : from azureml.core.authentication import MsiAuthentication auth = MsiAuthentication () elif args . aml_auth == \"azurecli\" : from azureml.core.authentication import AzureCliAuthentication auth = AzureCliAuthentication () elif args . aml_auth == \"interactive\" : from azureml.core.authentication import InteractiveLoginAuthentication auth = InteractiveLoginAuthentication ( tenant_id = args . aml_tenant , force = args . aml_force ) else : auth = None if args . aml_config : config_dir = os . path . dirname ( args . aml_config ) config_file_name = os . path . basename ( args . aml_config ) aml_ws = Workspace . from_config ( path = config_dir , _file_name = config_file_name , auth = auth ) else : aml_ws = Workspace . get ( subscription_id = args . aml_subscription_id , name = args . aml_workspace_name , resource_group = args . aml_resource_group , auth = auth , ) print ( \"Connected to Workspace\" , \"-- subscription:\" + aml_ws . subscription_id , \"-- name: \" + aml_ws . name , \"-- Azure region: \" + aml_ws . location , \"-- Resource group: \" + aml_ws . resource_group , sep = \" \\n \" , ) return current_workspace ( aml_ws )","title":"azureml_connect_cli()"},{"location":"pipeline/aml-connect/#shrike.pipeline.aml_connect.current_workspace","text":"Sets/Gets the current AML workspace used all accross code. Parameters: Name Type Description Default workspace azureml.core.Workspace any given workspace None Returns: Type Description azureml.core.Workspace current (last) workspace given to current_workspace() Source code in shrike/pipeline/aml_connect.py def current_workspace ( workspace = None ): \"\"\"Sets/Gets the current AML workspace used all accross code. Args: workspace (azureml.core.Workspace): any given workspace Returns: azureml.core.Workspace: current (last) workspace given to current_workspace() \"\"\" global CURRENT_AML_WORKSPACE if workspace : CURRENT_AML_WORKSPACE = workspace if not CURRENT_AML_WORKSPACE : raise Exception ( \"You need to initialize current_workspace() with an AML workspace\" ) return CURRENT_AML_WORKSPACE","title":"current_workspace()"},{"location":"pipeline/aml-connect/#shrike.pipeline.aml_connect.main","text":"Main function (for testing) Source code in shrike/pipeline/aml_connect.py def main (): \"\"\"Main function (for testing)\"\"\" parser = argparse . ArgumentParser ( description = __doc__ ) group = parser . add_argument_group ( \"AzureML connect arguments\" ) add_cli_args ( group ) args , unknown_args = parser . parse_known_args () if unknown_args : print ( \"WARNING: you have provided unknown arguments {} \" . format ( unknown_args )) return azureml_connect_cli ( args )","title":"main()"},{"location":"pipeline/canary-helper/","text":"Canary helper Canary helper code get_repo_info () [EXPERIMENTAL] Obtains info on the current repo the code is in. Returns: Type Description dict git meta data Source code in shrike/pipeline/canary_helper.py def get_repo_info (): \"\"\"[EXPERIMENTAL] Obtains info on the current repo the code is in. Returns: dict: git meta data\"\"\" try : import git repo = git . Repo ( search_parent_directories = True ) branch = repo . active_branch head = repo . head return { \"git\" : repo . remotes . origin . url , \"branch\" : branch . name , \"commit\" : head . commit . hexsha , \"last_known_author\" : head . commit . author . name , } except : return { \"git\" : \"n/a\" } test_pipeline_step_metrics ( pipeline_run , expected_metrics ) Tests a pipeline run against a set of expected metrics. Parameters: Name Type Description Default pipeline_run PipelineRun the AzureML pipeline run required expected_metrics dict defines the tests to execute required Returns: Type Description List errors collected during tests !!! notes example entries in expected_metrics \"SelectJsonField\" : [{\"row\" : {\"name\" : \"output\", \"key\" : \"size\", \"value\" : 369559}}], tests module \"SelectJsonField\" for a metric row named \"output\", checks key \"size\" must have value 369559 \"tokenizerparallel\" : [{\"metric\" : {\"key\" : \"Failed Items\", \"value\" : 0}}], tests module \"tokenizerparallel\" for a metric named \"Failed Items\", value must be 0 Source code in shrike/pipeline/canary_helper.py def test_pipeline_step_metrics ( pipeline_run , expected_metrics ): \"\"\"Tests a pipeline run against a set of expected metrics. Args: pipeline_run (PipelineRun): the AzureML pipeline run expected_metrics (dict): defines the tests to execute Returns: List: errors collected during tests Notes: example entries in expected_metrics \"SelectJsonField\" : [{\"row\" : {\"name\" : \"output\", \"key\" : \"size\", \"value\" : 369559}}], tests module \"SelectJsonField\" for a metric row named \"output\", checks key \"size\" must have value 369559 \"tokenizerparallel\" : [{\"metric\" : {\"key\" : \"Failed Items\", \"value\" : 0}}], tests module \"tokenizerparallel\" for a metric named \"Failed Items\", value must be 0 \"\"\" errors = [] print ( \"Looping through PipelineRun steps to test metrics...\" ) for step in pipeline_run . get_steps (): print ( f \"Checking status of step { step . name } ...\" ) observed_metrics = step . get_metrics () print ( f \"Step Metrics: { observed_metrics } \" ) status = step . get_status () if status != \"Finished\" : errors . append ( f \"Pipeline step { step . name } status is { status } != Finished\" ) if step . name in expected_metrics : for expected_metric_test in expected_metrics [ step . name ]: if \"row\" in expected_metric_test : print ( f \"Checking metrics, looking for { expected_metric_test } \" ) row_key = expected_metric_test [ \"row\" ][ \"name\" ] metric_key = expected_metric_test [ \"row\" ][ \"key\" ] expected_value = expected_metric_test [ \"row\" ][ \"value\" ] if row_key not in observed_metrics : errors . append ( f \"Step { step . name } metric row ' { row_key } ' not available in observed metrics { observed_metrics } \" ) elif metric_key not in observed_metrics [ row_key ]: errors . append ( f \"Step { step . name } metric row ' { row_key } ' does not have a metric ' { metric_key } ' in observed metrics { observed_metrics [ row_key ] } \" ) elif observed_metrics [ row_key ][ metric_key ] != expected_value : errors . append ( f \"Step { step . name } metric row ' { row_key } ' - metric ' { metric_key } ' - does not have expected value { expected_value } in observed metrics { observed_metrics [ row_key ] } \" ) if \"metric\" in expected_metric_test : print ( f \"Checking metrics, looking for { expected_metric_test } \" ) metric_key = expected_metric_test [ \"metric\" ][ \"key\" ] expected_value = expected_metric_test [ \"metric\" ][ \"value\" ] if metric_key not in observed_metrics : errors . append ( f \"Step { step . name } metric ' { metric_key } ' not available in observed metrics { observed_metrics } \" ) elif observed_metrics [ metric_key ] != expected_value : errors . append ( f \"Step { step . name } metric row ' { metric_key } ' does not have expected value { expected_value } in observed metrics { observed_metrics [ metric_key ] } \" ) return errors test_pipeline_step_output ( pipeline_run , step_name , output_name , ** kwargs ) Verify a given pipeline output for some basic checks. Parameters: Name Type Description Default pipeline_run PipelineRun the pipeline run required step_name str name of the step to check required output_name str name of the output to check required **kwargs Arbitrary keyword arguments defining the test {} !!! kwargs length (int) : to verify the length Returns: Type Description dict results Source code in shrike/pipeline/canary_helper.py def test_pipeline_step_output ( pipeline_run , step_name , output_name , ** kwargs ): \"\"\"Verify a given pipeline output for some basic checks. Args: pipeline_run (PipelineRun): the pipeline run step_name (str): name of the step to check output_name (str): name of the output to check **kwargs: Arbitrary keyword arguments defining the test Kwargs: length (int) : to verify the length Returns: dict: results \"\"\" pipeline_step = pipeline_run . find_step_run ( step_name ) results = { \"errors\" : []} if not pipeline_step : results [ \"exception\" ] = f \"Could not find step { step_name } in pipeline { pipeline_run . _run_id } .\" return results output_port = pipeline_step [ 0 ] . get_output_data ( output_name ) if not output_port : results [ \"exception\" ] = f \"Could not find output { output_name } in step { step_name } in pipeline { pipeline_run . _run_id } .\" return results data_reference = output_port . _data_reference data_path = DataPath ( datastore = data_reference . datastore , path_on_datastore = data_reference . path_on_datastore , name = data_reference . data_reference_name , ) if kwargs . get ( \"length\" ): expected_length = kwargs . get ( \"length\" ) print ( f \"Checking count= { expected_length } of files for step { step_name } output { output_name } ...\" ) data_set = Dataset . File . from_files ( data_path ) files_list = data_set . to_path () if expected_length < 0 : # test any length > 0 results [ \"length\" ] = { \"expected\" : \">0\" , \"observed\" : len ( files_list )} if results [ \"length\" ][ \"observed\" ] == 0 : message = \"\"\"Length mismatch in output {output_name} in step {step_name} in pipeline {run_id} . Expected len {a} found {b} .\"\"\" . format ( output_name = output_name , step_name = step_name , run_id = pipeline_run . _run_id , b = results [ \"length\" ][ \"observed\" ], a = results [ \"length\" ][ \"expected\" ], ) # logging.error(message) results [ \"errors\" ] . append ( message ) else : results [ \"length\" ] = { \"expected\" : expected_length , \"observed\" : len ( files_list ), } if results [ \"length\" ][ \"observed\" ] != results [ \"length\" ][ \"expected\" ]: message = \"\"\"Length mismatch in output {output_name} in step {step_name} in pipeline {run_id} . Expected len {a} found {b} .\"\"\" . format ( output_name = output_name , step_name = step_name , run_id = pipeline_run . _run_id , b = results [ \"length\" ][ \"observed\" ], a = results [ \"length\" ][ \"expected\" ], ) # logging.error(message) results [ \"errors\" ] . append ( message ) return results","title":"canary_helper"},{"location":"pipeline/canary-helper/#canary-helper","text":"Canary helper code","title":"Canary helper"},{"location":"pipeline/canary-helper/#shrike.pipeline.canary_helper.get_repo_info","text":"[EXPERIMENTAL] Obtains info on the current repo the code is in. Returns: Type Description dict git meta data Source code in shrike/pipeline/canary_helper.py def get_repo_info (): \"\"\"[EXPERIMENTAL] Obtains info on the current repo the code is in. Returns: dict: git meta data\"\"\" try : import git repo = git . Repo ( search_parent_directories = True ) branch = repo . active_branch head = repo . head return { \"git\" : repo . remotes . origin . url , \"branch\" : branch . name , \"commit\" : head . commit . hexsha , \"last_known_author\" : head . commit . author . name , } except : return { \"git\" : \"n/a\" }","title":"get_repo_info()"},{"location":"pipeline/canary-helper/#shrike.pipeline.canary_helper.test_pipeline_step_metrics","text":"Tests a pipeline run against a set of expected metrics. Parameters: Name Type Description Default pipeline_run PipelineRun the AzureML pipeline run required expected_metrics dict defines the tests to execute required Returns: Type Description List errors collected during tests !!! notes example entries in expected_metrics \"SelectJsonField\" : [{\"row\" : {\"name\" : \"output\", \"key\" : \"size\", \"value\" : 369559}}], tests module \"SelectJsonField\" for a metric row named \"output\", checks key \"size\" must have value 369559 \"tokenizerparallel\" : [{\"metric\" : {\"key\" : \"Failed Items\", \"value\" : 0}}], tests module \"tokenizerparallel\" for a metric named \"Failed Items\", value must be 0 Source code in shrike/pipeline/canary_helper.py def test_pipeline_step_metrics ( pipeline_run , expected_metrics ): \"\"\"Tests a pipeline run against a set of expected metrics. Args: pipeline_run (PipelineRun): the AzureML pipeline run expected_metrics (dict): defines the tests to execute Returns: List: errors collected during tests Notes: example entries in expected_metrics \"SelectJsonField\" : [{\"row\" : {\"name\" : \"output\", \"key\" : \"size\", \"value\" : 369559}}], tests module \"SelectJsonField\" for a metric row named \"output\", checks key \"size\" must have value 369559 \"tokenizerparallel\" : [{\"metric\" : {\"key\" : \"Failed Items\", \"value\" : 0}}], tests module \"tokenizerparallel\" for a metric named \"Failed Items\", value must be 0 \"\"\" errors = [] print ( \"Looping through PipelineRun steps to test metrics...\" ) for step in pipeline_run . get_steps (): print ( f \"Checking status of step { step . name } ...\" ) observed_metrics = step . get_metrics () print ( f \"Step Metrics: { observed_metrics } \" ) status = step . get_status () if status != \"Finished\" : errors . append ( f \"Pipeline step { step . name } status is { status } != Finished\" ) if step . name in expected_metrics : for expected_metric_test in expected_metrics [ step . name ]: if \"row\" in expected_metric_test : print ( f \"Checking metrics, looking for { expected_metric_test } \" ) row_key = expected_metric_test [ \"row\" ][ \"name\" ] metric_key = expected_metric_test [ \"row\" ][ \"key\" ] expected_value = expected_metric_test [ \"row\" ][ \"value\" ] if row_key not in observed_metrics : errors . append ( f \"Step { step . name } metric row ' { row_key } ' not available in observed metrics { observed_metrics } \" ) elif metric_key not in observed_metrics [ row_key ]: errors . append ( f \"Step { step . name } metric row ' { row_key } ' does not have a metric ' { metric_key } ' in observed metrics { observed_metrics [ row_key ] } \" ) elif observed_metrics [ row_key ][ metric_key ] != expected_value : errors . append ( f \"Step { step . name } metric row ' { row_key } ' - metric ' { metric_key } ' - does not have expected value { expected_value } in observed metrics { observed_metrics [ row_key ] } \" ) if \"metric\" in expected_metric_test : print ( f \"Checking metrics, looking for { expected_metric_test } \" ) metric_key = expected_metric_test [ \"metric\" ][ \"key\" ] expected_value = expected_metric_test [ \"metric\" ][ \"value\" ] if metric_key not in observed_metrics : errors . append ( f \"Step { step . name } metric ' { metric_key } ' not available in observed metrics { observed_metrics } \" ) elif observed_metrics [ metric_key ] != expected_value : errors . append ( f \"Step { step . name } metric row ' { metric_key } ' does not have expected value { expected_value } in observed metrics { observed_metrics [ metric_key ] } \" ) return errors","title":"test_pipeline_step_metrics()"},{"location":"pipeline/canary-helper/#shrike.pipeline.canary_helper.test_pipeline_step_output","text":"Verify a given pipeline output for some basic checks. Parameters: Name Type Description Default pipeline_run PipelineRun the pipeline run required step_name str name of the step to check required output_name str name of the output to check required **kwargs Arbitrary keyword arguments defining the test {} !!! kwargs length (int) : to verify the length Returns: Type Description dict results Source code in shrike/pipeline/canary_helper.py def test_pipeline_step_output ( pipeline_run , step_name , output_name , ** kwargs ): \"\"\"Verify a given pipeline output for some basic checks. Args: pipeline_run (PipelineRun): the pipeline run step_name (str): name of the step to check output_name (str): name of the output to check **kwargs: Arbitrary keyword arguments defining the test Kwargs: length (int) : to verify the length Returns: dict: results \"\"\" pipeline_step = pipeline_run . find_step_run ( step_name ) results = { \"errors\" : []} if not pipeline_step : results [ \"exception\" ] = f \"Could not find step { step_name } in pipeline { pipeline_run . _run_id } .\" return results output_port = pipeline_step [ 0 ] . get_output_data ( output_name ) if not output_port : results [ \"exception\" ] = f \"Could not find output { output_name } in step { step_name } in pipeline { pipeline_run . _run_id } .\" return results data_reference = output_port . _data_reference data_path = DataPath ( datastore = data_reference . datastore , path_on_datastore = data_reference . path_on_datastore , name = data_reference . data_reference_name , ) if kwargs . get ( \"length\" ): expected_length = kwargs . get ( \"length\" ) print ( f \"Checking count= { expected_length } of files for step { step_name } output { output_name } ...\" ) data_set = Dataset . File . from_files ( data_path ) files_list = data_set . to_path () if expected_length < 0 : # test any length > 0 results [ \"length\" ] = { \"expected\" : \">0\" , \"observed\" : len ( files_list )} if results [ \"length\" ][ \"observed\" ] == 0 : message = \"\"\"Length mismatch in output {output_name} in step {step_name} in pipeline {run_id} . Expected len {a} found {b} .\"\"\" . format ( output_name = output_name , step_name = step_name , run_id = pipeline_run . _run_id , b = results [ \"length\" ][ \"observed\" ], a = results [ \"length\" ][ \"expected\" ], ) # logging.error(message) results [ \"errors\" ] . append ( message ) else : results [ \"length\" ] = { \"expected\" : expected_length , \"observed\" : len ( files_list ), } if results [ \"length\" ][ \"observed\" ] != results [ \"length\" ][ \"expected\" ]: message = \"\"\"Length mismatch in output {output_name} in step {step_name} in pipeline {run_id} . Expected len {a} found {b} .\"\"\" . format ( output_name = output_name , step_name = step_name , run_id = pipeline_run . _run_id , b = results [ \"length\" ][ \"observed\" ], a = results [ \"length\" ][ \"expected\" ], ) # logging.error(message) results [ \"errors\" ] . append ( message ) return results","title":"test_pipeline_step_output()"},{"location":"pipeline/configure-aml-pipeline/","text":"How to use pipeline config files when using shrike.pipeline Structure of config files In this page, we perform a detailed review of an example standard config file . This should give you a good idea of how to use config files properly based on your scenarios. For a pipeline, we set up 4 config files under the config directory , which includes 4 sub-folders: experiments , modules , aml , and compute . The demograph_eyeson.yaml file linked above lives in the experiments folder; it is the main config file which specifies the overall pipeline configuration. This main config file refers to three other config files under the config directory: a config file under the aml folder which lets you point at your Azure ML workspace by specifying subscription_id, resource_group, workspace_name, tenant and auth; a config file under the compute folder which specifies configurations such as the compliant data store name, compute targets names, data I/O methods, etc; a config file under the modules folder , which lists all the available components with their properties (key, name, default version, and location of the component specification file). Now we will go through the config file linked above and explain each section. 1. Brief summary section At the beginning of the config file, it is suggested to provide a brief comment explaining which pipeline this config file is used for, and also provide an example command to run the pipeline with this config file. See below for an example: # This yaml file configures the accelerator tutorial pipeline for eyes-on # command for running the pipeline: # python pipelines/experiments/demograph_eyeson.py --config-dir pipelines/config --config-name experiments/demograph_eyeson run.submit=True 2. defaults section The defaults section contains references of the aml resources, pointing to two other config files under the aml and compute folders. It also points to the file listing all available components, which is located in the modules folder. This section looks like below. defaults : - aml : eyeson # default aml references - compute : eyeson # default compute target names - modules : module_defaults # list of modules + versions See below for the contents of the aml config file. You will need to update the info based on your own aml resources. To find your workspace name, subscription Id, and resource group, go to your AML workspace, then click the \"change subscription\" icon in the top right (between the settings and question mark), then \"Download config file\". You will find the 3 values in this file. The Torus TenantId for eyes-off workspaces is cdc5aeea-15c5-4db6-b079-fcadd2505dc2 , whereas the 72f988bf-86f1-41af-91ab-2d7cd011db47 used below is the Microsoft TenantId that you will use for personal workspaces. # @package _group_ subscription_id : 48bbc269-ce89-4f6f-9a12-c6f91fcb772d resource_group : aml1p-rg workspace_name : aml1p-ml-wus2 tenant : 72f988bf-86f1-41af-91ab-2d7cd011db47 auth : \"interactive\" See below for the contents of the compute config file (update the info based on your own aml resources). # @package _group_ # name of default target default_compute_target : \"cpu-cluster\" # where intermediary output is written compliant_datastore : \"workspaceblobstore\" # Linux targets linux_cpu_dc_target : \"cpu-cluster\" linux_cpu_prod_target : \"cpu-cluster\" linux_gpu_dc_target : \"gpu-cluster\" linux_gpu_prod_target : \"gpu-cluster\" # data I/O for linux modules linux_input_mode : \"download\" linux_output_mode : \"upload\" # Windows targets windows_cpu_prod_target : \"cpu-cluster\" # data I/O for windows modules windows_input_mode : \"download\" windows_output_mode : \"upload\" # hdi cluster hdi_prod_target : \"hdi-cluster\" # data transfer cluster datatransfer_target : \"data-factory\" 3. run section In this section, you configure the parameters controlling how to run your experiment. Update the info based on your own pipeline. Parameter names should be self-explanatory. # run parameters are command line arguments for running your experiment run : # params for running pipeline experiment_name : \"demo_graph_eyeson\" # IMPORTANT regenerate_outputs : false continue_on_failure : false verbose : false submit : false resume : false canary : false silent : false wait : false 4. module_loader section This section includes 4 arguments: use_local , force_default_module_version , force_all_module_version , and local_steps_folder . The use_local parameter specifies which components of the pipeline you would like to build from your local code (rather than consuming the remote registered component). Use a comma-separated string to specify the list of components from your local code. If you use \"*\", all components are loaded from local code. The force_default_module_version argument enables you to change the default version of the component in your branch (the default version is the latest version, but this argument allows you to pin it to a given release version if you prefer). The force_all_module_version argument enables you to force all components to consume a fixed version, even if the version is specified otherwise in the pipeline code. The argument local_steps_folder should be clear and self-explanatory: this is the directory where all the component folders are located. # module_loader module_loader : # module loading params # IMPORTANT: if you want to modify a given module, add its key here # see the code for identifying the module key # use comma separation in this string to use multiple local modules use_local : \"DemoComponent\" # fix the version of modules in all subgraphs (if left unspecified) # NOTE: use the latest release version to \"fix\" your branch to a given release # see https://eemo.visualstudio.com/TEE/_release?_a=releases&view=mine&definitionId=76 force_default_module_version : null # forces ALL module versions to this unique value (even if specified otherwise in code) force_all_module_version : null # path to the steps folder, don't modify this one # NOTE: we're working on deprecating this one local_steps_folder : \"../../../components\" # NOTE: run scripts from accelerator-repo 5. Other sections The sections above only defined overall pipeline parameters, not component parameters. We recommend gathering the component parameters into distinct sections, one per component. The example for the eyes-on demo experiment is shown below. # DemoComponent config democomponent : input_data : irisdata # the data we'll be working on input_data_version : \"latest\" # use this to pin a specific version message : \"Hello, World!\" value : 1000 # the size of the sample to analyze","title":"Configure your AML pipeline"},{"location":"pipeline/configure-aml-pipeline/#how-to-use-pipeline-config-files-when-using-shrikepipeline","text":"","title":"How to use pipeline config files when using shrike.pipeline"},{"location":"pipeline/configure-aml-pipeline/#structure-of-config-files","text":"In this page, we perform a detailed review of an example standard config file . This should give you a good idea of how to use config files properly based on your scenarios. For a pipeline, we set up 4 config files under the config directory , which includes 4 sub-folders: experiments , modules , aml , and compute . The demograph_eyeson.yaml file linked above lives in the experiments folder; it is the main config file which specifies the overall pipeline configuration. This main config file refers to three other config files under the config directory: a config file under the aml folder which lets you point at your Azure ML workspace by specifying subscription_id, resource_group, workspace_name, tenant and auth; a config file under the compute folder which specifies configurations such as the compliant data store name, compute targets names, data I/O methods, etc; a config file under the modules folder , which lists all the available components with their properties (key, name, default version, and location of the component specification file). Now we will go through the config file linked above and explain each section.","title":"Structure of config files"},{"location":"pipeline/configure-aml-pipeline/#1-brief-summary-section","text":"At the beginning of the config file, it is suggested to provide a brief comment explaining which pipeline this config file is used for, and also provide an example command to run the pipeline with this config file. See below for an example: # This yaml file configures the accelerator tutorial pipeline for eyes-on # command for running the pipeline: # python pipelines/experiments/demograph_eyeson.py --config-dir pipelines/config --config-name experiments/demograph_eyeson run.submit=True","title":"1. Brief summary section"},{"location":"pipeline/configure-aml-pipeline/#2-defaults-section","text":"The defaults section contains references of the aml resources, pointing to two other config files under the aml and compute folders. It also points to the file listing all available components, which is located in the modules folder. This section looks like below. defaults : - aml : eyeson # default aml references - compute : eyeson # default compute target names - modules : module_defaults # list of modules + versions See below for the contents of the aml config file. You will need to update the info based on your own aml resources. To find your workspace name, subscription Id, and resource group, go to your AML workspace, then click the \"change subscription\" icon in the top right (between the settings and question mark), then \"Download config file\". You will find the 3 values in this file. The Torus TenantId for eyes-off workspaces is cdc5aeea-15c5-4db6-b079-fcadd2505dc2 , whereas the 72f988bf-86f1-41af-91ab-2d7cd011db47 used below is the Microsoft TenantId that you will use for personal workspaces. # @package _group_ subscription_id : 48bbc269-ce89-4f6f-9a12-c6f91fcb772d resource_group : aml1p-rg workspace_name : aml1p-ml-wus2 tenant : 72f988bf-86f1-41af-91ab-2d7cd011db47 auth : \"interactive\" See below for the contents of the compute config file (update the info based on your own aml resources). # @package _group_ # name of default target default_compute_target : \"cpu-cluster\" # where intermediary output is written compliant_datastore : \"workspaceblobstore\" # Linux targets linux_cpu_dc_target : \"cpu-cluster\" linux_cpu_prod_target : \"cpu-cluster\" linux_gpu_dc_target : \"gpu-cluster\" linux_gpu_prod_target : \"gpu-cluster\" # data I/O for linux modules linux_input_mode : \"download\" linux_output_mode : \"upload\" # Windows targets windows_cpu_prod_target : \"cpu-cluster\" # data I/O for windows modules windows_input_mode : \"download\" windows_output_mode : \"upload\" # hdi cluster hdi_prod_target : \"hdi-cluster\" # data transfer cluster datatransfer_target : \"data-factory\"","title":"2. defaults section"},{"location":"pipeline/configure-aml-pipeline/#3-run-section","text":"In this section, you configure the parameters controlling how to run your experiment. Update the info based on your own pipeline. Parameter names should be self-explanatory. # run parameters are command line arguments for running your experiment run : # params for running pipeline experiment_name : \"demo_graph_eyeson\" # IMPORTANT regenerate_outputs : false continue_on_failure : false verbose : false submit : false resume : false canary : false silent : false wait : false","title":"3. run section"},{"location":"pipeline/configure-aml-pipeline/#4-module_loader-section","text":"This section includes 4 arguments: use_local , force_default_module_version , force_all_module_version , and local_steps_folder . The use_local parameter specifies which components of the pipeline you would like to build from your local code (rather than consuming the remote registered component). Use a comma-separated string to specify the list of components from your local code. If you use \"*\", all components are loaded from local code. The force_default_module_version argument enables you to change the default version of the component in your branch (the default version is the latest version, but this argument allows you to pin it to a given release version if you prefer). The force_all_module_version argument enables you to force all components to consume a fixed version, even if the version is specified otherwise in the pipeline code. The argument local_steps_folder should be clear and self-explanatory: this is the directory where all the component folders are located. # module_loader module_loader : # module loading params # IMPORTANT: if you want to modify a given module, add its key here # see the code for identifying the module key # use comma separation in this string to use multiple local modules use_local : \"DemoComponent\" # fix the version of modules in all subgraphs (if left unspecified) # NOTE: use the latest release version to \"fix\" your branch to a given release # see https://eemo.visualstudio.com/TEE/_release?_a=releases&view=mine&definitionId=76 force_default_module_version : null # forces ALL module versions to this unique value (even if specified otherwise in code) force_all_module_version : null # path to the steps folder, don't modify this one # NOTE: we're working on deprecating this one local_steps_folder : \"../../../components\" # NOTE: run scripts from accelerator-repo","title":"4. module_loader section"},{"location":"pipeline/configure-aml-pipeline/#5-other-sections","text":"The sections above only defined overall pipeline parameters, not component parameters. We recommend gathering the component parameters into distinct sections, one per component. The example for the eyes-on demo experiment is shown below. # DemoComponent config democomponent : input_data : irisdata # the data we'll be working on input_data_version : \"latest\" # use this to pin a specific version message : \"Hello, World!\" value : 1000 # the size of the sample to analyze","title":"5. Other sections"},{"location":"pipeline/create-aml-pipeline/","text":"Instructions for creating a reusable AML pipeline using shrike.pipeline To enjoy this doc, you need to: have already setup your python environment with the AML SDK following these instructions and cloned the accelerator repository as described in the \"Set up\" section here ; have access to an AML workspace Note: If your AML workspace is a newly created workspace, you would have to first run one sample pipeline from the designer page of the workspace to warm up (more details available here ). Otherwise your submitted job would get stuck with the \"Not Started\" status. This is a known caveat and AML has a work item to track this. Motivation The AML pipeline helper class ( shrike.pipeline ) was developed with the goal of helping data scientists to more easily create reusable pipelines. These instructions explain how to use the AML pipeline helper class. 1. Review an existing AML pipeline created using AML pipeline helper class The accelerator repository already has examples of pipelines created using the pipeline helper code. We will now have an overview of the structure of the two most important directories ( components and pipelines , under aml-ds/recipes/compliant-experimentation ) and go over the key files defining these pipelines. 1.1 \"components\" directory This is where the components are defined, one folder per component. Each folder contains the following files: component_spec.yaml : this is where the component's inputs, outputs and parameters are defined. This is the AML equivalent to the component manifest in Aether. component_env.yaml : this is where the component dependencies are listed (not required for HDI components). run.py : this is the python file actually run in AML; in most cases, it is just importing a python file from elsewhere in the repo. Further reading on components is available here . 1.2 \"pipelines\" directory This is where the graphs, a.k.a. pipelines, are defined. Here is what you will find in its subdirectories: - The config directory contains the config files which contain the parameter values, organized in four sub-folders: experiments which contains the overall graph configs, then aml and compute which contain auxiliary config files referred to in the graph configs. The modules folder hosts the file where the components are defined (by their key, name, default version, and location of the component specification file). Once you have created new modules, you will need to add them to that file. - The subgraphs directory contain python files that define graphs that are not meant to be used on their own but as part of larger graphs. There is a demo subgraph available there, which consists of 2 probe modules chained after each other. - The experiments directory contain the python files whichactually define the graphs. Now let's take a closer look at the definition of a graph in python. We will stick with the demo graph for eyes-off and open the demograph_eyesoff.py file in the experiments folder. The key parts are listed below. - The required_subgraphs() function ( line 37 , also shown below) defines the subgraphs that are used in the graph. # line 37 def required_subgraphs ( cls ): \"\"\"Declare dependencies on other subgraphs to allow AMLPipelineHelper to build them for you. This method should return a dictionary: - Keys will be used in self.subgraph_load(key) to build each required subgraph. - Values are classes inheriting from AMLPipelineHelper Returns: dict[str->AMLPipelineHelper]: dictionary of subgraphs used for building this one. \"\"\" return { \"DemoSubgraph\" : DemoSubgraph } The build() function, well, builds the graph. First, the required subgraph is loaded in line 62 : probe_subgraph = self . subgraph_load ( \"DemoSubgraph\" ) Then we define a pipeline function for the graph starting line 70 . This is where all the components and subgraphs are given their parameters and inputs. Note how the parameter values are read from the config files. To see how the outputs of some components can be used as inputs of the following components see here in the subgraph python file . For the time being, we have to manually apply run settings to every component. In the future, this will not be necessary anymore. For the current example, it is also done in the subgraph python file, by calling the apply_recommended_runsettings() function . The pipeline_instance() function creates a runnable instance of the pipeline. The input dataset is defined in lines 104-107 , by calling the dataset_load() function with the name and version values provided in the config file. The pipeline function is then called with the input data as argument. Next, let's open the demograph_eyesoff.yaml config file under the pipelines/config/experiments directory, and note how the other config files are referenced, and how the parameters are organized in sections. We also explain config files in more details in this page: Configure your pipeline . Finally, below is the command to run this existing pipeline (a very basic demo pipeline): python pipelines/experiments/demograph_eyesoff.py --config-dir pipelines/config --config-name experiments/demograph_eyesoff run.submit=True 2. Create your own simple AML pipeline using the pipeline helper class and an already existing module In this section, We will create a pipeline graph consisting of a single component called probe , which is readily available in the accelerator repository. We will pass the parameters through a config file. Procedure: [1] For creating your own pipeline, we invite you to start from an already existing pipeline definition such as demograph_eyeson.py and build from there. Just copy demograph_eyeson.py , rename it as demograph_workshop.py , update the contents accordingly, and put it under the same directory (i.e., pipelines/experiments ). The important parts to modify for this file are those listed in the section on key files above: build() , and pipeline_instance() (since we won't be using a subgraph, we don't need to worry about the required_subgraphs part). [2] To prepare the yaml config file, start from an existing example, such as demograph_eyeson.yaml . Just copy demograph_eyeson.yaml , rename it as demograph_eyeson_workshop.yaml , update the contents accordingly, and put it under the same directory (i.e., pipelines/config/experiments ). The important parts are defining the component parameter values, and declaring that we want to use the local version of the component (argument use_local ) for probe . > Note: you will also need to update two auxiliary config files ( eyesoff.yaml / eyeson.yaml file under directory pipelines/config/aml and eyesoff.yaml / eyeson.yaml under directory pipelines/config/compute ), referenced by this main config file demograph_eyeson.yaml , to point to the AML workspace and compute targets to which you have access. And now you should be able to run your pipeline using the following command: python pipelines/experiments/demograph_eyesoff.py --config-dir pipelines/config --config-name experiments/demograph_eyesoff run.submit=True If you are using an eyes-on workspace, you will also need to update the base image info in component_spec.yaml since only eyes-off workspaces can connect to the polymer prod ACR which hosts the base image. When a parameter is not specified in the config file, you need to use + when overriding directly from command line. Otherwise there'll be errors. For example, if run.submit is not in the config file, you need to use python pipelines/experiments/demograph_eyesoff.py --config-dir pipelines/config --config-name experiments/demograph_eyesoff +run.submit=True . Please refer to Hydra override syntax for more info.","title":"Create an AML pipeline"},{"location":"pipeline/create-aml-pipeline/#instructions-for-creating-a-reusable-aml-pipeline-using-shrikepipeline","text":"To enjoy this doc, you need to: have already setup your python environment with the AML SDK following these instructions and cloned the accelerator repository as described in the \"Set up\" section here ; have access to an AML workspace Note: If your AML workspace is a newly created workspace, you would have to first run one sample pipeline from the designer page of the workspace to warm up (more details available here ). Otherwise your submitted job would get stuck with the \"Not Started\" status. This is a known caveat and AML has a work item to track this.","title":"Instructions for creating a reusable AML pipeline using shrike.pipeline"},{"location":"pipeline/create-aml-pipeline/#motivation","text":"The AML pipeline helper class ( shrike.pipeline ) was developed with the goal of helping data scientists to more easily create reusable pipelines. These instructions explain how to use the AML pipeline helper class.","title":"Motivation"},{"location":"pipeline/create-aml-pipeline/#1-review-an-existing-aml-pipeline-created-using-aml-pipeline-helper-class","text":"The accelerator repository already has examples of pipelines created using the pipeline helper code. We will now have an overview of the structure of the two most important directories ( components and pipelines , under aml-ds/recipes/compliant-experimentation ) and go over the key files defining these pipelines.","title":"1. Review an existing  AML pipeline created using AML pipeline helper class"},{"location":"pipeline/create-aml-pipeline/#11-components-directory","text":"This is where the components are defined, one folder per component. Each folder contains the following files: component_spec.yaml : this is where the component's inputs, outputs and parameters are defined. This is the AML equivalent to the component manifest in Aether. component_env.yaml : this is where the component dependencies are listed (not required for HDI components). run.py : this is the python file actually run in AML; in most cases, it is just importing a python file from elsewhere in the repo. Further reading on components is available here .","title":"1.1 \"components\" directory"},{"location":"pipeline/create-aml-pipeline/#12-pipelines-directory","text":"This is where the graphs, a.k.a. pipelines, are defined. Here is what you will find in its subdirectories: - The config directory contains the config files which contain the parameter values, organized in four sub-folders: experiments which contains the overall graph configs, then aml and compute which contain auxiliary config files referred to in the graph configs. The modules folder hosts the file where the components are defined (by their key, name, default version, and location of the component specification file). Once you have created new modules, you will need to add them to that file. - The subgraphs directory contain python files that define graphs that are not meant to be used on their own but as part of larger graphs. There is a demo subgraph available there, which consists of 2 probe modules chained after each other. - The experiments directory contain the python files whichactually define the graphs. Now let's take a closer look at the definition of a graph in python. We will stick with the demo graph for eyes-off and open the demograph_eyesoff.py file in the experiments folder. The key parts are listed below. - The required_subgraphs() function ( line 37 , also shown below) defines the subgraphs that are used in the graph. # line 37 def required_subgraphs ( cls ): \"\"\"Declare dependencies on other subgraphs to allow AMLPipelineHelper to build them for you. This method should return a dictionary: - Keys will be used in self.subgraph_load(key) to build each required subgraph. - Values are classes inheriting from AMLPipelineHelper Returns: dict[str->AMLPipelineHelper]: dictionary of subgraphs used for building this one. \"\"\" return { \"DemoSubgraph\" : DemoSubgraph } The build() function, well, builds the graph. First, the required subgraph is loaded in line 62 : probe_subgraph = self . subgraph_load ( \"DemoSubgraph\" ) Then we define a pipeline function for the graph starting line 70 . This is where all the components and subgraphs are given their parameters and inputs. Note how the parameter values are read from the config files. To see how the outputs of some components can be used as inputs of the following components see here in the subgraph python file . For the time being, we have to manually apply run settings to every component. In the future, this will not be necessary anymore. For the current example, it is also done in the subgraph python file, by calling the apply_recommended_runsettings() function . The pipeline_instance() function creates a runnable instance of the pipeline. The input dataset is defined in lines 104-107 , by calling the dataset_load() function with the name and version values provided in the config file. The pipeline function is then called with the input data as argument. Next, let's open the demograph_eyesoff.yaml config file under the pipelines/config/experiments directory, and note how the other config files are referenced, and how the parameters are organized in sections. We also explain config files in more details in this page: Configure your pipeline . Finally, below is the command to run this existing pipeline (a very basic demo pipeline): python pipelines/experiments/demograph_eyesoff.py --config-dir pipelines/config --config-name experiments/demograph_eyesoff run.submit=True","title":"1.2 \"pipelines\" directory"},{"location":"pipeline/create-aml-pipeline/#2-create-your-own-simple-aml-pipeline-using-the-pipeline-helper-class-and-an-already-existing-module","text":"In this section, We will create a pipeline graph consisting of a single component called probe , which is readily available in the accelerator repository. We will pass the parameters through a config file. Procedure: [1] For creating your own pipeline, we invite you to start from an already existing pipeline definition such as demograph_eyeson.py and build from there. Just copy demograph_eyeson.py , rename it as demograph_workshop.py , update the contents accordingly, and put it under the same directory (i.e., pipelines/experiments ). The important parts to modify for this file are those listed in the section on key files above: build() , and pipeline_instance() (since we won't be using a subgraph, we don't need to worry about the required_subgraphs part). [2] To prepare the yaml config file, start from an existing example, such as demograph_eyeson.yaml . Just copy demograph_eyeson.yaml , rename it as demograph_eyeson_workshop.yaml , update the contents accordingly, and put it under the same directory (i.e., pipelines/config/experiments ). The important parts are defining the component parameter values, and declaring that we want to use the local version of the component (argument use_local ) for probe . > Note: you will also need to update two auxiliary config files ( eyesoff.yaml / eyeson.yaml file under directory pipelines/config/aml and eyesoff.yaml / eyeson.yaml under directory pipelines/config/compute ), referenced by this main config file demograph_eyeson.yaml , to point to the AML workspace and compute targets to which you have access. And now you should be able to run your pipeline using the following command: python pipelines/experiments/demograph_eyesoff.py --config-dir pipelines/config --config-name experiments/demograph_eyesoff run.submit=True If you are using an eyes-on workspace, you will also need to update the base image info in component_spec.yaml since only eyes-off workspaces can connect to the polymer prod ACR which hosts the base image. When a parameter is not specified in the config file, you need to use + when overriding directly from command line. Otherwise there'll be errors. For example, if run.submit is not in the config file, you need to use python pipelines/experiments/demograph_eyesoff.py --config-dir pipelines/config --config-name experiments/demograph_eyesoff +run.submit=True . Please refer to Hydra override syntax for more info.","title":"2. Create your own simple AML pipeline using the pipeline helper class and an already existing module"},{"location":"pipeline/module-helper/","text":"Module helper Pipeline helper class to create pipelines loading modules from a flexible manifest. AMLModuleLoader Helper class to load modules from within an AMLPipelineHelper. __init__ ( self , config ) special Creates module instances for AMLPipelineHelper. Parameters: Name Type Description Default config DictConfig configuration options required Source code in shrike/pipeline/module_helper.py def __init__ ( self , config ): \"\"\"Creates module instances for AMLPipelineHelper. Args: config (DictConfig): configuration options \"\"\" if \"use_local\" not in config . module_loader : self . use_local = [] elif config . module_loader . use_local is None : self . use_local = [] elif config . module_loader . use_local == \"*\" : self . use_local = \"*\" elif isinstance ( config . module_loader . use_local , str ): self . use_local = [ x . strip () for x in config . module_loader . use_local . split ( \",\" ) ] self . force_default_module_version = ( config . module_loader . force_default_module_version if \"force_default_module_version\" in config . module_loader else None ) self . force_all_module_version = ( config . module_loader . force_all_module_version if \"force_all_module_version\" in config . module_loader else None ) self . local_steps_folder = config . module_loader . local_steps_folder self . module_cache = {} # internal manifest built from yaml config self . modules_manifest = {} self . load_config_manifest ( config ) print ( f \"AMLModuleLoader initialized (use_local= { self . use_local } , force_default_module_version= { self . force_default_module_version } , force_all_module_version= { self . force_all_module_version } , local_steps_folder= { self . local_steps_folder } , manifest= { list ( self . modules_manifest . keys ()) } )\" ) get_from_cache ( self , module_cache_key ) Gets module class from internal cache (dict) Source code in shrike/pipeline/module_helper.py def get_from_cache ( self , module_cache_key ): \"\"\"Gets module class from internal cache (dict)\"\"\" print ( f \"--- Using cached module { module_cache_key } \" ) return self . module_cache . get ( module_cache_key , None ) get_module_manifest_entry ( self , module_key , modules_manifest = None ) Gets a particular entry in the module manifest. Parameters: Name Type Description Default module_key str module key from the manifest required modules_manifest dict manifest from required_modules() [DEPRECATED] None Returns: Type Description dict module manifest entry Source code in shrike/pipeline/module_helper.py def get_module_manifest_entry ( self , module_key , modules_manifest = None ): \"\"\"Gets a particular entry in the module manifest. Args: module_key (str): module key from the manifest modules_manifest (dict): manifest from required_modules() [DEPRECATED] Returns: dict: module manifest entry \"\"\" if module_key in self . modules_manifest : module_entry = self . modules_manifest [ module_key ] module_namespace = None elif modules_manifest and module_key in modules_manifest : print ( f \"WARNING: We highly recommend substituting the required_modules() method by the modules.manifest configuration.\" ) module_entry = modules_manifest [ module_key ] # map to new format module_entry [ \"yaml\" ] = module_entry [ \"yaml_spec\" ] module_entry [ \"name\" ] = module_entry [ \"remote_module_name\" ] module_namespace = module_entry . get ( \"namespace\" , None ) else : raise Exception ( f \"Module key ' { module_key } ' could not be found in modules.manifest configuration or in required_modules() method.\" ) return module_entry , module_namespace is_local ( self , module_name ) Tests is module is in local list Source code in shrike/pipeline/module_helper.py def is_local ( self , module_name ): \"\"\"Tests is module is in local list\"\"\" if self . use_local == \"*\" : return True return module_name in self . use_local load_config_manifest ( self , config ) Fills the internal module manifest based on config object Source code in shrike/pipeline/module_helper.py def load_config_manifest ( self , config ): \"\"\"Fills the internal module manifest based on config object\"\"\" for entry in config . modules . manifest : if entry . key : module_key = entry . key elif entry . name : module_key = entry . name else : raise Exception ( \"In module manifest, you have to provide at least key or name.\" ) self . modules_manifest [ module_key ] = entry load_local_module ( self , module_spec_path ) Creates one module instance. Parameters: Name Type Description Default module_spec_path str path to local module yaml spec required Returns: Type Description object module class loaded Source code in shrike/pipeline/module_helper.py def load_local_module ( self , module_spec_path ): \"\"\"Creates one module instance. Args: module_spec_path (str): path to local module yaml spec Returns: object: module class loaded \"\"\" module_cache_key = module_spec_path if self . module_in_cache ( module_cache_key ): return self . get_from_cache ( module_cache_key ) print ( \"--- Building module from local code at {} \" . format ( module_spec_path )) if not os . path . isfile ( module_spec_path ): module_spec_path = os . path . join ( self . local_steps_folder , module_spec_path ) loaded_module_class = Component . from_yaml ( current_workspace (), module_spec_path ) self . put_in_cache ( module_cache_key , loaded_module_class ) return loaded_module_class load_module ( self , module_key , modules_manifest = None ) Loads a particular module from the manifest. Parameters: Name Type Description Default module_key str module key from the manifest required modules_manifest dict manifest from required_modules() [DEPRECATED] None Returns: Type Description object module class loaded Source code in shrike/pipeline/module_helper.py def load_module ( self , module_key , modules_manifest = None ): \"\"\"Loads a particular module from the manifest. Args: module_key (str): module key from the manifest modules_manifest (dict): manifest from required_modules() [DEPRECATED] Returns: object: module class loaded \"\"\" module_entry , module_namespace = self . get_module_manifest_entry ( module_key , modules_manifest ) if ( self . use_local == \"*\" ) or ( module_key in self . use_local ): loaded_module = self . load_local_module ( module_entry [ \"yaml\" ]) else : loaded_module = self . load_prod_module ( module_entry [ \"name\" ], module_entry [ \"version\" ], module_namespace = module_namespace , ) return loaded_module load_modules_manifest ( self , modules_manifest ) Creates module instances from modules_manifest. Parameters: Name Type Description Default modules_manifest dict manifest of modules to load required Returns: Type Description dict modules loaded, keys are taken from module_manifest. Exceptions: Type Description Exception if loading module has an error or manifest is wrong. Source code in shrike/pipeline/module_helper.py def load_modules_manifest ( self , modules_manifest ): \"\"\"Creates module instances from modules_manifest. Args: modules_manifest (dict): manifest of modules to load Returns: dict: modules loaded, keys are taken from module_manifest. Raises: Exception: if loading module has an error or manifest is wrong. \"\"\" print ( f \"Loading module manifest (use_local= { self . use_local } )\" ) test_results = self . verify_manifest ( modules_manifest ) if test_results : raise Exception ( \"Loading modules from manifest raised errors: \\n\\n MANIFEST: {} \\n\\n ERRORS: {} \" . format ( modules_manifest , \" \\n \" . join ( test_results ) ) ) loaded_modules = {} for module_key in modules_manifest : print ( f \"Loading module { module_key } from manifest\" ) loaded_modules [ module_key ] = self . load_module ( module_key , modules_manifest ) return loaded_modules load_prod_module ( self , module_name , module_version , module_namespace = None ) Creates one module instance. Parameters: Name Type Description Default module_name str) module name required module_version str) module version required Returns: Type Description object module class loaded Source code in shrike/pipeline/module_helper.py def load_prod_module ( self , module_name , module_version , module_namespace = None ): \"\"\"Creates one module instance. Args: module_name (str) : module name module_version (str) : module version Returns: object: module class loaded \"\"\" if self . force_all_module_version : module_version = self . force_all_module_version else : module_version = module_version or self . force_default_module_version module_cache_key = f \" { module_name } : { module_version } \" if self . module_in_cache ( module_cache_key ): return self . get_from_cache ( module_cache_key ) print ( f \"--- Loading remote module { module_cache_key } (name= { module_name } , version= { module_version } , namespace= { module_namespace } )\" ) loading_raised_exception = None try : # try without namespace first loaded_module_class = Component . load ( current_workspace (), name = module_name , version = module_version , ) except BaseException as e : # save the exception to raise it if namespace not provided if not module_namespace : raise e if module_namespace : print ( f \" Trying to load module { module_name } with namespace { module_namespace } .\" ) module_name = module_namespace + \"://\" + module_name loaded_module_class = Component . load ( current_workspace (), name = module_name , version = module_version , ) self . put_in_cache ( module_cache_key , loaded_module_class ) return loaded_module_class module_in_cache ( self , module_cache_key ) Tests if module in internal cache (dict) Source code in shrike/pipeline/module_helper.py def module_in_cache ( self , module_cache_key ): \"\"\"Tests if module in internal cache (dict)\"\"\" return module_cache_key in self . module_cache put_in_cache ( self , module_cache_key , module_class ) Puts module class in internal cache (dict) Source code in shrike/pipeline/module_helper.py def put_in_cache ( self , module_cache_key , module_class ): \"\"\"Puts module class in internal cache (dict)\"\"\" self . module_cache [ module_cache_key ] = module_class verify_manifest ( self , modules_manifest ) Tests a module manifest schema Source code in shrike/pipeline/module_helper.py def verify_manifest ( self , modules_manifest ): \"\"\"Tests a module manifest schema\"\"\" errors = [] for ( k , module_entry ) in modules_manifest . items (): # TODO: merge error checking code with processing code so we do all this in one pass if ( self . use_local == \"*\" ) or ( k in self . use_local ): if \"yaml_spec\" not in module_entry : errors . append ( f \" { k } : You need to specify a yaml_spec for your module to use_local=[' { k } ']\" ) elif not os . path . isfile ( module_entry [ \"yaml_spec\" ] ) and not os . path . isfile ( os . path . join ( self . local_steps_folder , module_entry [ \"yaml_spec\" ]) ): errors . append ( \" {} : Could not find yaml spec {} for use_local=[' {} ']\" . format ( k , module_entry [ \"yaml_spec\" ], k ) ) else : if \"remote_module_name\" not in module_entry : errors . append ( f \" { k } : You need to specify a name for your module to use_local=False\" ) if \"namespace\" not in module_entry : errors . append ( f \" { k } : You need to specify a namespace for your module to use_local=False\" ) if ( \"version\" not in module_entry ) and ( self . force_default_module_version or self . force_all_module_version ): errors . append ( f \" { k } : You need to specify a version for your module to use_local=False, or use either force_default_module_version or force_all_module_version in config\" ) return errors module_loader_config dataclass Config for the AMLModuleLoader class module_manifest dataclass module_manifest(manifest: List[shrike.pipeline.module_helper.module_reference] = ) module_reference dataclass module_reference(key: Union[str, NoneType] = None, name: Union[str, NoneType] = None, source: Union[str, NoneType] = 'registered', yaml: Union[str, NoneType] = None, version: Union[str, NoneType] = None)","title":"module_helper"},{"location":"pipeline/module-helper/#module-helper","text":"Pipeline helper class to create pipelines loading modules from a flexible manifest.","title":"Module helper"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader","text":"Helper class to load modules from within an AMLPipelineHelper.","title":"AMLModuleLoader"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.__init__","text":"Creates module instances for AMLPipelineHelper. Parameters: Name Type Description Default config DictConfig configuration options required Source code in shrike/pipeline/module_helper.py def __init__ ( self , config ): \"\"\"Creates module instances for AMLPipelineHelper. Args: config (DictConfig): configuration options \"\"\" if \"use_local\" not in config . module_loader : self . use_local = [] elif config . module_loader . use_local is None : self . use_local = [] elif config . module_loader . use_local == \"*\" : self . use_local = \"*\" elif isinstance ( config . module_loader . use_local , str ): self . use_local = [ x . strip () for x in config . module_loader . use_local . split ( \",\" ) ] self . force_default_module_version = ( config . module_loader . force_default_module_version if \"force_default_module_version\" in config . module_loader else None ) self . force_all_module_version = ( config . module_loader . force_all_module_version if \"force_all_module_version\" in config . module_loader else None ) self . local_steps_folder = config . module_loader . local_steps_folder self . module_cache = {} # internal manifest built from yaml config self . modules_manifest = {} self . load_config_manifest ( config ) print ( f \"AMLModuleLoader initialized (use_local= { self . use_local } , force_default_module_version= { self . force_default_module_version } , force_all_module_version= { self . force_all_module_version } , local_steps_folder= { self . local_steps_folder } , manifest= { list ( self . modules_manifest . keys ()) } )\" )","title":"__init__()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.get_from_cache","text":"Gets module class from internal cache (dict) Source code in shrike/pipeline/module_helper.py def get_from_cache ( self , module_cache_key ): \"\"\"Gets module class from internal cache (dict)\"\"\" print ( f \"--- Using cached module { module_cache_key } \" ) return self . module_cache . get ( module_cache_key , None )","title":"get_from_cache()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.get_module_manifest_entry","text":"Gets a particular entry in the module manifest. Parameters: Name Type Description Default module_key str module key from the manifest required modules_manifest dict manifest from required_modules() [DEPRECATED] None Returns: Type Description dict module manifest entry Source code in shrike/pipeline/module_helper.py def get_module_manifest_entry ( self , module_key , modules_manifest = None ): \"\"\"Gets a particular entry in the module manifest. Args: module_key (str): module key from the manifest modules_manifest (dict): manifest from required_modules() [DEPRECATED] Returns: dict: module manifest entry \"\"\" if module_key in self . modules_manifest : module_entry = self . modules_manifest [ module_key ] module_namespace = None elif modules_manifest and module_key in modules_manifest : print ( f \"WARNING: We highly recommend substituting the required_modules() method by the modules.manifest configuration.\" ) module_entry = modules_manifest [ module_key ] # map to new format module_entry [ \"yaml\" ] = module_entry [ \"yaml_spec\" ] module_entry [ \"name\" ] = module_entry [ \"remote_module_name\" ] module_namespace = module_entry . get ( \"namespace\" , None ) else : raise Exception ( f \"Module key ' { module_key } ' could not be found in modules.manifest configuration or in required_modules() method.\" ) return module_entry , module_namespace","title":"get_module_manifest_entry()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.is_local","text":"Tests is module is in local list Source code in shrike/pipeline/module_helper.py def is_local ( self , module_name ): \"\"\"Tests is module is in local list\"\"\" if self . use_local == \"*\" : return True return module_name in self . use_local","title":"is_local()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.load_config_manifest","text":"Fills the internal module manifest based on config object Source code in shrike/pipeline/module_helper.py def load_config_manifest ( self , config ): \"\"\"Fills the internal module manifest based on config object\"\"\" for entry in config . modules . manifest : if entry . key : module_key = entry . key elif entry . name : module_key = entry . name else : raise Exception ( \"In module manifest, you have to provide at least key or name.\" ) self . modules_manifest [ module_key ] = entry","title":"load_config_manifest()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.load_local_module","text":"Creates one module instance. Parameters: Name Type Description Default module_spec_path str path to local module yaml spec required Returns: Type Description object module class loaded Source code in shrike/pipeline/module_helper.py def load_local_module ( self , module_spec_path ): \"\"\"Creates one module instance. Args: module_spec_path (str): path to local module yaml spec Returns: object: module class loaded \"\"\" module_cache_key = module_spec_path if self . module_in_cache ( module_cache_key ): return self . get_from_cache ( module_cache_key ) print ( \"--- Building module from local code at {} \" . format ( module_spec_path )) if not os . path . isfile ( module_spec_path ): module_spec_path = os . path . join ( self . local_steps_folder , module_spec_path ) loaded_module_class = Component . from_yaml ( current_workspace (), module_spec_path ) self . put_in_cache ( module_cache_key , loaded_module_class ) return loaded_module_class","title":"load_local_module()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.load_module","text":"Loads a particular module from the manifest. Parameters: Name Type Description Default module_key str module key from the manifest required modules_manifest dict manifest from required_modules() [DEPRECATED] None Returns: Type Description object module class loaded Source code in shrike/pipeline/module_helper.py def load_module ( self , module_key , modules_manifest = None ): \"\"\"Loads a particular module from the manifest. Args: module_key (str): module key from the manifest modules_manifest (dict): manifest from required_modules() [DEPRECATED] Returns: object: module class loaded \"\"\" module_entry , module_namespace = self . get_module_manifest_entry ( module_key , modules_manifest ) if ( self . use_local == \"*\" ) or ( module_key in self . use_local ): loaded_module = self . load_local_module ( module_entry [ \"yaml\" ]) else : loaded_module = self . load_prod_module ( module_entry [ \"name\" ], module_entry [ \"version\" ], module_namespace = module_namespace , ) return loaded_module","title":"load_module()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.load_modules_manifest","text":"Creates module instances from modules_manifest. Parameters: Name Type Description Default modules_manifest dict manifest of modules to load required Returns: Type Description dict modules loaded, keys are taken from module_manifest. Exceptions: Type Description Exception if loading module has an error or manifest is wrong. Source code in shrike/pipeline/module_helper.py def load_modules_manifest ( self , modules_manifest ): \"\"\"Creates module instances from modules_manifest. Args: modules_manifest (dict): manifest of modules to load Returns: dict: modules loaded, keys are taken from module_manifest. Raises: Exception: if loading module has an error or manifest is wrong. \"\"\" print ( f \"Loading module manifest (use_local= { self . use_local } )\" ) test_results = self . verify_manifest ( modules_manifest ) if test_results : raise Exception ( \"Loading modules from manifest raised errors: \\n\\n MANIFEST: {} \\n\\n ERRORS: {} \" . format ( modules_manifest , \" \\n \" . join ( test_results ) ) ) loaded_modules = {} for module_key in modules_manifest : print ( f \"Loading module { module_key } from manifest\" ) loaded_modules [ module_key ] = self . load_module ( module_key , modules_manifest ) return loaded_modules","title":"load_modules_manifest()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.load_prod_module","text":"Creates one module instance. Parameters: Name Type Description Default module_name str) module name required module_version str) module version required Returns: Type Description object module class loaded Source code in shrike/pipeline/module_helper.py def load_prod_module ( self , module_name , module_version , module_namespace = None ): \"\"\"Creates one module instance. Args: module_name (str) : module name module_version (str) : module version Returns: object: module class loaded \"\"\" if self . force_all_module_version : module_version = self . force_all_module_version else : module_version = module_version or self . force_default_module_version module_cache_key = f \" { module_name } : { module_version } \" if self . module_in_cache ( module_cache_key ): return self . get_from_cache ( module_cache_key ) print ( f \"--- Loading remote module { module_cache_key } (name= { module_name } , version= { module_version } , namespace= { module_namespace } )\" ) loading_raised_exception = None try : # try without namespace first loaded_module_class = Component . load ( current_workspace (), name = module_name , version = module_version , ) except BaseException as e : # save the exception to raise it if namespace not provided if not module_namespace : raise e if module_namespace : print ( f \" Trying to load module { module_name } with namespace { module_namespace } .\" ) module_name = module_namespace + \"://\" + module_name loaded_module_class = Component . load ( current_workspace (), name = module_name , version = module_version , ) self . put_in_cache ( module_cache_key , loaded_module_class ) return loaded_module_class","title":"load_prod_module()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.module_in_cache","text":"Tests if module in internal cache (dict) Source code in shrike/pipeline/module_helper.py def module_in_cache ( self , module_cache_key ): \"\"\"Tests if module in internal cache (dict)\"\"\" return module_cache_key in self . module_cache","title":"module_in_cache()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.put_in_cache","text":"Puts module class in internal cache (dict) Source code in shrike/pipeline/module_helper.py def put_in_cache ( self , module_cache_key , module_class ): \"\"\"Puts module class in internal cache (dict)\"\"\" self . module_cache [ module_cache_key ] = module_class","title":"put_in_cache()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.verify_manifest","text":"Tests a module manifest schema Source code in shrike/pipeline/module_helper.py def verify_manifest ( self , modules_manifest ): \"\"\"Tests a module manifest schema\"\"\" errors = [] for ( k , module_entry ) in modules_manifest . items (): # TODO: merge error checking code with processing code so we do all this in one pass if ( self . use_local == \"*\" ) or ( k in self . use_local ): if \"yaml_spec\" not in module_entry : errors . append ( f \" { k } : You need to specify a yaml_spec for your module to use_local=[' { k } ']\" ) elif not os . path . isfile ( module_entry [ \"yaml_spec\" ] ) and not os . path . isfile ( os . path . join ( self . local_steps_folder , module_entry [ \"yaml_spec\" ]) ): errors . append ( \" {} : Could not find yaml spec {} for use_local=[' {} ']\" . format ( k , module_entry [ \"yaml_spec\" ], k ) ) else : if \"remote_module_name\" not in module_entry : errors . append ( f \" { k } : You need to specify a name for your module to use_local=False\" ) if \"namespace\" not in module_entry : errors . append ( f \" { k } : You need to specify a namespace for your module to use_local=False\" ) if ( \"version\" not in module_entry ) and ( self . force_default_module_version or self . force_all_module_version ): errors . append ( f \" { k } : You need to specify a version for your module to use_local=False, or use either force_default_module_version or force_all_module_version in config\" ) return errors","title":"verify_manifest()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.module_loader_config","text":"Config for the AMLModuleLoader class","title":"module_loader_config"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.module_manifest","text":"module_manifest(manifest: List[shrike.pipeline.module_helper.module_reference] = )","title":"module_manifest"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.module_reference","text":"module_reference(key: Union[str, NoneType] = None, name: Union[str, NoneType] = None, source: Union[str, NoneType] = 'registered', yaml: Union[str, NoneType] = None, version: Union[str, NoneType] = None)","title":"module_reference"},{"location":"pipeline/pipeline-helper/","text":"Pipeline helper Pipeline helper class to create pipelines loading modules from a flexible manifest. AMLPipelineHelper Helper class for building pipelines __init__ ( self , config , module_loader = None ) special Constructs the pipeline helper. Parameters: Name Type Description Default config DictConfig config for this object required module_loader AMLModuleLoader which module loader to (re)use None Source code in shrike/pipeline/pipeline_helper.py def __init__ ( self , config , module_loader = None ): \"\"\"Constructs the pipeline helper. Args: config (DictConfig): config for this object module_loader (AMLModuleLoader): which module loader to (re)use \"\"\" self . config = config if module_loader is None : print ( f \"Creating instance of AMLModuleLoader for { self . __class__ . __name__ } \" ) self . module_loader = AMLModuleLoader ( self . config ) else : self . module_loader = module_loader apply_recommended_runsettings ( self , module_name , module_instance , gpu = False , hdi = 'auto' , windows = 'auto' , parallel = 'auto' , mpi = 'auto' , scope = 'auto' , datatransfer = 'auto' , ** custom_runtime_arguments ) Applies regular settings for a given module. Parameters: Name Type Description Default module_name str name of the module from the module manifest (required_modules() method) required module_instance Module the aml module we need to add settings to required gpu bool is the module using gpu? False hdi bool is the module using hdi/spark? 'auto' windows bool is the module using windows compute? 'auto' parallel bool is the module using ParallelRunStep? 'auto' mpi bool is the module using Mpi? 'auto' custom_runtime_arguments dict any additional custom args {} Source code in shrike/pipeline/pipeline_helper.py def apply_recommended_runsettings ( self , module_name , module_instance , gpu = False , # can't autodetect that hdi = \"auto\" , windows = \"auto\" , parallel = \"auto\" , mpi = \"auto\" , scope = \"auto\" , datatransfer = \"auto\" , ** custom_runtime_arguments , ): \"\"\"Applies regular settings for a given module. Args: module_name (str): name of the module from the module manifest (required_modules() method) module_instance (Module): the aml module we need to add settings to gpu (bool): is the module using gpu? hdi (bool): is the module using hdi/spark? windows (bool): is the module using windows compute? parallel (bool): is the module using ParallelRunStep? mpi (bool): is the module using Mpi? custom_runtime_arguments (dict): any additional custom args \"\"\" # verifies if module_name corresponds to module_instance self . _check_module_runsettings_consistency ( module_name , module_instance ) # Auto detect runsettings if hdi == \"auto\" : hdi = str ( module_instance . type ) == \"HDInsightComponent\" if hdi : print ( f \"Module { module_name } detected as HDI: { hdi } \" ) if parallel == \"auto\" : parallel = str ( module_instance . type ) == \"ParallelComponent\" if parallel : print ( f \"Module { module_name } detected as PARALLEL: { parallel } \" ) if mpi == \"auto\" : mpi = str ( module_instance . type ) == \"DistributedComponent\" if mpi : print ( f \"Module { module_name } detected as MPI: { mpi } \" ) if scope == \"auto\" : scope = str ( module_instance . type ) == \"ScopeComponent\" if scope : print ( f \"Module { module_name } detected as SCOPE: { scope } \" ) if windows == \"auto\" : if ( str ( module_instance . type ) == \"HDInsightComponent\" or str ( module_instance . type ) == \"ScopeComponent\" or str ( module_instance . type ) == \"DataTransferComponent\" ): # hdi/scope/datatransfer modules might not have that environment object windows = False else : windows = ( module_instance . _definition . environment . os . lower () == \"windows\" ) if windows : print ( f \"Module { module_name } detected as WINDOWS: { windows } \" ) if datatransfer == \"auto\" : datatransfer = str ( module_instance . type ) == \"DataTransferComponent\" if datatransfer : print ( f \"Module { module_name } detected as DATATRANSFER: { datatransfer } \" ) if parallel : self . _apply_parallel_runsettings ( module_name , module_instance , windows = windows , gpu = gpu , ** custom_runtime_arguments , ) return if windows : # no detonation chamber, we an't use \"use_local\" here self . _apply_windows_runsettings ( module_name , module_instance , mpi = mpi , ** custom_runtime_arguments ) return if hdi : # no detonation chamber, we an't use \"use_local\" here self . _apply_hdi_runsettings ( module_name , module_instance , ** custom_runtime_arguments ) return if scope : self . _apply_scope_runsettings ( module_name , module_instance , ** custom_runtime_arguments ) return if datatransfer : self . _apply_datatransfer_runsettings ( module_name , module_instance , ** custom_runtime_arguments ) return self . _apply_linux_runsettings ( module_name , module_instance , mpi = mpi , gpu = gpu , ** custom_runtime_arguments ) build ( self , config ) Builds a pipeline function for this pipeline. Parameters: Name Type Description Default config DictConfig configuration object (see get_config_class()) required Returns: Type Description pipeline_function the function to create your pipeline Source code in shrike/pipeline/pipeline_helper.py def build ( self , config ): \"\"\"Builds a pipeline function for this pipeline. Args: config (DictConfig): configuration object (see get_config_class()) Returns: pipeline_function: the function to create your pipeline \"\"\" raise NotImplementedError ( \"You need to implement your build() method.\" ) canary ( self , args , experiment , pipeline_run ) Tests the output of the pipeline Source code in shrike/pipeline/pipeline_helper.py def canary ( self , args , experiment , pipeline_run ): \"\"\"Tests the output of the pipeline\"\"\" pass component_load ( self , component_key ) Loads one component from the manifest Source code in shrike/pipeline/pipeline_helper.py def component_load ( self , component_key ): \"\"\"Loads one component from the manifest\"\"\" return self . module_loader . load_module ( component_key , self . required_modules ()) connect ( self ) Connect to the AML workspace using internal config Source code in shrike/pipeline/pipeline_helper.py def connect ( self ): \"\"\"Connect to the AML workspace using internal config\"\"\" return azureml_connect ( aml_subscription_id = self . config . aml . subscription_id , aml_resource_group = self . config . aml . resource_group , aml_workspace_name = self . config . aml . workspace_name , aml_auth = self . config . aml . auth , aml_tenant = self . config . aml . tenant , aml_force = self . config . aml . force , ) # NOTE: this also stores aml workspace in internal global variable dataset_load ( self , name , version = 'latest' ) Loads a dataset by either id or name. Parameters: Name Type Description Default name str name or uuid of dataset to load required version str if loading by name, used to specify version (default \"latest\") 'latest' NOTE: in AzureML SDK there are 2 different methods for loading dataset one for id, one for name. This method just wraps them up in one. Source code in shrike/pipeline/pipeline_helper.py def dataset_load ( self , name , version = \"latest\" ): \"\"\"Loads a dataset by either id or name. Args: name (str): name or uuid of dataset to load version (str): if loading by name, used to specify version (default \"latest\") NOTE: in AzureML SDK there are 2 different methods for loading dataset one for id, one for name. This method just wraps them up in one.\"\"\" # test if given name is a uuid try : parsed_uuid = uuid . UUID ( name ) print ( f \"Getting a dataset handle [id= { name } ]...\" ) return Dataset . get_by_id ( self . workspace (), id = name ) except ValueError : print ( f \"Getting a dataset handle [name= { name } version= { version } ]...\" ) return Dataset . get_by_name ( self . workspace (), name = name , version = version ) get_config_class () classmethod Returns a dataclass containing config for this pipeline Source code in shrike/pipeline/pipeline_helper.py @classmethod def get_config_class ( cls ): \"\"\"Returns a dataclass containing config for this pipeline\"\"\" pass main () classmethod Pipeline helper main function, parses arguments and run pipeline. Source code in shrike/pipeline/pipeline_helper.py @classmethod def main ( cls ): \"\"\"Pipeline helper main function, parses arguments and run pipeline.\"\"\" config_dict = cls . _default_config () @hydra . main ( config_name = \"default\" ) def hydra_run ( cfg : DictConfig ): # merge cli config with default config cfg = OmegaConf . merge ( config_dict , cfg ) print ( \"*** CONFIGURATION ***\" ) print ( OmegaConf . to_yaml ( cfg )) # create class instance main_instance = cls ( cfg ) # run main_instance . run () hydra_run () return cls . BUILT_PIPELINE # return so we can have some unit tests done module_load ( self , module_key ) Loads one module from the manifest Source code in shrike/pipeline/pipeline_helper.py def module_load ( self , module_key ): \"\"\"Loads one module from the manifest\"\"\" return self . module_loader . load_module ( module_key , self . required_modules ()) pipeline_instance ( self , pipeline_function , config ) Creates an instance of the pipeline using arguments. Parameters: Name Type Description Default pipeline_function function the pipeline function obtained from self.build() required config DictConfig configuration object (see get_config_class()) required Returns: Type Description pipeline the instance constructed using build() function Source code in shrike/pipeline/pipeline_helper.py def pipeline_instance ( self , pipeline_function , config ): \"\"\"Creates an instance of the pipeline using arguments. Args: pipeline_function (function): the pipeline function obtained from self.build() config (DictConfig): configuration object (see get_config_class()) Returns: pipeline: the instance constructed using build() function \"\"\" raise NotImplementedError ( \"You need to implement your pipeline_instance() method.\" ) required_modules () classmethod Dependencies on modules/components Returns: Type Description dict[str, dict] manifest Source code in shrike/pipeline/pipeline_helper.py @classmethod def required_modules ( cls ): \"\"\"Dependencies on modules/components Returns: dict[str, dict]: manifest \"\"\" return {} required_subgraphs () classmethod Dependencies on other subgraphs Returns: Type Description dict[str, AMLPipelineHelper] dictionary of subgraphs used for building this one. keys are whatever string you want for building your graph values should be classes inherinting from AMLPipelineHelper. Source code in shrike/pipeline/pipeline_helper.py @classmethod def required_subgraphs ( cls ): \"\"\"Dependencies on other subgraphs Returns: dict[str, AMLPipelineHelper]: dictionary of subgraphs used for building this one. keys are whatever string you want for building your graph values should be classes inherinting from AMLPipelineHelper. \"\"\" return {} run ( self ) Run pipeline using arguments Source code in shrike/pipeline/pipeline_helper.py def run ( self ): \"\"\"Run pipeline using arguments\"\"\" # Log the telemetry information in the Azure Application Insights telemetry_logger = TelemetryLogger ( enable_telemetry = not self . config . run . disable_telemetry ) telemetry_logger . log_trace ( message = f \"shrike.pipeline== { __version__ } \" , properties = { \"custom_dimensions\" : { \"configuration\" : str ( self . config )}}, ) # Check whether the experiment name is valid self . validate_experiment_name ( self . config . run . experiment_name ) repository_info = get_repo_info () print ( f \"Running from repository: { repository_info } \" ) print ( \"azureml.core.VERSION = {} \" . format ( azureml . core . VERSION )) self . connect () if self . config . run . verbose : logging . getLogger () . setLevel ( logging . DEBUG ) if self . config . run . resume : if not self . config . run . pipeline_run_id : raise Exception ( \"To be able to use --resume you need to provide both --experiment-name and --run-id.\" ) print ( f \"Resuming Experiment { self . config . run . experiment_name } ...\" ) experiment = Experiment ( current_workspace (), self . config . run . experiment_name ) print ( f \"Resuming PipelineRun { self . config . run . pipeline_run_id } ...\" ) # pipeline_run is of the class \"azureml.pipeline.core.PipelineRun\" pipeline_run = PipelineRun ( experiment , self . config . run . pipeline_run_id ) else : print ( f \"Building Pipeline [ { self . __class__ . __name__ } ]...\" ) pipeline_function = self . build ( self . config ) print ( \"Creating Pipeline Instance...\" ) pipeline = self . pipeline_instance ( pipeline_function , self . config ) print ( \"Validating...\" ) pipeline . validate () if self . config . run . export : print ( f \"Exporting to { self . config . run . export } ...\" ) with open ( self . config . run . export , \"w\" ) as export_file : export_file . write ( pipeline . _get_graph_json ()) if self . config . run . submit : pipeline_tags = self . _parse_pipeline_tags () pipeline_tags . update ( repository_info ) print ( f \"Submitting Experiment... [tags= { pipeline_tags } ]\" ) # pipeline_run is of the class \"azure.ml.component.run\", which # is different from \"azureml.pipeline.core.PipelineRun\" pipeline_run = pipeline . submit ( experiment_name = self . config . run . experiment_name , tags = pipeline_tags , default_compute_target = self . config . compute . default_compute_target , regenerate_outputs = self . config . run . regenerate_outputs , continue_on_step_failure = self . config . run . continue_on_failure , ) # Forece pipeline_run to be of the class \"azureml.pipeline.core.PipelineRun\" pipeline_run = PipelineRun ( experiment = pipeline_run . _experiment , run_id = pipeline_run . _id , ) else : print ( \"Exiting now, if you want to submit please override run.submit=True\" ) self . __class__ . BUILT_PIPELINE = ( pipeline # return so we can have some unit tests done ) return # launch the pipeline execution print ( f \"Pipeline Run Id: { pipeline_run . id } \" ) print ( f \"\"\" ################################# ################################# ################################# Follow link below to access your pipeline run directly: ------------------------------------------------------- { pipeline_run . get_portal_url () } ################################# ################################# ################################# \"\"\" ) if self . config . run . canary : print ( \"*** CANARY MODE *** \\n ----------------------------------------------------------\" ) pipeline_run . wait_for_completion ( show_output = True ) # azureml.pipeline.core.PipelineRun.get_status(): [\"Running\", \"Finished\", \"Failed\"] # azureml.core.run.get_status(): [\"Running\", \"Completed\", \"Failed\"] if pipeline_run . get_status () in [ \"Finished\" , \"Completed\" ]: print ( \"*** PIPELINE FINISHED, TESTING WITH canary() METHOD ***\" ) self . canary ( self . config , pipeline_run . experiment , pipeline_run ) print ( \"OK\" ) elif pipeline_run . get_status () == \"Failed\" : print ( \"*** PIPELINE FAILED ***\" ) raise Exception ( \"Pipeline failed.\" ) else : print ( \"*** PIPELINE STATUS {} UNKNOWN ***\" ) raise Exception ( \"Pipeline status is unknown.\" ) else : if not self . config . run . silent : webbrowser . open ( url = pipeline_run . get_portal_url ()) # This will wait for the completion of the pipeline execution # and show the full logs in the meantime if self . config . run . resume or self . config . run . wait : print ( \"Below are the raw debug logs from your pipeline execution: \\n ----------------------------------------------------------\" ) pipeline_run . wait_for_completion ( show_output = True ) subgraph_load ( self , subgraph_key ) Loads one subgraph from the manifest Source code in shrike/pipeline/pipeline_helper.py def subgraph_load ( self , subgraph_key ): \"\"\"Loads one subgraph from the manifest\"\"\" subgraph_class = self . required_subgraphs ()[ subgraph_key ] print ( f \"Building subgraph [ { subgraph_key } as { subgraph_class . __name__ } ]...\" ) # NOTE: below creates subgraph with same pipeline_config subgraph_instance = subgraph_class ( config = self . config , module_loader = self . module_loader ) # subgraph_instance.setup(self.pipeline_config) return subgraph_instance . build ( self . config ) validate_experiment_name ( name ) staticmethod Check whether the experiment name is valid. It's required that experiment names must be between 1 to 250 characters, start\u202fwith letters or numbers. Valid characters are letters, numbers, \"_\", and the \"-\" character. Source code in shrike/pipeline/pipeline_helper.py @staticmethod def validate_experiment_name ( name ): \"\"\" Check whether the experiment name is valid. It's required that experiment names must be between 1 to 250 characters, start\u202fwith letters or numbers. Valid characters are letters, numbers, \"_\", and the \"-\" character. \"\"\" if len ( name ) < 1 or len ( name ) > 250 : raise ValueError ( \"Experiment names must be between 1 to 250 characters!\" ) if not re . match ( \"^[a-zA-Z0-9]$\" , name [ 0 ]): raise ValueError ( \"Experiment names must start\u202fwith letters or numbers!\" ) if not re . match ( \"^[a-zA-Z0-9_-]*$\" , name ): raise ValueError ( \"Valiad experiment names must only contain letters, numbers, underscore and dash!\" ) return True workspace ( self ) Gets the current workspace Source code in shrike/pipeline/pipeline_helper.py def workspace ( self ): \"\"\"Gets the current workspace\"\"\" return current_workspace ()","title":"pipeline_helper"},{"location":"pipeline/pipeline-helper/#pipeline-helper","text":"Pipeline helper class to create pipelines loading modules from a flexible manifest.","title":"Pipeline helper"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper","text":"Helper class for building pipelines","title":"AMLPipelineHelper"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.__init__","text":"Constructs the pipeline helper. Parameters: Name Type Description Default config DictConfig config for this object required module_loader AMLModuleLoader which module loader to (re)use None Source code in shrike/pipeline/pipeline_helper.py def __init__ ( self , config , module_loader = None ): \"\"\"Constructs the pipeline helper. Args: config (DictConfig): config for this object module_loader (AMLModuleLoader): which module loader to (re)use \"\"\" self . config = config if module_loader is None : print ( f \"Creating instance of AMLModuleLoader for { self . __class__ . __name__ } \" ) self . module_loader = AMLModuleLoader ( self . config ) else : self . module_loader = module_loader","title":"__init__()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.apply_recommended_runsettings","text":"Applies regular settings for a given module. Parameters: Name Type Description Default module_name str name of the module from the module manifest (required_modules() method) required module_instance Module the aml module we need to add settings to required gpu bool is the module using gpu? False hdi bool is the module using hdi/spark? 'auto' windows bool is the module using windows compute? 'auto' parallel bool is the module using ParallelRunStep? 'auto' mpi bool is the module using Mpi? 'auto' custom_runtime_arguments dict any additional custom args {} Source code in shrike/pipeline/pipeline_helper.py def apply_recommended_runsettings ( self , module_name , module_instance , gpu = False , # can't autodetect that hdi = \"auto\" , windows = \"auto\" , parallel = \"auto\" , mpi = \"auto\" , scope = \"auto\" , datatransfer = \"auto\" , ** custom_runtime_arguments , ): \"\"\"Applies regular settings for a given module. Args: module_name (str): name of the module from the module manifest (required_modules() method) module_instance (Module): the aml module we need to add settings to gpu (bool): is the module using gpu? hdi (bool): is the module using hdi/spark? windows (bool): is the module using windows compute? parallel (bool): is the module using ParallelRunStep? mpi (bool): is the module using Mpi? custom_runtime_arguments (dict): any additional custom args \"\"\" # verifies if module_name corresponds to module_instance self . _check_module_runsettings_consistency ( module_name , module_instance ) # Auto detect runsettings if hdi == \"auto\" : hdi = str ( module_instance . type ) == \"HDInsightComponent\" if hdi : print ( f \"Module { module_name } detected as HDI: { hdi } \" ) if parallel == \"auto\" : parallel = str ( module_instance . type ) == \"ParallelComponent\" if parallel : print ( f \"Module { module_name } detected as PARALLEL: { parallel } \" ) if mpi == \"auto\" : mpi = str ( module_instance . type ) == \"DistributedComponent\" if mpi : print ( f \"Module { module_name } detected as MPI: { mpi } \" ) if scope == \"auto\" : scope = str ( module_instance . type ) == \"ScopeComponent\" if scope : print ( f \"Module { module_name } detected as SCOPE: { scope } \" ) if windows == \"auto\" : if ( str ( module_instance . type ) == \"HDInsightComponent\" or str ( module_instance . type ) == \"ScopeComponent\" or str ( module_instance . type ) == \"DataTransferComponent\" ): # hdi/scope/datatransfer modules might not have that environment object windows = False else : windows = ( module_instance . _definition . environment . os . lower () == \"windows\" ) if windows : print ( f \"Module { module_name } detected as WINDOWS: { windows } \" ) if datatransfer == \"auto\" : datatransfer = str ( module_instance . type ) == \"DataTransferComponent\" if datatransfer : print ( f \"Module { module_name } detected as DATATRANSFER: { datatransfer } \" ) if parallel : self . _apply_parallel_runsettings ( module_name , module_instance , windows = windows , gpu = gpu , ** custom_runtime_arguments , ) return if windows : # no detonation chamber, we an't use \"use_local\" here self . _apply_windows_runsettings ( module_name , module_instance , mpi = mpi , ** custom_runtime_arguments ) return if hdi : # no detonation chamber, we an't use \"use_local\" here self . _apply_hdi_runsettings ( module_name , module_instance , ** custom_runtime_arguments ) return if scope : self . _apply_scope_runsettings ( module_name , module_instance , ** custom_runtime_arguments ) return if datatransfer : self . _apply_datatransfer_runsettings ( module_name , module_instance , ** custom_runtime_arguments ) return self . _apply_linux_runsettings ( module_name , module_instance , mpi = mpi , gpu = gpu , ** custom_runtime_arguments )","title":"apply_recommended_runsettings()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.build","text":"Builds a pipeline function for this pipeline. Parameters: Name Type Description Default config DictConfig configuration object (see get_config_class()) required Returns: Type Description pipeline_function the function to create your pipeline Source code in shrike/pipeline/pipeline_helper.py def build ( self , config ): \"\"\"Builds a pipeline function for this pipeline. Args: config (DictConfig): configuration object (see get_config_class()) Returns: pipeline_function: the function to create your pipeline \"\"\" raise NotImplementedError ( \"You need to implement your build() method.\" )","title":"build()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.canary","text":"Tests the output of the pipeline Source code in shrike/pipeline/pipeline_helper.py def canary ( self , args , experiment , pipeline_run ): \"\"\"Tests the output of the pipeline\"\"\" pass","title":"canary()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.component_load","text":"Loads one component from the manifest Source code in shrike/pipeline/pipeline_helper.py def component_load ( self , component_key ): \"\"\"Loads one component from the manifest\"\"\" return self . module_loader . load_module ( component_key , self . required_modules ())","title":"component_load()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.connect","text":"Connect to the AML workspace using internal config Source code in shrike/pipeline/pipeline_helper.py def connect ( self ): \"\"\"Connect to the AML workspace using internal config\"\"\" return azureml_connect ( aml_subscription_id = self . config . aml . subscription_id , aml_resource_group = self . config . aml . resource_group , aml_workspace_name = self . config . aml . workspace_name , aml_auth = self . config . aml . auth , aml_tenant = self . config . aml . tenant , aml_force = self . config . aml . force , ) # NOTE: this also stores aml workspace in internal global variable","title":"connect()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.dataset_load","text":"Loads a dataset by either id or name. Parameters: Name Type Description Default name str name or uuid of dataset to load required version str if loading by name, used to specify version (default \"latest\") 'latest' NOTE: in AzureML SDK there are 2 different methods for loading dataset one for id, one for name. This method just wraps them up in one. Source code in shrike/pipeline/pipeline_helper.py def dataset_load ( self , name , version = \"latest\" ): \"\"\"Loads a dataset by either id or name. Args: name (str): name or uuid of dataset to load version (str): if loading by name, used to specify version (default \"latest\") NOTE: in AzureML SDK there are 2 different methods for loading dataset one for id, one for name. This method just wraps them up in one.\"\"\" # test if given name is a uuid try : parsed_uuid = uuid . UUID ( name ) print ( f \"Getting a dataset handle [id= { name } ]...\" ) return Dataset . get_by_id ( self . workspace (), id = name ) except ValueError : print ( f \"Getting a dataset handle [name= { name } version= { version } ]...\" ) return Dataset . get_by_name ( self . workspace (), name = name , version = version )","title":"dataset_load()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.get_config_class","text":"Returns a dataclass containing config for this pipeline Source code in shrike/pipeline/pipeline_helper.py @classmethod def get_config_class ( cls ): \"\"\"Returns a dataclass containing config for this pipeline\"\"\" pass","title":"get_config_class()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.main","text":"Pipeline helper main function, parses arguments and run pipeline. Source code in shrike/pipeline/pipeline_helper.py @classmethod def main ( cls ): \"\"\"Pipeline helper main function, parses arguments and run pipeline.\"\"\" config_dict = cls . _default_config () @hydra . main ( config_name = \"default\" ) def hydra_run ( cfg : DictConfig ): # merge cli config with default config cfg = OmegaConf . merge ( config_dict , cfg ) print ( \"*** CONFIGURATION ***\" ) print ( OmegaConf . to_yaml ( cfg )) # create class instance main_instance = cls ( cfg ) # run main_instance . run () hydra_run () return cls . BUILT_PIPELINE # return so we can have some unit tests done","title":"main()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.module_load","text":"Loads one module from the manifest Source code in shrike/pipeline/pipeline_helper.py def module_load ( self , module_key ): \"\"\"Loads one module from the manifest\"\"\" return self . module_loader . load_module ( module_key , self . required_modules ())","title":"module_load()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.pipeline_instance","text":"Creates an instance of the pipeline using arguments. Parameters: Name Type Description Default pipeline_function function the pipeline function obtained from self.build() required config DictConfig configuration object (see get_config_class()) required Returns: Type Description pipeline the instance constructed using build() function Source code in shrike/pipeline/pipeline_helper.py def pipeline_instance ( self , pipeline_function , config ): \"\"\"Creates an instance of the pipeline using arguments. Args: pipeline_function (function): the pipeline function obtained from self.build() config (DictConfig): configuration object (see get_config_class()) Returns: pipeline: the instance constructed using build() function \"\"\" raise NotImplementedError ( \"You need to implement your pipeline_instance() method.\" )","title":"pipeline_instance()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.required_modules","text":"Dependencies on modules/components Returns: Type Description dict[str, dict] manifest Source code in shrike/pipeline/pipeline_helper.py @classmethod def required_modules ( cls ): \"\"\"Dependencies on modules/components Returns: dict[str, dict]: manifest \"\"\" return {}","title":"required_modules()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.required_subgraphs","text":"Dependencies on other subgraphs Returns: Type Description dict[str, AMLPipelineHelper] dictionary of subgraphs used for building this one. keys are whatever string you want for building your graph values should be classes inherinting from AMLPipelineHelper. Source code in shrike/pipeline/pipeline_helper.py @classmethod def required_subgraphs ( cls ): \"\"\"Dependencies on other subgraphs Returns: dict[str, AMLPipelineHelper]: dictionary of subgraphs used for building this one. keys are whatever string you want for building your graph values should be classes inherinting from AMLPipelineHelper. \"\"\" return {}","title":"required_subgraphs()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.run","text":"Run pipeline using arguments Source code in shrike/pipeline/pipeline_helper.py def run ( self ): \"\"\"Run pipeline using arguments\"\"\" # Log the telemetry information in the Azure Application Insights telemetry_logger = TelemetryLogger ( enable_telemetry = not self . config . run . disable_telemetry ) telemetry_logger . log_trace ( message = f \"shrike.pipeline== { __version__ } \" , properties = { \"custom_dimensions\" : { \"configuration\" : str ( self . config )}}, ) # Check whether the experiment name is valid self . validate_experiment_name ( self . config . run . experiment_name ) repository_info = get_repo_info () print ( f \"Running from repository: { repository_info } \" ) print ( \"azureml.core.VERSION = {} \" . format ( azureml . core . VERSION )) self . connect () if self . config . run . verbose : logging . getLogger () . setLevel ( logging . DEBUG ) if self . config . run . resume : if not self . config . run . pipeline_run_id : raise Exception ( \"To be able to use --resume you need to provide both --experiment-name and --run-id.\" ) print ( f \"Resuming Experiment { self . config . run . experiment_name } ...\" ) experiment = Experiment ( current_workspace (), self . config . run . experiment_name ) print ( f \"Resuming PipelineRun { self . config . run . pipeline_run_id } ...\" ) # pipeline_run is of the class \"azureml.pipeline.core.PipelineRun\" pipeline_run = PipelineRun ( experiment , self . config . run . pipeline_run_id ) else : print ( f \"Building Pipeline [ { self . __class__ . __name__ } ]...\" ) pipeline_function = self . build ( self . config ) print ( \"Creating Pipeline Instance...\" ) pipeline = self . pipeline_instance ( pipeline_function , self . config ) print ( \"Validating...\" ) pipeline . validate () if self . config . run . export : print ( f \"Exporting to { self . config . run . export } ...\" ) with open ( self . config . run . export , \"w\" ) as export_file : export_file . write ( pipeline . _get_graph_json ()) if self . config . run . submit : pipeline_tags = self . _parse_pipeline_tags () pipeline_tags . update ( repository_info ) print ( f \"Submitting Experiment... [tags= { pipeline_tags } ]\" ) # pipeline_run is of the class \"azure.ml.component.run\", which # is different from \"azureml.pipeline.core.PipelineRun\" pipeline_run = pipeline . submit ( experiment_name = self . config . run . experiment_name , tags = pipeline_tags , default_compute_target = self . config . compute . default_compute_target , regenerate_outputs = self . config . run . regenerate_outputs , continue_on_step_failure = self . config . run . continue_on_failure , ) # Forece pipeline_run to be of the class \"azureml.pipeline.core.PipelineRun\" pipeline_run = PipelineRun ( experiment = pipeline_run . _experiment , run_id = pipeline_run . _id , ) else : print ( \"Exiting now, if you want to submit please override run.submit=True\" ) self . __class__ . BUILT_PIPELINE = ( pipeline # return so we can have some unit tests done ) return # launch the pipeline execution print ( f \"Pipeline Run Id: { pipeline_run . id } \" ) print ( f \"\"\" ################################# ################################# ################################# Follow link below to access your pipeline run directly: ------------------------------------------------------- { pipeline_run . get_portal_url () } ################################# ################################# ################################# \"\"\" ) if self . config . run . canary : print ( \"*** CANARY MODE *** \\n ----------------------------------------------------------\" ) pipeline_run . wait_for_completion ( show_output = True ) # azureml.pipeline.core.PipelineRun.get_status(): [\"Running\", \"Finished\", \"Failed\"] # azureml.core.run.get_status(): [\"Running\", \"Completed\", \"Failed\"] if pipeline_run . get_status () in [ \"Finished\" , \"Completed\" ]: print ( \"*** PIPELINE FINISHED, TESTING WITH canary() METHOD ***\" ) self . canary ( self . config , pipeline_run . experiment , pipeline_run ) print ( \"OK\" ) elif pipeline_run . get_status () == \"Failed\" : print ( \"*** PIPELINE FAILED ***\" ) raise Exception ( \"Pipeline failed.\" ) else : print ( \"*** PIPELINE STATUS {} UNKNOWN ***\" ) raise Exception ( \"Pipeline status is unknown.\" ) else : if not self . config . run . silent : webbrowser . open ( url = pipeline_run . get_portal_url ()) # This will wait for the completion of the pipeline execution # and show the full logs in the meantime if self . config . run . resume or self . config . run . wait : print ( \"Below are the raw debug logs from your pipeline execution: \\n ----------------------------------------------------------\" ) pipeline_run . wait_for_completion ( show_output = True )","title":"run()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.subgraph_load","text":"Loads one subgraph from the manifest Source code in shrike/pipeline/pipeline_helper.py def subgraph_load ( self , subgraph_key ): \"\"\"Loads one subgraph from the manifest\"\"\" subgraph_class = self . required_subgraphs ()[ subgraph_key ] print ( f \"Building subgraph [ { subgraph_key } as { subgraph_class . __name__ } ]...\" ) # NOTE: below creates subgraph with same pipeline_config subgraph_instance = subgraph_class ( config = self . config , module_loader = self . module_loader ) # subgraph_instance.setup(self.pipeline_config) return subgraph_instance . build ( self . config )","title":"subgraph_load()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.validate_experiment_name","text":"Check whether the experiment name is valid. It's required that experiment names must be between 1 to 250 characters, start\u202fwith letters or numbers. Valid characters are letters, numbers, \"_\", and the \"-\" character. Source code in shrike/pipeline/pipeline_helper.py @staticmethod def validate_experiment_name ( name ): \"\"\" Check whether the experiment name is valid. It's required that experiment names must be between 1 to 250 characters, start\u202fwith letters or numbers. Valid characters are letters, numbers, \"_\", and the \"-\" character. \"\"\" if len ( name ) < 1 or len ( name ) > 250 : raise ValueError ( \"Experiment names must be between 1 to 250 characters!\" ) if not re . match ( \"^[a-zA-Z0-9]$\" , name [ 0 ]): raise ValueError ( \"Experiment names must start\u202fwith letters or numbers!\" ) if not re . match ( \"^[a-zA-Z0-9_-]*$\" , name ): raise ValueError ( \"Valiad experiment names must only contain letters, numbers, underscore and dash!\" ) return True","title":"validate_experiment_name()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.workspace","text":"Gets the current workspace Source code in shrike/pipeline/pipeline_helper.py def workspace ( self ): \"\"\"Gets the current workspace\"\"\" return current_workspace ()","title":"workspace()"},{"location":"pipeline/reuse-aml-pipeline/","text":"General process for reusing and configuring an existing AML experiment Motivations Essentially, an AML experiment is made of: - a runnable python script, - a configuration file. They are checked in a repository, making it reusable and shareable in your team. In many cases, it will be easy to copy an existing experiment to make it your own by just modifying the configuration file. In some instances, you'll want to modify the python script as well to have a more in-depth change on the structure of the graph. 1. Branch from an existing experiment In this section, we'll assume you have identified an existing runnable script (ex: baseline.py ) and a configuration file (ex: conf/reference/baseline.yaml ) 1. Create a new branch in your team's repository (to avoid conflicts). In the conf/reference/ or conf/experiments/ folders, identify a config file you want to start from. Copy this file into a new configuration file of your own. If this configuration file has a module manifest specified in the defaults section, as an example: defaults : - aml : eyesoff - compute : eyesoff - modules : baseline # list of modules + versions, see conf/modules/ Copy the file under conf/modules/baseline.yaml to a file of your own (ex: baseline-modelX.yaml ), and rename the name under defaults in your experiment config accordingly. defaults : - aml : eyesoff - compute : eyesoff - modules : baseline-modelX # <<< modify here In the following sections, you will be able to pin each module in this manifest to a particular version number, different from your colleagues version numbers. In the run section, modify your experiment name: run : # params for running pipeline experiment_name : \"modelX\" # <<< modify here Note: we recommend to use a different experiment for each project you work on. Experiments are essentially just a folder to put all your runs in order. That's it, at this point we invite you to try running the script for validating the graph (see below as an example): python pipelines / reference / baseline . py - -config-dir ./ conf - -config-name experiments / modelX This will try to build the graph, but will not submit it as an experiment. You're ready to modify the experiment now. 2. Modifying the code (best practices) Here's a couple recommendations depending on what you want to do. Your experiment consists in modifying modules only If your experiment consists in modifying one or several modules only (not the structure of the graph). work in a branch containing your module code use local code to experiment create a specific configuration file for your experiment that shows which module versions to use when satisfied, merge in to main/master and register the new versions for your team to share. Your experiment consists in modifying the graph structure work in a branch containing your graph identify conflicts of versions with your team and either create a new graph or modify the existing one with options to switch on/off your changes create a specific configuration file for your experiment that shows which module versions to use when satisfied, merge in to main/master so that the team can reuse the new graph 3. Experiment with local code If you want to modify a particular module for your experiment, we recommend to iterate on the module code using detonation chamber. IMPORTANT : this is NOT possible for HDI modules. If you want to modify HDI modules, we recommend to test those first in eyes-on, or to register new versions of those HDI modules in parallel of your experiment branch (create a specific branch for your new module versions). To use the local code for a given module: Identify the module key Identify the module key , this is the key used to map to the module in the graphs/subgraphs code. Go to the graph or subgraph you want to modify, check in the build() function to identify the module load key used. For instance below we want to modify VocabGenerator : def build ( self , config ): # ... vocab_generator = self . module_load ( \"VocabGenerator\" ) # ... If your pipeline uses the required_modules() method, this key will match with an entry in the required modules dictionary: @classmethod def required_modules ( cls ): return { \"VocabGenerator\" :{ # references of remote module \"remote_module_name\" : \"SparkVocabGenerator\" , \"namespace\" : \"microsoft.com/office/smartcompose\" , \"version\" : None , # references of local module \"yaml_spec\" : \"spark_vocab_generator/module_spec.yaml\" }, The key here is VocabGenerator , which is not the module name, but its key in the required modules dictionary. If your pipeline uses a module manifest in yaml (recommended!), this key will map to an entry in the modules manifest file conf/modules/baseline-modelX.yaml : manifest : # ... - key : \"VocabGenerator\" name : \"microsoft.com/office/smartcompose://SparkVocabGenerator\" version : null yaml : \"spark_vocab_generator/module_spec.yaml\" # ... The key here is VocabGenerator , which is not the module name, but its key in the required modules dictionary. Note: if no key is specified, the name is used as key. Use module key to run this module locally Use the use_local command with that key. You can either add it to the command line: python pipelines / reference / baseline . py - -config-dir ./ conf - -config-name experiments / modelX module_loader . use_local = \"VocabGenerator\" Or you can write it in your configuration file: # module_loader module_loader : # module loading params # IMPORTANT: if you want to modify a given module, add its key here # see the code for identifying the module key # use comma separation in this string to use multiple local modules use_local : \"VocabGenerator\" When running the experiment, watch-out in the logs for a line that will indicate this module has loaded from local code: Building subgraph [Encoding as EncodingHdiPipeline]... --- Building module from local code at spark_vocab_generator/module_spec.yaml 4. Experiment with different versions of (registered) modules The way the helper code decides which version to use for a given module M is (in order): if module_loader.force_all_module_version is set, use it as version for module M (and all others) if a version is set under module M in modules.manifest , use it if a version is hardcoded in required_modules() for module M, use it if module_loader.force_default_module_version is set, use it as version for module M (and all others non specified versions) else, use default version registered in AML (usually, the latest). Version management for your experiment modules can have multiple use cases. Use a specific version for all unspecified ( force_default_module_version ) If all your modules versions are synchronized in the registration build, you can use this to use a single version number accross all the graph for all modules that have unspecified versions ( version:null ). If you want to pin down a specific version number outside of this, you can add a specific version in your module manifest, or in the required_modules() method. Use a specific version for all modules If all your modules versions are synchronized in the registration build, you can use this to use a single version number accross all the graph for all modules. This will give you an exact replication of the modules at a particular point in time. This will override all other version settings. Use specific versions for some modules If you want to pin down a specific version number for some particular modules, specify this version in the module manifest: # @package _group_ manifest : # ... - key : \"LMPytorchTrainer\" name : \"[SC] [AML] PyTorch LM Trainer\" version : null # <<< HERE yaml : \"pytorch_trainer/module_spec.yaml\" # ... or hardcode it (not recommended) in your required_modules() method: @classmethod def required_modules ( cls ): return { \"LMPytorchTrainer\" :{ \"remote_module_name\" : \"[SC] [AML] PyTorch LM Trainer\" , \"namespace\" : \"microsoft.com/office/smartcompose\" , \"version\" : None , # <<< HERE \"yaml_spec\" : \"pytorch_trainer/module_spec.yaml\" } }","title":"Reuse your AML pipeline"},{"location":"pipeline/reuse-aml-pipeline/#general-process-for-reusing-and-configuring-an-existing-aml-experiment","text":"","title":"General process for reusing and configuring an existing AML experiment"},{"location":"pipeline/reuse-aml-pipeline/#motivations","text":"Essentially, an AML experiment is made of: - a runnable python script, - a configuration file. They are checked in a repository, making it reusable and shareable in your team. In many cases, it will be easy to copy an existing experiment to make it your own by just modifying the configuration file. In some instances, you'll want to modify the python script as well to have a more in-depth change on the structure of the graph.","title":"Motivations"},{"location":"pipeline/reuse-aml-pipeline/#1-branch-from-an-existing-experiment","text":"In this section, we'll assume you have identified an existing runnable script (ex: baseline.py ) and a configuration file (ex: conf/reference/baseline.yaml ) 1. Create a new branch in your team's repository (to avoid conflicts). In the conf/reference/ or conf/experiments/ folders, identify a config file you want to start from. Copy this file into a new configuration file of your own. If this configuration file has a module manifest specified in the defaults section, as an example: defaults : - aml : eyesoff - compute : eyesoff - modules : baseline # list of modules + versions, see conf/modules/ Copy the file under conf/modules/baseline.yaml to a file of your own (ex: baseline-modelX.yaml ), and rename the name under defaults in your experiment config accordingly. defaults : - aml : eyesoff - compute : eyesoff - modules : baseline-modelX # <<< modify here In the following sections, you will be able to pin each module in this manifest to a particular version number, different from your colleagues version numbers. In the run section, modify your experiment name: run : # params for running pipeline experiment_name : \"modelX\" # <<< modify here Note: we recommend to use a different experiment for each project you work on. Experiments are essentially just a folder to put all your runs in order. That's it, at this point we invite you to try running the script for validating the graph (see below as an example): python pipelines / reference / baseline . py - -config-dir ./ conf - -config-name experiments / modelX This will try to build the graph, but will not submit it as an experiment. You're ready to modify the experiment now.","title":"1. Branch from an existing experiment"},{"location":"pipeline/reuse-aml-pipeline/#2-modifying-the-code-best-practices","text":"Here's a couple recommendations depending on what you want to do.","title":"2. Modifying the code (best practices)"},{"location":"pipeline/reuse-aml-pipeline/#your-experiment-consists-in-modifying-modules-only","text":"If your experiment consists in modifying one or several modules only (not the structure of the graph). work in a branch containing your module code use local code to experiment create a specific configuration file for your experiment that shows which module versions to use when satisfied, merge in to main/master and register the new versions for your team to share.","title":"Your experiment consists in modifying modules only"},{"location":"pipeline/reuse-aml-pipeline/#your-experiment-consists-in-modifying-the-graph-structure","text":"work in a branch containing your graph identify conflicts of versions with your team and either create a new graph or modify the existing one with options to switch on/off your changes create a specific configuration file for your experiment that shows which module versions to use when satisfied, merge in to main/master so that the team can reuse the new graph","title":"Your experiment consists in modifying the graph structure"},{"location":"pipeline/reuse-aml-pipeline/#3-experiment-with-local-code","text":"If you want to modify a particular module for your experiment, we recommend to iterate on the module code using detonation chamber. IMPORTANT : this is NOT possible for HDI modules. If you want to modify HDI modules, we recommend to test those first in eyes-on, or to register new versions of those HDI modules in parallel of your experiment branch (create a specific branch for your new module versions). To use the local code for a given module:","title":"3. Experiment with local code"},{"location":"pipeline/reuse-aml-pipeline/#identify-the-module-key","text":"Identify the module key , this is the key used to map to the module in the graphs/subgraphs code. Go to the graph or subgraph you want to modify, check in the build() function to identify the module load key used. For instance below we want to modify VocabGenerator : def build ( self , config ): # ... vocab_generator = self . module_load ( \"VocabGenerator\" ) # ... If your pipeline uses the required_modules() method, this key will match with an entry in the required modules dictionary: @classmethod def required_modules ( cls ): return { \"VocabGenerator\" :{ # references of remote module \"remote_module_name\" : \"SparkVocabGenerator\" , \"namespace\" : \"microsoft.com/office/smartcompose\" , \"version\" : None , # references of local module \"yaml_spec\" : \"spark_vocab_generator/module_spec.yaml\" }, The key here is VocabGenerator , which is not the module name, but its key in the required modules dictionary. If your pipeline uses a module manifest in yaml (recommended!), this key will map to an entry in the modules manifest file conf/modules/baseline-modelX.yaml : manifest : # ... - key : \"VocabGenerator\" name : \"microsoft.com/office/smartcompose://SparkVocabGenerator\" version : null yaml : \"spark_vocab_generator/module_spec.yaml\" # ... The key here is VocabGenerator , which is not the module name, but its key in the required modules dictionary. Note: if no key is specified, the name is used as key.","title":"Identify the module key"},{"location":"pipeline/reuse-aml-pipeline/#use-module-key-to-run-this-module-locally","text":"Use the use_local command with that key. You can either add it to the command line: python pipelines / reference / baseline . py - -config-dir ./ conf - -config-name experiments / modelX module_loader . use_local = \"VocabGenerator\" Or you can write it in your configuration file: # module_loader module_loader : # module loading params # IMPORTANT: if you want to modify a given module, add its key here # see the code for identifying the module key # use comma separation in this string to use multiple local modules use_local : \"VocabGenerator\" When running the experiment, watch-out in the logs for a line that will indicate this module has loaded from local code: Building subgraph [Encoding as EncodingHdiPipeline]... --- Building module from local code at spark_vocab_generator/module_spec.yaml","title":"Use module key to run this module locally"},{"location":"pipeline/reuse-aml-pipeline/#4-experiment-with-different-versions-of-registered-modules","text":"The way the helper code decides which version to use for a given module M is (in order): if module_loader.force_all_module_version is set, use it as version for module M (and all others) if a version is set under module M in modules.manifest , use it if a version is hardcoded in required_modules() for module M, use it if module_loader.force_default_module_version is set, use it as version for module M (and all others non specified versions) else, use default version registered in AML (usually, the latest). Version management for your experiment modules can have multiple use cases.","title":"4. Experiment with different versions of (registered) modules"},{"location":"pipeline/reuse-aml-pipeline/#use-a-specific-version-for-all-unspecified-force_default_module_version","text":"If all your modules versions are synchronized in the registration build, you can use this to use a single version number accross all the graph for all modules that have unspecified versions ( version:null ). If you want to pin down a specific version number outside of this, you can add a specific version in your module manifest, or in the required_modules() method.","title":"Use a specific version for all unspecified (force_default_module_version)"},{"location":"pipeline/reuse-aml-pipeline/#use-a-specific-version-for-all-modules","text":"If all your modules versions are synchronized in the registration build, you can use this to use a single version number accross all the graph for all modules. This will give you an exact replication of the modules at a particular point in time. This will override all other version settings.","title":"Use a specific version for all modules"},{"location":"pipeline/reuse-aml-pipeline/#use-specific-versions-for-some-modules","text":"If you want to pin down a specific version number for some particular modules, specify this version in the module manifest: # @package _group_ manifest : # ... - key : \"LMPytorchTrainer\" name : \"[SC] [AML] PyTorch LM Trainer\" version : null # <<< HERE yaml : \"pytorch_trainer/module_spec.yaml\" # ... or hardcode it (not recommended) in your required_modules() method: @classmethod def required_modules ( cls ): return { \"LMPytorchTrainer\" :{ \"remote_module_name\" : \"[SC] [AML] PyTorch LM Trainer\" , \"namespace\" : \"microsoft.com/office/smartcompose\" , \"version\" : None , # <<< HERE \"yaml_spec\" : \"pytorch_trainer/module_spec.yaml\" } }","title":"Use specific versions for some modules"},{"location":"pipeline/testing-components/","text":"PyTest suite for testing if run.py is aligned with module specification: Status: this code relates to the recipe and is a proposition component_run_get_arg_parser ( component_spec_path ) Tests is module run.py has function get_arg_parser(parser) Source code in shrike/pipeline/testing/components.py def component_run_get_arg_parser ( component_spec_path ): \"\"\"Tests is module run.py has function get_arg_parser(parser)\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) run_py_command , definition_command = find_run_py_in_command ( definition , use_component_sdk ) component_import_path = os . path . dirname ( component_spec_path ) run_py_absdir = os . path . join ( component_import_path , run_py_command ) assert os . path . isfile ( run_py_absdir ), \"Component {} has command {} using a python script {} that cannot be found\" . format ( component_spec_path , definition_command , run_py_command ) if component_import_path not in sys . path : sys . path . insert ( 0 , component_import_path ) try : assert os . path . isfile ( run_py_absdir ), f \"module command { run_py_absdir } should exist\" get_arg_parser_func = import_and_test_class ( run_py_absdir , \"get_arg_parser\" ) except : assert ( False ), \"importing {} function get_arg_parser() resulted in an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) try : returned_parser = get_arg_parser_func () except : assert ( False ), \"Component script {} .get_arg_parser() should be able to run on argparse.ArgumentParser, but raised an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) assert ( returned_parser is not None ), \"component script {} .get_arg_parser() is supposed to return a parser when provided with None, please add 'return parser' at the end of the function.\" . format ( run_py_absdir ) try : parser = argparse . ArgumentParser () returned_parser = get_arg_parser_func ( parser ) except : assert ( False ), \"Component script {} .get_arg_parser() should be able to run on argparse.ArgumentParser, but raised an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) assert ( returned_parser is not None ), \"Component script {} .get_arg_parser() is not supposed to return None, please add 'return parser' at the end of the function.\" . format ( run_py_absdir ) # test object equality assert ( returned_parser is parser ), \"Component script {} .get_arg_parser() is supposed to return the parser it was provided, please do not create a new instance if provided with a parser.\" . format ( run_py_absdir ) return parser component_run_py_import ( component_spec_path ) Try importing run.py, just to check if basic script passes syntax/imports checks Source code in shrike/pipeline/testing/components.py def component_run_py_import ( component_spec_path ): \"\"\"Try importing run.py, just to check if basic script passes syntax/imports checks\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) run_py_command , definition_command = find_run_py_in_command ( definition , use_component_sdk ) component_import_path = os . path . dirname ( component_spec_path ) run_py_absdir = os . path . join ( component_import_path , run_py_command ) assert os . path . isfile ( run_py_absdir ), \"Component {} has command {} using a python script {} that cannot be found\" . format ( component_spec_path , definition_command , run_py_command ) if component_import_path not in sys . path : sys . path . insert ( 0 , component_import_path ) try : spec , mod = dynamic_import_module ( run_py_absdir ) except : assert False , \"importing {} resulted in an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) component_spec_yaml_exists_and_is_parsable ( component_spec_path ) Checks component spec file Source code in shrike/pipeline/testing/components.py def component_spec_yaml_exists_and_is_parsable ( component_spec_path ): \"\"\"Checks component spec file\"\"\" assert os . path . isfile ( component_spec_path ), f \"Component spec file under path { component_spec_path } could not be found\" # opens file for testing schema with open ( component_spec_path , \"r\" ) as ifile : component_spec_content = ifile . read () if \"$schema: http://azureml/\" in component_spec_content : use_component_sdk = True else : use_component_sdk = False # Block unit tests from working with module sdk if not enabled if not os . environ . get ( \"MODULE_SDK_ENABLE\" ): assert ( use_component_sdk ), \"These unit tests are intentionnally blocked from support Module SDK, which is DEPRECATED. To bypass, create env variable MODULE_SDK_ENABLE.\" if use_component_sdk : try : definition = ComponentDefinition . load ( component_spec_path ) except BaseException as e : assert ( False ), \"Failed: failed to load (sdk 2.0) component yaml %r , exception= %r \" % ( component_spec_path , e , ) else : try : with open ( component_spec_path , \"r\" ) as ifile : definition = yaml . safe_load ( ifile ) except BaseException as e : assert ( False ), \"Failed: failed to load (old style) module yaml %r , exception= %r \" % ( component_spec_path , e , ) return definition , use_component_sdk component_uses_private_acr ( component_spec_path , acr_url ) Tests base image in private ACR Source code in shrike/pipeline/testing/components.py def component_uses_private_acr ( component_spec_path , acr_url ): \"\"\"Tests base image in private ACR\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) if use_component_sdk : component_uses_private_acr_componentsdk ( component_spec_path , definition , acr_url ) else : component_uses_private_acr_modulesdk ( component_spec_path , definition , acr_url ) component_uses_private_acr_componentsdk ( component_spec_path , definition , acr_url ) Tests base image in private ACR Source code in shrike/pipeline/testing/components.py def component_uses_private_acr_componentsdk ( component_spec_path , definition , acr_url ): \"\"\"Tests base image in private ACR\"\"\" definition_type = definition . type if definition_type in [ ComponentType . HDInsightComponent , ComponentType . ScopeComponent , ComponentType . DataTransferComponent , ]: return try : base_image_url = definition . environment . docker [ \"image\" ] except KeyError : base_image_url = None pass if base_image_url is not None : assert base_image_url . startswith ( acr_url ), \"Component {} baseImage should be drawn from polymerprod, instead found url {} \" . format ( component_spec_path , base_image_url ) component_uses_private_acr_modulesdk ( component_spec_path , definition , acr_url ) Tests base image in private ACR Source code in shrike/pipeline/testing/components.py def component_uses_private_acr_modulesdk ( component_spec_path , definition , acr_url ): \"\"\"Tests base image in private ACR\"\"\" try : base_image_url = definition [ \"implementation\" ][ \"container\" ][ \"amlEnvironment\" ][ \"docker\" ][ \"baseImage\" ] except KeyError : base_image_url = None pass if base_image_url is not None : assert base_image_url . startswith ( acr_url ), \"Component(1.5) {} baseImage should be drawn from polymerprod, instead found url {} \" . format ( component_spec_path , base_image_url ) component_uses_private_python_feed ( component_spec_path , feed_url ) Tests private python feed referenced in conda Source code in shrike/pipeline/testing/components.py def component_uses_private_python_feed ( component_spec_path , feed_url ): \"\"\"Tests private python feed referenced in conda\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) if use_component_sdk : if definition . type in [ ComponentType . HDInsightComponent , ComponentType . ScopeComponent , ComponentType . DataTransferComponent , ]: return try : conda_deps_path = definition . environment . conda [ \"conda_dependencies_file\" ] except KeyError : conda_deps_path = None pass else : job_type = str ( definition [ \"jobType\" ]) . lower () if job_type in [ \"hdinsight\" , \"scopecomponent\" , \"datatransfercomponent\" ]: # hdi/scope/datatransfer jobs don't have python feed return if job_type == \"parallel\" : try : conda_deps_path = definition [ \"implementation\" ][ \"parallel\" ][ \"amlEnvironment\" ][ \"python\" ][ \"condaDependenciesFile\" ] except KeyError : conda_deps_path = None pass else : try : conda_deps_path = definition [ \"implementation\" ][ \"container\" ][ \"amlEnvironment\" ][ \"python\" ][ \"condaDependenciesFile\" ] except KeyError : conda_deps_path = None pass if conda_deps_path is None : # no conda yaml provided, nothing to do here return conda_deps_abspath = os . path . join ( os . path . dirname ( component_spec_path ), conda_deps_path ) assert os . path . isfile ( conda_deps_abspath ), \"Component {} specified a conda_dependencies_file {} that cannot be found (abspath: {} )\" . format ( component_spec_path , conda_deps_path , conda_deps_abspath ) try : with open ( conda_deps_abspath , \"r\" ) as ifile : conda_deps_yaml = yaml . safe_load ( ifile ) if \"channels\" in conda_deps_yaml : assert conda_deps_yaml [ \"channels\" ] == [ \".\" ], \"In conda deps {} no channels must be specified, or use . as channel\" . format ( conda_deps_abspath ) if \"dependencies\" in conda_deps_yaml : for entry in conda_deps_yaml [ \"dependencies\" ]: if \"pip\" in entry : assert ( f \"--index-url { feed_url } \" in entry [ \"pip\" ] ), \"conda deps under {} must reference private python feed under pip dependencies.\" . format ( conda_deps_abspath ) except : assert ( False ), \"Component {} conda_dependencies_file under path {} should be yaml parsable, but loading it raised an exception: {} \" . format ( component_spec_path , conda_deps_abspath , traceback . format_exc () ) find_run_py_in_command ( definition , use_component_sdk ) Finds runnable python script in command Source code in shrike/pipeline/testing/components.py def find_run_py_in_command ( definition , use_component_sdk ): \"\"\"Finds runnable python script in command\"\"\" run_py_command , definition_command = None , None if use_component_sdk : definition_type = definition . type if definition_type == ComponentType . HDInsightComponent : run_py_command = definition . file definition_command = definition . args elif definition_type == ComponentType . DistributedComponent : # run_py_command not provided, we need to find it definition_command = definition . launcher . additional_arguments elif definition_type == ComponentType . ParallelComponent : run_py_command = definition . entry definition_command = definition . args elif definition_type == ComponentType . CommandComponent : # run_py_command not provided, we need to find it definition_command = definition . command elif definition_type not in [ ComponentType . ScopeComponent , ComponentType . DataTransferComponent , ]: raise Exception ( f \"Component type { definition_type } is not supported in the helper code unit tests (yet).\" ) if ( run_py_command is None and definition . type != ComponentType . ScopeComponent and definition . type != ComponentType . DataTransferComponent ): # search for python script for entry in definition_command . split ( \" \" ): if entry . endswith ( \".py\" ): run_py_command = entry break else : assert ( False ), \"Could not find any script name like *.py in component command {} \" . format ( definition_command ) else : job_type = str ( definition [ \"jobType\" ]) . lower () if job_type == \"hdinsight\" : run_py_command = definition [ \"implementation\" ][ \"hdinsight\" ][ \"file\" ] definition_command = run_py_command elif job_type == \"parallel\" : run_py_command = definition [ \"implementation\" ][ \"parallel\" ][ \"entry\" ] definition_command = run_py_command elif job_type not in [ \"scopecomponent\" , \"datatransfercomponent\" ]: definition_command = definition [ \"implementation\" ][ \"container\" ][ \"command\" ] for entry in definition_command : if entry . endswith ( \".py\" ): run_py_command = entry break else : assert ( False ), \"Could not find any script name like *.py in component command {} \" . format ( definition_command . split ( \" \" ) ) return run_py_command , definition_command generate_component_arguments_componentsdk ( component_spec , arg , output_script_arguments ) Recursively generate fake arguments to test script argparse. Parameters: Name Type Description Default component_spec dict module specification in yaml required arg list or str or dict) argument specification required output_script_arguments list) output required Returns: Type Description list output_script_arguments Source code in shrike/pipeline/testing/components.py def generate_component_arguments_componentsdk ( component_spec , arg , output_script_arguments ): \"\"\"Recursively generate fake arguments to test script argparse. Args: component_spec (dict): module specification in yaml arg (list or str or dict) : argument specification output_script_arguments (list) : output Returns: list: output_script_arguments \"\"\" print ( f \"generate_component_arguments(spec, { arg } , ...)\" ) if isinstance ( arg , list ): # optional argument or root list for entry in arg : generate_component_arguments_componentsdk ( component_spec , entry , output_script_arguments ) elif isinstance ( arg , str ) and arg . startswith ( \"{\" ): io_key = arg . lstrip ( \"{\" ) . rstrip ( \"}\" ) if io_key . startswith ( \"inputs.\" ): input_key = io_key [ 7 :] print ( \"inputs keys: \" + \" \" . join ([ key for key in component_spec . inputs ])) print ( \"parameter keys: \" + \" \" . join ([ key for key in component_spec . parameters ]) ) if input_key in component_spec . inputs : output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( component_spec . inputs [ input_key ] ) ) ) elif input_key in component_spec . parameters : output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( component_spec . parameters [ input_key ] ) ) ) else : raise Exception ( f \"Input key { input_key } is neither an input or a parameter\" ) elif io_key . startswith ( \"outputs.\" ): output_key = io_key [ 8 :] print ( \"outputs keys: \" + \" \" . join ([ key for key in component_spec . outputs ])) output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( component_spec . outputs [ output_key ] ) ) ) else : raise NotImplementedError ( \"In argument spec {} , I/O key arg spec is not supported {} \" . format ( arg , io_key ) ) elif isinstance ( arg , str ): output_script_arguments . append ( arg ) elif isinstance ( arg , dict ): # for old module def if \"inputValue\" in arg : # find in inputs for i_spec in component_spec . inputs : if i_spec [ \"name\" ] == arg [ \"inputValue\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( i_spec )) ) elif \"inputPath\" in arg : # find in inputs for i_spec in component_spec . inputs : if i_spec [ \"name\" ] == arg [ \"inputPath\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( i_spec )) ) elif \"outputPath\" in arg : # find in outputs output_script_arguments . append ( \"/mnt/fakeoutputpath\" ) return output_script_arguments generate_component_arguments_modulesdk ( module_spec , arg , output_script_arguments ) Recursively generate fake arguments to test script argparse. Parameters: Name Type Description Default component_spec dict module specification in yaml required arg list or str or dict) argument specification required output_script_arguments list) output required Returns: Type Description list output_script_arguments Source code in shrike/pipeline/testing/components.py def generate_component_arguments_modulesdk ( module_spec , arg , output_script_arguments ): \"\"\"Recursively generate fake arguments to test script argparse. Args: component_spec (dict): module specification in yaml arg (list or str or dict) : argument specification output_script_arguments (list) : output Returns: list: output_script_arguments \"\"\" if isinstance ( arg , list ): # optional argument or root list for entry in arg : generate_component_arguments_modulesdk ( module_spec , entry , output_script_arguments ) elif isinstance ( arg , str ): output_script_arguments . append ( arg ) elif isinstance ( arg , dict ): if \"inputValue\" in arg : # find in inputs for i_spec in module_spec [ \"inputs\" ]: if i_spec [ \"name\" ] == arg [ \"inputValue\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_modulesdk ( i_spec )) ) elif \"inputPath\" in arg : # find in inputs for i_spec in module_spec [ \"inputs\" ]: if i_spec [ \"name\" ] == arg [ \"inputPath\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_modulesdk ( i_spec )) ) elif \"outputPath\" in arg : # find in outputs output_script_arguments . append ( \"/mnt/fakeoutputpath\" ) return output_script_arguments if_arguments_from_component_spec_match_script_argparse ( component_spec_path ) Tests alignment between spec arguments and script parser arguments Source code in shrike/pipeline/testing/components.py def if_arguments_from_component_spec_match_script_argparse ( component_spec_path ): \"\"\"Tests alignment between spec arguments and script parser arguments\"\"\" # assuming we have a yaml spec file that is loadable definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) # assuming we can import the get_arg_parser() function parser = component_run_get_arg_parser ( component_spec_path ) run_py_command , definition_command = find_run_py_in_command ( definition , use_component_sdk ) if use_component_sdk : arguments_spec = [ entry . lstrip ( \"[\" ) . rstrip ( \"]\" ) for entry in definition_command . split ( \" \" ) ] if arguments_spec [ 0 ] . startswith ( \"python\" ): arguments_spec . pop ( 0 ) if arguments_spec [ 0 ] . endswith ( \".py\" ): arguments_spec . pop ( 0 ) script_arguments = [] generate_component_arguments_componentsdk ( definition , arguments_spec , script_arguments ) else : job_type = str ( definition [ \"jobType\" ]) . lower () if job_type == \"hdinsight\" : arguments_spec = definition [ \"implementation\" ][ \"hdinsight\" ][ \"args\" ] elif job_type == \"parallel\" : arguments_spec = definition [ \"implementation\" ][ \"parallel\" ][ \"args\" ] elif job_type not in [ \"scopecomponent\" , \"datatransfercomponent\" ]: arguments_spec = definition [ \"implementation\" ][ \"container\" ][ \"args\" ] script_arguments = [] generate_component_arguments_modulesdk ( definition , arguments_spec , script_arguments ) try : _ , unknown_args = parser . parse_known_args ( script_arguments ) except : assert ( False ), \"Component {} , in run.py, parse_known_args() should be able to parse {} , instead raised an exception: {} \" . format ( component_spec_path , script_arguments , traceback . format_exc () ) assert ( len ( unknown_args ) == 0 ), \"Component {} , while calling run.py with args {} , parsing arguments from module spec should not return unknown args, instead we observed unknown args : {} \" . format ( component_spec_path , script_arguments , unknown_args ) script_main_with_synthetic_arguments ( module , mocker ) Try importing run.py, just to check if basic script passes syntax/imports checks Source code in shrike/pipeline/testing/components.py def script_main_with_synthetic_arguments ( module , mocker ): \"\"\"Try importing run.py, just to check if basic script passes syntax/imports checks\"\"\" paths = _get_module_paths ( module ) # assuming we have a yaml spec file that is loadable module_spec = module_spec_yaml_exists_and_is_parsable ( module ) # import module to get main() function if paths . module_spec_absdir not in sys . path : sys . path . insert ( 0 , paths . module_spec_absdir ) try : spec , mod = dynamic_import_module ( paths . module_import_path ) except : assert False , \"importing {} resulted in an exception: {} \" . format ( paths . module_import_path , traceback . format_exc () ) if module_spec [ \"jobType\" ] . lower () == \"hdinsight\" : arguments_spec = module_spec [ \"implementation\" ][ \"hdinsight\" ][ \"args\" ] elif ( module_spec [ \"jobType\" ] . lower () != \"scopecomponent\" and module_spec [ \"jobType\" ] . lower () != \"datatransfercomponent\" ): arguments_spec = module_spec [ \"implementation\" ][ \"container\" ][ \"args\" ] script_arguments = [] generate_argument ( module_spec , arguments_spec , script_arguments ) print ( script_arguments ) # https://medium.com/python-pandemonium/testing-sys-exit-with-pytest-10c6e5f7726f with pytest . raises ( SystemExit ) as pytest_wrapped_e : mod . main ( script_arguments + [ \"-h\" ]) assert pytest_wrapped_e . type == SystemExit assert pytest_wrapped_e . value . code == 0","title":"testing.componets"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_run_get_arg_parser","text":"Tests is module run.py has function get_arg_parser(parser) Source code in shrike/pipeline/testing/components.py def component_run_get_arg_parser ( component_spec_path ): \"\"\"Tests is module run.py has function get_arg_parser(parser)\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) run_py_command , definition_command = find_run_py_in_command ( definition , use_component_sdk ) component_import_path = os . path . dirname ( component_spec_path ) run_py_absdir = os . path . join ( component_import_path , run_py_command ) assert os . path . isfile ( run_py_absdir ), \"Component {} has command {} using a python script {} that cannot be found\" . format ( component_spec_path , definition_command , run_py_command ) if component_import_path not in sys . path : sys . path . insert ( 0 , component_import_path ) try : assert os . path . isfile ( run_py_absdir ), f \"module command { run_py_absdir } should exist\" get_arg_parser_func = import_and_test_class ( run_py_absdir , \"get_arg_parser\" ) except : assert ( False ), \"importing {} function get_arg_parser() resulted in an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) try : returned_parser = get_arg_parser_func () except : assert ( False ), \"Component script {} .get_arg_parser() should be able to run on argparse.ArgumentParser, but raised an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) assert ( returned_parser is not None ), \"component script {} .get_arg_parser() is supposed to return a parser when provided with None, please add 'return parser' at the end of the function.\" . format ( run_py_absdir ) try : parser = argparse . ArgumentParser () returned_parser = get_arg_parser_func ( parser ) except : assert ( False ), \"Component script {} .get_arg_parser() should be able to run on argparse.ArgumentParser, but raised an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) assert ( returned_parser is not None ), \"Component script {} .get_arg_parser() is not supposed to return None, please add 'return parser' at the end of the function.\" . format ( run_py_absdir ) # test object equality assert ( returned_parser is parser ), \"Component script {} .get_arg_parser() is supposed to return the parser it was provided, please do not create a new instance if provided with a parser.\" . format ( run_py_absdir ) return parser","title":"component_run_get_arg_parser()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_run_py_import","text":"Try importing run.py, just to check if basic script passes syntax/imports checks Source code in shrike/pipeline/testing/components.py def component_run_py_import ( component_spec_path ): \"\"\"Try importing run.py, just to check if basic script passes syntax/imports checks\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) run_py_command , definition_command = find_run_py_in_command ( definition , use_component_sdk ) component_import_path = os . path . dirname ( component_spec_path ) run_py_absdir = os . path . join ( component_import_path , run_py_command ) assert os . path . isfile ( run_py_absdir ), \"Component {} has command {} using a python script {} that cannot be found\" . format ( component_spec_path , definition_command , run_py_command ) if component_import_path not in sys . path : sys . path . insert ( 0 , component_import_path ) try : spec , mod = dynamic_import_module ( run_py_absdir ) except : assert False , \"importing {} resulted in an exception: {} \" . format ( run_py_absdir , traceback . format_exc () )","title":"component_run_py_import()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_spec_yaml_exists_and_is_parsable","text":"Checks component spec file Source code in shrike/pipeline/testing/components.py def component_spec_yaml_exists_and_is_parsable ( component_spec_path ): \"\"\"Checks component spec file\"\"\" assert os . path . isfile ( component_spec_path ), f \"Component spec file under path { component_spec_path } could not be found\" # opens file for testing schema with open ( component_spec_path , \"r\" ) as ifile : component_spec_content = ifile . read () if \"$schema: http://azureml/\" in component_spec_content : use_component_sdk = True else : use_component_sdk = False # Block unit tests from working with module sdk if not enabled if not os . environ . get ( \"MODULE_SDK_ENABLE\" ): assert ( use_component_sdk ), \"These unit tests are intentionnally blocked from support Module SDK, which is DEPRECATED. To bypass, create env variable MODULE_SDK_ENABLE.\" if use_component_sdk : try : definition = ComponentDefinition . load ( component_spec_path ) except BaseException as e : assert ( False ), \"Failed: failed to load (sdk 2.0) component yaml %r , exception= %r \" % ( component_spec_path , e , ) else : try : with open ( component_spec_path , \"r\" ) as ifile : definition = yaml . safe_load ( ifile ) except BaseException as e : assert ( False ), \"Failed: failed to load (old style) module yaml %r , exception= %r \" % ( component_spec_path , e , ) return definition , use_component_sdk","title":"component_spec_yaml_exists_and_is_parsable()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_uses_private_acr","text":"Tests base image in private ACR Source code in shrike/pipeline/testing/components.py def component_uses_private_acr ( component_spec_path , acr_url ): \"\"\"Tests base image in private ACR\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) if use_component_sdk : component_uses_private_acr_componentsdk ( component_spec_path , definition , acr_url ) else : component_uses_private_acr_modulesdk ( component_spec_path , definition , acr_url )","title":"component_uses_private_acr()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_uses_private_acr_componentsdk","text":"Tests base image in private ACR Source code in shrike/pipeline/testing/components.py def component_uses_private_acr_componentsdk ( component_spec_path , definition , acr_url ): \"\"\"Tests base image in private ACR\"\"\" definition_type = definition . type if definition_type in [ ComponentType . HDInsightComponent , ComponentType . ScopeComponent , ComponentType . DataTransferComponent , ]: return try : base_image_url = definition . environment . docker [ \"image\" ] except KeyError : base_image_url = None pass if base_image_url is not None : assert base_image_url . startswith ( acr_url ), \"Component {} baseImage should be drawn from polymerprod, instead found url {} \" . format ( component_spec_path , base_image_url )","title":"component_uses_private_acr_componentsdk()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_uses_private_acr_modulesdk","text":"Tests base image in private ACR Source code in shrike/pipeline/testing/components.py def component_uses_private_acr_modulesdk ( component_spec_path , definition , acr_url ): \"\"\"Tests base image in private ACR\"\"\" try : base_image_url = definition [ \"implementation\" ][ \"container\" ][ \"amlEnvironment\" ][ \"docker\" ][ \"baseImage\" ] except KeyError : base_image_url = None pass if base_image_url is not None : assert base_image_url . startswith ( acr_url ), \"Component(1.5) {} baseImage should be drawn from polymerprod, instead found url {} \" . format ( component_spec_path , base_image_url )","title":"component_uses_private_acr_modulesdk()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_uses_private_python_feed","text":"Tests private python feed referenced in conda Source code in shrike/pipeline/testing/components.py def component_uses_private_python_feed ( component_spec_path , feed_url ): \"\"\"Tests private python feed referenced in conda\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) if use_component_sdk : if definition . type in [ ComponentType . HDInsightComponent , ComponentType . ScopeComponent , ComponentType . DataTransferComponent , ]: return try : conda_deps_path = definition . environment . conda [ \"conda_dependencies_file\" ] except KeyError : conda_deps_path = None pass else : job_type = str ( definition [ \"jobType\" ]) . lower () if job_type in [ \"hdinsight\" , \"scopecomponent\" , \"datatransfercomponent\" ]: # hdi/scope/datatransfer jobs don't have python feed return if job_type == \"parallel\" : try : conda_deps_path = definition [ \"implementation\" ][ \"parallel\" ][ \"amlEnvironment\" ][ \"python\" ][ \"condaDependenciesFile\" ] except KeyError : conda_deps_path = None pass else : try : conda_deps_path = definition [ \"implementation\" ][ \"container\" ][ \"amlEnvironment\" ][ \"python\" ][ \"condaDependenciesFile\" ] except KeyError : conda_deps_path = None pass if conda_deps_path is None : # no conda yaml provided, nothing to do here return conda_deps_abspath = os . path . join ( os . path . dirname ( component_spec_path ), conda_deps_path ) assert os . path . isfile ( conda_deps_abspath ), \"Component {} specified a conda_dependencies_file {} that cannot be found (abspath: {} )\" . format ( component_spec_path , conda_deps_path , conda_deps_abspath ) try : with open ( conda_deps_abspath , \"r\" ) as ifile : conda_deps_yaml = yaml . safe_load ( ifile ) if \"channels\" in conda_deps_yaml : assert conda_deps_yaml [ \"channels\" ] == [ \".\" ], \"In conda deps {} no channels must be specified, or use . as channel\" . format ( conda_deps_abspath ) if \"dependencies\" in conda_deps_yaml : for entry in conda_deps_yaml [ \"dependencies\" ]: if \"pip\" in entry : assert ( f \"--index-url { feed_url } \" in entry [ \"pip\" ] ), \"conda deps under {} must reference private python feed under pip dependencies.\" . format ( conda_deps_abspath ) except : assert ( False ), \"Component {} conda_dependencies_file under path {} should be yaml parsable, but loading it raised an exception: {} \" . format ( component_spec_path , conda_deps_abspath , traceback . format_exc () )","title":"component_uses_private_python_feed()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.find_run_py_in_command","text":"Finds runnable python script in command Source code in shrike/pipeline/testing/components.py def find_run_py_in_command ( definition , use_component_sdk ): \"\"\"Finds runnable python script in command\"\"\" run_py_command , definition_command = None , None if use_component_sdk : definition_type = definition . type if definition_type == ComponentType . HDInsightComponent : run_py_command = definition . file definition_command = definition . args elif definition_type == ComponentType . DistributedComponent : # run_py_command not provided, we need to find it definition_command = definition . launcher . additional_arguments elif definition_type == ComponentType . ParallelComponent : run_py_command = definition . entry definition_command = definition . args elif definition_type == ComponentType . CommandComponent : # run_py_command not provided, we need to find it definition_command = definition . command elif definition_type not in [ ComponentType . ScopeComponent , ComponentType . DataTransferComponent , ]: raise Exception ( f \"Component type { definition_type } is not supported in the helper code unit tests (yet).\" ) if ( run_py_command is None and definition . type != ComponentType . ScopeComponent and definition . type != ComponentType . DataTransferComponent ): # search for python script for entry in definition_command . split ( \" \" ): if entry . endswith ( \".py\" ): run_py_command = entry break else : assert ( False ), \"Could not find any script name like *.py in component command {} \" . format ( definition_command ) else : job_type = str ( definition [ \"jobType\" ]) . lower () if job_type == \"hdinsight\" : run_py_command = definition [ \"implementation\" ][ \"hdinsight\" ][ \"file\" ] definition_command = run_py_command elif job_type == \"parallel\" : run_py_command = definition [ \"implementation\" ][ \"parallel\" ][ \"entry\" ] definition_command = run_py_command elif job_type not in [ \"scopecomponent\" , \"datatransfercomponent\" ]: definition_command = definition [ \"implementation\" ][ \"container\" ][ \"command\" ] for entry in definition_command : if entry . endswith ( \".py\" ): run_py_command = entry break else : assert ( False ), \"Could not find any script name like *.py in component command {} \" . format ( definition_command . split ( \" \" ) ) return run_py_command , definition_command","title":"find_run_py_in_command()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.generate_component_arguments_componentsdk","text":"Recursively generate fake arguments to test script argparse. Parameters: Name Type Description Default component_spec dict module specification in yaml required arg list or str or dict) argument specification required output_script_arguments list) output required Returns: Type Description list output_script_arguments Source code in shrike/pipeline/testing/components.py def generate_component_arguments_componentsdk ( component_spec , arg , output_script_arguments ): \"\"\"Recursively generate fake arguments to test script argparse. Args: component_spec (dict): module specification in yaml arg (list or str or dict) : argument specification output_script_arguments (list) : output Returns: list: output_script_arguments \"\"\" print ( f \"generate_component_arguments(spec, { arg } , ...)\" ) if isinstance ( arg , list ): # optional argument or root list for entry in arg : generate_component_arguments_componentsdk ( component_spec , entry , output_script_arguments ) elif isinstance ( arg , str ) and arg . startswith ( \"{\" ): io_key = arg . lstrip ( \"{\" ) . rstrip ( \"}\" ) if io_key . startswith ( \"inputs.\" ): input_key = io_key [ 7 :] print ( \"inputs keys: \" + \" \" . join ([ key for key in component_spec . inputs ])) print ( \"parameter keys: \" + \" \" . join ([ key for key in component_spec . parameters ]) ) if input_key in component_spec . inputs : output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( component_spec . inputs [ input_key ] ) ) ) elif input_key in component_spec . parameters : output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( component_spec . parameters [ input_key ] ) ) ) else : raise Exception ( f \"Input key { input_key } is neither an input or a parameter\" ) elif io_key . startswith ( \"outputs.\" ): output_key = io_key [ 8 :] print ( \"outputs keys: \" + \" \" . join ([ key for key in component_spec . outputs ])) output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( component_spec . outputs [ output_key ] ) ) ) else : raise NotImplementedError ( \"In argument spec {} , I/O key arg spec is not supported {} \" . format ( arg , io_key ) ) elif isinstance ( arg , str ): output_script_arguments . append ( arg ) elif isinstance ( arg , dict ): # for old module def if \"inputValue\" in arg : # find in inputs for i_spec in component_spec . inputs : if i_spec [ \"name\" ] == arg [ \"inputValue\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( i_spec )) ) elif \"inputPath\" in arg : # find in inputs for i_spec in component_spec . inputs : if i_spec [ \"name\" ] == arg [ \"inputPath\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( i_spec )) ) elif \"outputPath\" in arg : # find in outputs output_script_arguments . append ( \"/mnt/fakeoutputpath\" ) return output_script_arguments","title":"generate_component_arguments_componentsdk()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.generate_component_arguments_modulesdk","text":"Recursively generate fake arguments to test script argparse. Parameters: Name Type Description Default component_spec dict module specification in yaml required arg list or str or dict) argument specification required output_script_arguments list) output required Returns: Type Description list output_script_arguments Source code in shrike/pipeline/testing/components.py def generate_component_arguments_modulesdk ( module_spec , arg , output_script_arguments ): \"\"\"Recursively generate fake arguments to test script argparse. Args: component_spec (dict): module specification in yaml arg (list or str or dict) : argument specification output_script_arguments (list) : output Returns: list: output_script_arguments \"\"\" if isinstance ( arg , list ): # optional argument or root list for entry in arg : generate_component_arguments_modulesdk ( module_spec , entry , output_script_arguments ) elif isinstance ( arg , str ): output_script_arguments . append ( arg ) elif isinstance ( arg , dict ): if \"inputValue\" in arg : # find in inputs for i_spec in module_spec [ \"inputs\" ]: if i_spec [ \"name\" ] == arg [ \"inputValue\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_modulesdk ( i_spec )) ) elif \"inputPath\" in arg : # find in inputs for i_spec in module_spec [ \"inputs\" ]: if i_spec [ \"name\" ] == arg [ \"inputPath\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_modulesdk ( i_spec )) ) elif \"outputPath\" in arg : # find in outputs output_script_arguments . append ( \"/mnt/fakeoutputpath\" ) return output_script_arguments","title":"generate_component_arguments_modulesdk()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.if_arguments_from_component_spec_match_script_argparse","text":"Tests alignment between spec arguments and script parser arguments Source code in shrike/pipeline/testing/components.py def if_arguments_from_component_spec_match_script_argparse ( component_spec_path ): \"\"\"Tests alignment between spec arguments and script parser arguments\"\"\" # assuming we have a yaml spec file that is loadable definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) # assuming we can import the get_arg_parser() function parser = component_run_get_arg_parser ( component_spec_path ) run_py_command , definition_command = find_run_py_in_command ( definition , use_component_sdk ) if use_component_sdk : arguments_spec = [ entry . lstrip ( \"[\" ) . rstrip ( \"]\" ) for entry in definition_command . split ( \" \" ) ] if arguments_spec [ 0 ] . startswith ( \"python\" ): arguments_spec . pop ( 0 ) if arguments_spec [ 0 ] . endswith ( \".py\" ): arguments_spec . pop ( 0 ) script_arguments = [] generate_component_arguments_componentsdk ( definition , arguments_spec , script_arguments ) else : job_type = str ( definition [ \"jobType\" ]) . lower () if job_type == \"hdinsight\" : arguments_spec = definition [ \"implementation\" ][ \"hdinsight\" ][ \"args\" ] elif job_type == \"parallel\" : arguments_spec = definition [ \"implementation\" ][ \"parallel\" ][ \"args\" ] elif job_type not in [ \"scopecomponent\" , \"datatransfercomponent\" ]: arguments_spec = definition [ \"implementation\" ][ \"container\" ][ \"args\" ] script_arguments = [] generate_component_arguments_modulesdk ( definition , arguments_spec , script_arguments ) try : _ , unknown_args = parser . parse_known_args ( script_arguments ) except : assert ( False ), \"Component {} , in run.py, parse_known_args() should be able to parse {} , instead raised an exception: {} \" . format ( component_spec_path , script_arguments , traceback . format_exc () ) assert ( len ( unknown_args ) == 0 ), \"Component {} , while calling run.py with args {} , parsing arguments from module spec should not return unknown args, instead we observed unknown args : {} \" . format ( component_spec_path , script_arguments , unknown_args )","title":"if_arguments_from_component_spec_match_script_argparse()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.script_main_with_synthetic_arguments","text":"Try importing run.py, just to check if basic script passes syntax/imports checks Source code in shrike/pipeline/testing/components.py def script_main_with_synthetic_arguments ( module , mocker ): \"\"\"Try importing run.py, just to check if basic script passes syntax/imports checks\"\"\" paths = _get_module_paths ( module ) # assuming we have a yaml spec file that is loadable module_spec = module_spec_yaml_exists_and_is_parsable ( module ) # import module to get main() function if paths . module_spec_absdir not in sys . path : sys . path . insert ( 0 , paths . module_spec_absdir ) try : spec , mod = dynamic_import_module ( paths . module_import_path ) except : assert False , \"importing {} resulted in an exception: {} \" . format ( paths . module_import_path , traceback . format_exc () ) if module_spec [ \"jobType\" ] . lower () == \"hdinsight\" : arguments_spec = module_spec [ \"implementation\" ][ \"hdinsight\" ][ \"args\" ] elif ( module_spec [ \"jobType\" ] . lower () != \"scopecomponent\" and module_spec [ \"jobType\" ] . lower () != \"datatransfercomponent\" ): arguments_spec = module_spec [ \"implementation\" ][ \"container\" ][ \"args\" ] script_arguments = [] generate_argument ( module_spec , arguments_spec , script_arguments ) print ( script_arguments ) # https://medium.com/python-pandemonium/testing-sys-exit-with-pytest-10c6e5f7726f with pytest . raises ( SystemExit ) as pytest_wrapped_e : mod . main ( script_arguments + [ \"-h\" ]) assert pytest_wrapped_e . type == SystemExit assert pytest_wrapped_e . value . code == 0","title":"script_main_with_synthetic_arguments()"},{"location":"pipeline/testing-importer/","text":"Importer script Can import a function from a given script from file path dynamic_import_class ( module_path , class_name ) Dynamically imports some class/function from a python file Parameters: Name Type Description Default module_path str) path to python file (ex: ./bla/foo.py) required class_name str) name of class/function to import from it required Returns: Type Description class_attr (class) object imported from module Source code in shrike/pipeline/testing/importer.py def dynamic_import_class ( module_path , class_name ): \"\"\"Dynamically imports some class/function from a python file Args: module_path (str) : path to python file (ex: ./bla/foo.py) class_name (str) : name of class/function to import from it Returns: class_attr (class) : object imported from module \"\"\" spec = importlib . util . spec_from_file_location ( \"dynimportmodulename\" , module_path ) mod = importlib . util . module_from_spec ( spec ) spec . loader . exec_module ( mod ) class_attr = getattr ( mod , class_name ) return class_attr dynamic_import_module ( module_path ) Dynamically imports some class/function from a python file Parameters: Name Type Description Default module_path str) path to python file (ex: ./bla/foo.py) required Returns: Type Description mod, spec Source code in shrike/pipeline/testing/importer.py def dynamic_import_module ( module_path ): \"\"\"Dynamically imports some class/function from a python file Args: module_path (str) : path to python file (ex: ./bla/foo.py) Returns: mod, spec \"\"\" spec = importlib . util . spec_from_file_location ( \"dynimportmodulename\" , module_path ) mod = importlib . util . module_from_spec ( spec ) spec . loader . exec_module ( mod ) return spec , mod import_and_test_class ( module_path , class_name ) Tests importing some class/function from a python file Parameters: Name Type Description Default module_path str) path to python file (ex: ./bla/foo.py) required class_name str) name of class/function to import from it required Returns: Type Description class_attr (class) object imported from module Source code in shrike/pipeline/testing/importer.py def import_and_test_class ( module_path , class_name ): \"\"\"Tests importing some class/function from a python file Args: module_path (str) : path to python file (ex: ./bla/foo.py) class_name (str) : name of class/function to import from it Returns: class_attr (class) : object imported from module \"\"\" # WORK IN PROGRESS # test if module_path can be found in path # test if class_name exists in module_path # test return attr if class import_success = True message = None try : imported_class = dynamic_import_class ( module_path , class_name ) except : import_success = False message = traceback . format_exc () assert import_success , \"\"\" Importing class ' {} ' from module path ' {} ' did not succeed. Current python path is {} . Traceback from exception: {} \"\"\" . format ( class_name , module_path , sys . path , message ) return imported_class","title":"testing.importer"},{"location":"pipeline/testing-importer/#shrike.pipeline.testing.importer.dynamic_import_class","text":"Dynamically imports some class/function from a python file Parameters: Name Type Description Default module_path str) path to python file (ex: ./bla/foo.py) required class_name str) name of class/function to import from it required Returns: Type Description class_attr (class) object imported from module Source code in shrike/pipeline/testing/importer.py def dynamic_import_class ( module_path , class_name ): \"\"\"Dynamically imports some class/function from a python file Args: module_path (str) : path to python file (ex: ./bla/foo.py) class_name (str) : name of class/function to import from it Returns: class_attr (class) : object imported from module \"\"\" spec = importlib . util . spec_from_file_location ( \"dynimportmodulename\" , module_path ) mod = importlib . util . module_from_spec ( spec ) spec . loader . exec_module ( mod ) class_attr = getattr ( mod , class_name ) return class_attr","title":"dynamic_import_class()"},{"location":"pipeline/testing-importer/#shrike.pipeline.testing.importer.dynamic_import_module","text":"Dynamically imports some class/function from a python file Parameters: Name Type Description Default module_path str) path to python file (ex: ./bla/foo.py) required Returns: Type Description mod, spec Source code in shrike/pipeline/testing/importer.py def dynamic_import_module ( module_path ): \"\"\"Dynamically imports some class/function from a python file Args: module_path (str) : path to python file (ex: ./bla/foo.py) Returns: mod, spec \"\"\" spec = importlib . util . spec_from_file_location ( \"dynimportmodulename\" , module_path ) mod = importlib . util . module_from_spec ( spec ) spec . loader . exec_module ( mod ) return spec , mod","title":"dynamic_import_module()"},{"location":"pipeline/testing-importer/#shrike.pipeline.testing.importer.import_and_test_class","text":"Tests importing some class/function from a python file Parameters: Name Type Description Default module_path str) path to python file (ex: ./bla/foo.py) required class_name str) name of class/function to import from it required Returns: Type Description class_attr (class) object imported from module Source code in shrike/pipeline/testing/importer.py def import_and_test_class ( module_path , class_name ): \"\"\"Tests importing some class/function from a python file Args: module_path (str) : path to python file (ex: ./bla/foo.py) class_name (str) : name of class/function to import from it Returns: class_attr (class) : object imported from module \"\"\" # WORK IN PROGRESS # test if module_path can be found in path # test if class_name exists in module_path # test return attr if class import_success = True message = None try : imported_class = dynamic_import_class ( module_path , class_name ) except : import_success = False message = traceback . format_exc () assert import_success , \"\"\" Importing class ' {} ' from module path ' {} ' did not succeed. Current python path is {} . Traceback from exception: {} \"\"\" . format ( class_name , module_path , sys . path , message ) return imported_class","title":"import_and_test_class()"},{"location":"pipeline/testing-module-run-tests/","text":"PyTest suite for testing if run.py is aligned with module specification: if_arguments_from_module_spec_match_script_argparse ( module ) Tests alignment between module_spec arguments and script parser arguments Source code in shrike/pipeline/testing/module_run_tests.py def if_arguments_from_module_spec_match_script_argparse ( module ): \"\"\"Tests alignment between module_spec arguments and script parser arguments\"\"\" if_arguments_from_component_spec_match_script_argparse ( os . path . join ( module , \"module_spec.yaml\" ) ) module_run_get_arg_parser ( module ) Tests is module run.py has function get_arg_parser(parser) Source code in shrike/pipeline/testing/module_run_tests.py def module_run_get_arg_parser ( module ): \"\"\"Tests is module run.py has function get_arg_parser(parser)\"\"\" component_run_get_arg_parser ( os . path . join ( module , \"module_spec.yaml\" )) module_spec_yaml_exists_and_is_parsable ( module ) Tests the presence of module specifications in yaml (and return it) Source code in shrike/pipeline/testing/module_run_tests.py def module_spec_yaml_exists_and_is_parsable ( module ): \"\"\"Tests the presence of module specifications in yaml (and return it)\"\"\" return component_spec_yaml_exists_and_is_parsable ( os . path . join ( module , \"module_spec.yaml\" ) ) module_uses_private_acr ( module , acr_url ) Tests base image in private ACR Source code in shrike/pipeline/testing/module_run_tests.py def module_uses_private_acr ( module , acr_url ): \"\"\"Tests base image in private ACR\"\"\" component_uses_private_acr ( os . path . join ( module , \"module_spec.yaml\" ), acr_url ) module_uses_private_python_feed ( module , feed_url ) Tests private python feed referenced in conda Source code in shrike/pipeline/testing/module_run_tests.py def module_uses_private_python_feed ( module , feed_url ): \"\"\"Tests private python feed referenced in conda\"\"\" component_uses_private_python_feed ( os . path . join ( module , \"module_spec.yaml\" ), feed_url ) run_py_import ( module ) Try importing run.py, just to check if basic script passes syntax/imports checks Source code in shrike/pipeline/testing/module_run_tests.py def run_py_import ( module ): \"\"\"Try importing run.py, just to check if basic script passes syntax/imports checks\"\"\" component_run_py_import ( os . path . join ( module , \"module_spec.yaml\" ))","title":"testing.module_run_tests"},{"location":"pipeline/testing-module-run-tests/#shrike.pipeline.testing.module_run_tests.if_arguments_from_module_spec_match_script_argparse","text":"Tests alignment between module_spec arguments and script parser arguments Source code in shrike/pipeline/testing/module_run_tests.py def if_arguments_from_module_spec_match_script_argparse ( module ): \"\"\"Tests alignment between module_spec arguments and script parser arguments\"\"\" if_arguments_from_component_spec_match_script_argparse ( os . path . join ( module , \"module_spec.yaml\" ) )","title":"if_arguments_from_module_spec_match_script_argparse()"},{"location":"pipeline/testing-module-run-tests/#shrike.pipeline.testing.module_run_tests.module_run_get_arg_parser","text":"Tests is module run.py has function get_arg_parser(parser) Source code in shrike/pipeline/testing/module_run_tests.py def module_run_get_arg_parser ( module ): \"\"\"Tests is module run.py has function get_arg_parser(parser)\"\"\" component_run_get_arg_parser ( os . path . join ( module , \"module_spec.yaml\" ))","title":"module_run_get_arg_parser()"},{"location":"pipeline/testing-module-run-tests/#shrike.pipeline.testing.module_run_tests.module_spec_yaml_exists_and_is_parsable","text":"Tests the presence of module specifications in yaml (and return it) Source code in shrike/pipeline/testing/module_run_tests.py def module_spec_yaml_exists_and_is_parsable ( module ): \"\"\"Tests the presence of module specifications in yaml (and return it)\"\"\" return component_spec_yaml_exists_and_is_parsable ( os . path . join ( module , \"module_spec.yaml\" ) )","title":"module_spec_yaml_exists_and_is_parsable()"},{"location":"pipeline/testing-module-run-tests/#shrike.pipeline.testing.module_run_tests.module_uses_private_acr","text":"Tests base image in private ACR Source code in shrike/pipeline/testing/module_run_tests.py def module_uses_private_acr ( module , acr_url ): \"\"\"Tests base image in private ACR\"\"\" component_uses_private_acr ( os . path . join ( module , \"module_spec.yaml\" ), acr_url )","title":"module_uses_private_acr()"},{"location":"pipeline/testing-module-run-tests/#shrike.pipeline.testing.module_run_tests.module_uses_private_python_feed","text":"Tests private python feed referenced in conda Source code in shrike/pipeline/testing/module_run_tests.py def module_uses_private_python_feed ( module , feed_url ): \"\"\"Tests private python feed referenced in conda\"\"\" component_uses_private_python_feed ( os . path . join ( module , \"module_spec.yaml\" ), feed_url )","title":"module_uses_private_python_feed()"},{"location":"pipeline/testing-module-run-tests/#shrike.pipeline.testing.module_run_tests.run_py_import","text":"Try importing run.py, just to check if basic script passes syntax/imports checks Source code in shrike/pipeline/testing/module_run_tests.py def run_py_import ( module ): \"\"\"Try importing run.py, just to check if basic script passes syntax/imports checks\"\"\" component_run_py_import ( os . path . join ( module , \"module_spec.yaml\" ))","title":"run_py_import()"},{"location":"pipeline/testing-pipeline-class-test/","text":"PyTest suite for testing all module specification: deeptest_graph ( pipeline , definition , path = 'ROOT' ) Recursively compare a pipeline object to a serialized definition [EXPERIMENTAL] Parameters: Name Type Description Default pipeline json source for the comparison required definition json target/reference for the comparison required path str current path of the comparison (in the json tree) 'ROOT' Returns: Type Description None Source code in shrike/pipeline/testing/pipeline_class_test.py def deeptest_graph ( pipeline , definition , path = \"ROOT\" ): \"\"\"Recursively compare a pipeline object to a serialized definition [EXPERIMENTAL] Args: pipeline (json): source for the comparison definition (json): target/reference for the comparison path (str): current path of the comparison (in the json tree) Returns: None \"\"\" if definition is None : # no definition provided, let's stop inspection at this path print ( f \"deeptest_graph @ { path } : nop, definition is None\" ) return # is inspecting a dictionary structure, iterate on keys if isinstance ( pipeline , dict ) and isinstance ( definition , dict ): print ( f \"deeptest_graph @ { path } : checking dictionary\" ) for key in definition : assert ( key in pipeline ), f \"pipeline graph does not have key { key } at level @ { path } \" # ignoring all ids if key in { \"id\" , \"node_id\" , \"module_id\" , \"dataset_id\" }: print ( f \"deeptest_graph @ { path } : ignore id key { key } \" ) return if ( key in { \"run_settings\" , \"compute_run_settings\" } and definition [ key ] is not None ): # this is a specific kind of key containing a list we're transforming into a dict print ( f \"deeptest_graph @ { path } : refactoring key { key } as dict\" ) pipeline_run_settings = dict ( [( entry [ \"name\" ], entry ) for entry in pipeline [ key ]] ) definition_run_settings = dict ( [( entry [ \"name\" ], entry ) for entry in definition [ key ]] ) deeptest_graph ( pipeline_run_settings , definition_run_settings , path + \".(runsettings)\" + key , ) else : deeptest_graph ( pipeline [ key ], definition [ key ], path + \".\" + key ) return # is inspecting a list structure, each element MUST passed # NOTE: this should be improved in case list can be shuffled ? if isinstance ( pipeline , list ) and isinstance ( definition , list ): print ( f \"deeptest_graph @ { path } : checking list\" ) for key , entry in enumerate ( definition ): deeptest_graph ( pipeline [ key ], entry , path + \"[\" + str ( key ) + \"]\" ) return # if anything else (int, str, unknown), just test plain equality print ( f \"deeptest_graph @ { path } : checking equality { pipeline } == { definition } \" ) assert pipeline == definition , f \"values mismatch @ { path } \" deeptest_graph_comparison ( pipeline_export_file , pipeline_definition_file ) Compare a pipeline object to a serialized definition [EXPERIMENTAL] Parameters: Name Type Description Default pipeline_export_file str path to pipeline exported file required pipeline_definition_file str path to reference file required Returns: Type Description None Source code in shrike/pipeline/testing/pipeline_class_test.py def deeptest_graph_comparison ( pipeline_export_file , pipeline_definition_file ): \"\"\"Compare a pipeline object to a serialized definition [EXPERIMENTAL] Args: pipeline_export_file (str): path to pipeline exported file pipeline_definition_file (str): path to reference file Returns: None \"\"\" # checks the exported file in temp dir assert os . path . isfile ( pipeline_export_file ), f \"deeptest_graph_comparison() expects a file as first argument but { pipeline_export_file } does not exist.\" assert os . path . isfile ( pipeline_definition_file ), f \"deeptest_graph_comparison() expects a file as second argument but { pipeline_definition_file } does not exist.\" # read the exported graph with open ( pipeline_export_file , \"r\" ) as export_file : pipeline = json . loads ( export_file . read ()) assert ( pipeline is not None ), f \"deeptest_graph_comparison() expects first argument to be a parsable json, instead it found None\" with open ( pipeline_definition_file , \"r\" ) as definition_file : definition = json . loads ( definition_file . read ()) assert ( definition is not None ), f \"deeptest_graph_comparison() expects first argument to be a parsable json, instead it found None\" deeptest_graph ( pipeline , definition ) get_config_class ( pipeline_class ) Test if the get_arg_parser() method is in there and behaves correctly Source code in shrike/pipeline/testing/pipeline_class_test.py def get_config_class ( pipeline_class ): \"\"\"Test if the get_arg_parser() method is in there and behaves correctly\"\"\" try : config_class = pipeline_class . get_config_class () except : assert ( False ), \"getting config class for pipeline class {} resulted in an exception: {} \" . format ( pipeline_class . __name__ , traceback . format_exc () ) pipeline_required_modules ( pipeline_class ) Test if the required_modules() returns the right list of modules with all required keys Source code in shrike/pipeline/testing/pipeline_class_test.py def pipeline_required_modules ( pipeline_class ): \"\"\"Test if the required_modules() returns the right list of modules with all required keys\"\"\" modules_manifest = pipeline_class . required_modules () assert isinstance ( modules_manifest , dict ), \"required_modules() must return a dictionary.\" error_log = [] for module_key , module_description in modules_manifest . items (): if not isinstance ( module_description , dict ): error_log . append ( f \"values in dictionary returned by required_modules() must be dictionaries (under key= { module_key } , found value of type= { module_description . __name__ } )\" ) continue if \"yaml_spec\" not in module_description : error_log . append ( f \"In pipeline class module { pipeline_class . __name__ } ,\" + f \" module under key= { module_key } (in required_modules() function)\" + \" does not provide any yaml_spec key.\" + \" You need to give such a yaml_spec path before creating your pull request\" + \" so that we're able to consume this module when running pre-merge tests (detonation chamber)\" ) if \"remote_module_name\" not in module_description : error_log . append ( f \"In pipeline class module { pipeline_class . __name__ } ,\" + f \" module under key= { module_key } (in required_modules() function)\" + \" does not provide any remote_module_name.\" + \" You need to give such a name before creating your pull request\" + \" so that we're able to consume this module when running in production.\" ) # if \"namespace\" not in module_description: # error_log.append( # f\"In pipeline class module {pipeline_class.__name__},\" # + f\" module under key={module_key} (in required_modules() function)\" # + \" does not provide any namespace.\" # ) # TODO : verify if the version exists or is in the yaml spec? assert not ( error_log ), ( f \"In pipeline class module { pipeline_class . __name__ } , validation of the dictionary returned by required_modules() method shows errors: \\n \" + \" \\n \" . join ( error_log ) ) pipeline_required_subgraphs ( pipeline_class ) Tests if the required_subgraphs() returns the right list of modules with all requires keys Source code in shrike/pipeline/testing/pipeline_class_test.py def pipeline_required_subgraphs ( pipeline_class ): \"\"\"Tests if the required_subgraphs() returns the right list of modules with all requires keys\"\"\" subgraphs_manifest = pipeline_class . required_subgraphs () assert isinstance ( subgraphs_manifest , dict ), \"required_subgraphs() must return a dictionary.\" error_log = [] for subgraph_key , subgraph_class in subgraphs_manifest . items (): if not issubclass ( subgraph_class , AMLPipelineHelper ): error_log . append ( f \"In pipeline class module { pipeline_class . __name__ } , values in dictionary returned by required_subgraphs() must be subclass of AMLPipelineHelper (under key= { subgraph_key } , found object { subgraph_class . __name__ } )\" ) continue assert not ( error_log ), ( f \"In pipeline class module { pipeline_class . __name__ } , validation of the dictionary returned by required_subgraphs() shows errors: \" + \" \\n \" . join ( error_log ) )","title":"testing.pipeline_class_test"},{"location":"pipeline/testing-pipeline-class-test/#shrike.pipeline.testing.pipeline_class_test.deeptest_graph","text":"Recursively compare a pipeline object to a serialized definition [EXPERIMENTAL] Parameters: Name Type Description Default pipeline json source for the comparison required definition json target/reference for the comparison required path str current path of the comparison (in the json tree) 'ROOT' Returns: Type Description None Source code in shrike/pipeline/testing/pipeline_class_test.py def deeptest_graph ( pipeline , definition , path = \"ROOT\" ): \"\"\"Recursively compare a pipeline object to a serialized definition [EXPERIMENTAL] Args: pipeline (json): source for the comparison definition (json): target/reference for the comparison path (str): current path of the comparison (in the json tree) Returns: None \"\"\" if definition is None : # no definition provided, let's stop inspection at this path print ( f \"deeptest_graph @ { path } : nop, definition is None\" ) return # is inspecting a dictionary structure, iterate on keys if isinstance ( pipeline , dict ) and isinstance ( definition , dict ): print ( f \"deeptest_graph @ { path } : checking dictionary\" ) for key in definition : assert ( key in pipeline ), f \"pipeline graph does not have key { key } at level @ { path } \" # ignoring all ids if key in { \"id\" , \"node_id\" , \"module_id\" , \"dataset_id\" }: print ( f \"deeptest_graph @ { path } : ignore id key { key } \" ) return if ( key in { \"run_settings\" , \"compute_run_settings\" } and definition [ key ] is not None ): # this is a specific kind of key containing a list we're transforming into a dict print ( f \"deeptest_graph @ { path } : refactoring key { key } as dict\" ) pipeline_run_settings = dict ( [( entry [ \"name\" ], entry ) for entry in pipeline [ key ]] ) definition_run_settings = dict ( [( entry [ \"name\" ], entry ) for entry in definition [ key ]] ) deeptest_graph ( pipeline_run_settings , definition_run_settings , path + \".(runsettings)\" + key , ) else : deeptest_graph ( pipeline [ key ], definition [ key ], path + \".\" + key ) return # is inspecting a list structure, each element MUST passed # NOTE: this should be improved in case list can be shuffled ? if isinstance ( pipeline , list ) and isinstance ( definition , list ): print ( f \"deeptest_graph @ { path } : checking list\" ) for key , entry in enumerate ( definition ): deeptest_graph ( pipeline [ key ], entry , path + \"[\" + str ( key ) + \"]\" ) return # if anything else (int, str, unknown), just test plain equality print ( f \"deeptest_graph @ { path } : checking equality { pipeline } == { definition } \" ) assert pipeline == definition , f \"values mismatch @ { path } \"","title":"deeptest_graph()"},{"location":"pipeline/testing-pipeline-class-test/#shrike.pipeline.testing.pipeline_class_test.deeptest_graph_comparison","text":"Compare a pipeline object to a serialized definition [EXPERIMENTAL] Parameters: Name Type Description Default pipeline_export_file str path to pipeline exported file required pipeline_definition_file str path to reference file required Returns: Type Description None Source code in shrike/pipeline/testing/pipeline_class_test.py def deeptest_graph_comparison ( pipeline_export_file , pipeline_definition_file ): \"\"\"Compare a pipeline object to a serialized definition [EXPERIMENTAL] Args: pipeline_export_file (str): path to pipeline exported file pipeline_definition_file (str): path to reference file Returns: None \"\"\" # checks the exported file in temp dir assert os . path . isfile ( pipeline_export_file ), f \"deeptest_graph_comparison() expects a file as first argument but { pipeline_export_file } does not exist.\" assert os . path . isfile ( pipeline_definition_file ), f \"deeptest_graph_comparison() expects a file as second argument but { pipeline_definition_file } does not exist.\" # read the exported graph with open ( pipeline_export_file , \"r\" ) as export_file : pipeline = json . loads ( export_file . read ()) assert ( pipeline is not None ), f \"deeptest_graph_comparison() expects first argument to be a parsable json, instead it found None\" with open ( pipeline_definition_file , \"r\" ) as definition_file : definition = json . loads ( definition_file . read ()) assert ( definition is not None ), f \"deeptest_graph_comparison() expects first argument to be a parsable json, instead it found None\" deeptest_graph ( pipeline , definition )","title":"deeptest_graph_comparison()"},{"location":"pipeline/testing-pipeline-class-test/#shrike.pipeline.testing.pipeline_class_test.get_config_class","text":"Test if the get_arg_parser() method is in there and behaves correctly Source code in shrike/pipeline/testing/pipeline_class_test.py def get_config_class ( pipeline_class ): \"\"\"Test if the get_arg_parser() method is in there and behaves correctly\"\"\" try : config_class = pipeline_class . get_config_class () except : assert ( False ), \"getting config class for pipeline class {} resulted in an exception: {} \" . format ( pipeline_class . __name__ , traceback . format_exc () )","title":"get_config_class()"},{"location":"pipeline/testing-pipeline-class-test/#shrike.pipeline.testing.pipeline_class_test.pipeline_required_modules","text":"Test if the required_modules() returns the right list of modules with all required keys Source code in shrike/pipeline/testing/pipeline_class_test.py def pipeline_required_modules ( pipeline_class ): \"\"\"Test if the required_modules() returns the right list of modules with all required keys\"\"\" modules_manifest = pipeline_class . required_modules () assert isinstance ( modules_manifest , dict ), \"required_modules() must return a dictionary.\" error_log = [] for module_key , module_description in modules_manifest . items (): if not isinstance ( module_description , dict ): error_log . append ( f \"values in dictionary returned by required_modules() must be dictionaries (under key= { module_key } , found value of type= { module_description . __name__ } )\" ) continue if \"yaml_spec\" not in module_description : error_log . append ( f \"In pipeline class module { pipeline_class . __name__ } ,\" + f \" module under key= { module_key } (in required_modules() function)\" + \" does not provide any yaml_spec key.\" + \" You need to give such a yaml_spec path before creating your pull request\" + \" so that we're able to consume this module when running pre-merge tests (detonation chamber)\" ) if \"remote_module_name\" not in module_description : error_log . append ( f \"In pipeline class module { pipeline_class . __name__ } ,\" + f \" module under key= { module_key } (in required_modules() function)\" + \" does not provide any remote_module_name.\" + \" You need to give such a name before creating your pull request\" + \" so that we're able to consume this module when running in production.\" ) # if \"namespace\" not in module_description: # error_log.append( # f\"In pipeline class module {pipeline_class.__name__},\" # + f\" module under key={module_key} (in required_modules() function)\" # + \" does not provide any namespace.\" # ) # TODO : verify if the version exists or is in the yaml spec? assert not ( error_log ), ( f \"In pipeline class module { pipeline_class . __name__ } , validation of the dictionary returned by required_modules() method shows errors: \\n \" + \" \\n \" . join ( error_log ) )","title":"pipeline_required_modules()"},{"location":"pipeline/testing-pipeline-class-test/#shrike.pipeline.testing.pipeline_class_test.pipeline_required_subgraphs","text":"Tests if the required_subgraphs() returns the right list of modules with all requires keys Source code in shrike/pipeline/testing/pipeline_class_test.py def pipeline_required_subgraphs ( pipeline_class ): \"\"\"Tests if the required_subgraphs() returns the right list of modules with all requires keys\"\"\" subgraphs_manifest = pipeline_class . required_subgraphs () assert isinstance ( subgraphs_manifest , dict ), \"required_subgraphs() must return a dictionary.\" error_log = [] for subgraph_key , subgraph_class in subgraphs_manifest . items (): if not issubclass ( subgraph_class , AMLPipelineHelper ): error_log . append ( f \"In pipeline class module { pipeline_class . __name__ } , values in dictionary returned by required_subgraphs() must be subclass of AMLPipelineHelper (under key= { subgraph_key } , found object { subgraph_class . __name__ } )\" ) continue assert not ( error_log ), ( f \"In pipeline class module { pipeline_class . __name__ } , validation of the dictionary returned by required_subgraphs() shows errors: \" + \" \\n \" . join ( error_log ) )","title":"pipeline_required_subgraphs()"}]}